---
title: Lecture 6
subtitle: Tidy Data [The Lecture 6 slides may be subject to change during the Module.]
format:
  clean-revealjs:
    self-contained: true
    #### logo: logo-title-slide.png
author:
  - name: Byeong-Hak Choe
    email: bchoe@geneseo.edu
    affiliations: SUNY Geneseo
date: 2024-03-12
callout-icon: false
execute: 
  eval: false
  echo: true
#### bibliography: refs.bib
#### include-after-body: backwards.html
---


```{r setup}
#| include: false
#| eval: true

library(knitr)
library(tidyverse)
#### set default options
opts_chunk$set(echo = FALSE,
               fig.width = 7.252,
               fig.height = 4,
               comment = "####",
               dpi = 300)

knitr::knit_engines$set("markdown")
```




# Tidy Data {background-color="#1c4982"}


## Tidy Data


```{r, echo=F, eval = T, warning=F, message=F, out.width = '80%', fig.align='center'}
knitr::include_graphics("lec_figs/tidy-1.png")
```

- In a tidy `data.frame`,
  - A **variable** is in a column.
  - An **observation** is in a row. 
  - A **value** are in a cell.
  
  
Tidy data is a framework to structure data sets so they can be easily analyzed and visualized. 



  

## Tidy Data
### Columns Contain Values, Not Variables 

Data can have columns that contain values instead of variables.

- Let's consider data on income and religion in the United States from the Pew Research Center

```{.python}
import pandas as pd
pew = pd.read_csv('https://bcdanl.github.io/data/pew.csv')
```

- Not every column here is a variable. 
  - The values that relate to income are spread across multiple columns.


For data analysis, the `pew` can be reshaped so that we have `religion`, `income`, and `count` variables.


  

## Tidy Data
### Columns Contain Values, Not Variables 

```{.python}
## Show only the first few columns
pew.iloc[:,0:5]
```


The form of the data like `pew` is known as “wide” data. 
  - To turn it into the “long” tidy data format, we will have to `melt` our DataFrame.
  



## Tidy Data
### 


Pandas DataFrames have a method called `.melt()` that will reshape the DataFrame into a tidy format and it takes a few parameters:
- `id_vars` is a container (list, tuple, ndarray) that represents the variables that will remain as is.
- `value_vars` identifies the columns you want to melt down (or unpivot). 
  - By default, it will melt all the columns not specified in the `id_vars` parameter.
- `var_name` is a string for the new column name when the `value_vars` is melted down. 
  - By default, it will be called `variable`.
- `value_name` is a string for the new column name that represents the values for the `var_name`. 
  - By default, it will be called `value`.
  
  
  


## Tidy Data
### Columns Contain Values, Not Variables 

::: {.panel-tabset}
## (1)
```{.python}
## we do not need to specify a value_vars since we want to pivot
## all the columns except for the 'religion' column
pew_long = pew.melt(id_vars='religion')
pew_long

## The .melt() method also exists as a pandas function, pd.melt()
## The below line of code is the equivalent one:
## melt function
pew_long = pd.melt(pew, id_vars='religion')
```



## (2)
- We can change the defaults so that the melted/unpivoted columns are named.
```{.python}
pew_long = pew.melt(
  id_vars ="religion", var_name="income", value_name ="count"
)

pew_long
```


::: 


## Tidy Data
### Keep Multiple Columns Fixed 

::: {.panel-tabset}
## (1)
Not every data set will have one column to hold still while we unpivot the rest of the columns. 
  - Let's consider the Billboard data set.

```{.python}
billboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')
```

Each week has its own column. 
  - What if we want to create a faceted plot of the weekly ratings?




## (2)
```{.python}
## use a list to reference more than 1 variable
billboard_long = billboard.melt(
  id_vars = ["year", "artist", "track", "time", "date.entered"],
  var_name = "week",
  value_name = "rating",
)
```




::: 



## Tidy Data
### Columns Contain Multiple Variables 

::: {.panel-tabset}
## (1)

Sometimes columns in a data set may represent multiple variables. 
  - Let’s look at the Ebola data set.


```{.python}
ebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv')
ebola.columns
ebola.iloc[ :5, [0, 1, 2, 10] ]
```






## (2)

The column names `Cases_Guinea` and `Deaths_Guinea` actually contain two variables.
  - The individual status (cases and deaths, respectively) as well as the country name, Guinea. 
- The data is also arranged in a wide format that needs to be reshaped (with the `.melt()` method).
  - First, let’s fix the problem by melting the data into long format.


```{.python}
ebola_long = ebola.melt(id_vars=['Date', 'Day'])
```



## (3)
- In this case, we can split the column of interest based on the underscore, `_`.
  - We can use the `.split()` method that takes a string and “splits” it up based on a given delimiter.
  -  To get access to the string methods, we need to use the `.str.` attribute.
  
```{.python}
## split the column based on a delimiter
variable_split = ebola_long.variable.str.split('_')
variable_split
type(variable_split); type(variable_split[0])
```



## (4)
- Now that the column has been split into various pieces, the next step is to assign those pieces to a new column. 
  - To do so, we can use the `.get()` string method to “get” the index we want for each row.
  
```{.python}
status_values = variable_split.str.get(0)
country_values = variable_split.str.get(1)
```


## (5)
- Now that we have the vectors we want, we can add them to our DataFrame.
  
```{.python}
ebola_long['status'] = status_values
ebola_long['country'] = country_values
ebola_long
```



## (6)
- In the `.split()` method, there is a parameter `expand` that defaults to `False`.
  - When we set it to `True`, it will return a `DataFrame` where each result of the split is in separate columns.
```{.python}
## reset our ebola_long data
ebola_long = ebola.melt(id_vars =['Date', 'Day'])
## split the column by _ into a dataframe using expand
variable_split = ebola_long.variable.str.split('_', expand=True)

ebola_long[['status', 'country']] = variable_split
```



::: 



## Tidy Data
### Variables in Both Rows and Columns 

::: {.panel-tabset}
## (1)

What happens if a column of data actually holds two variables instead of one variable?
  -  In this case, we will have to `pivot` the variable into separate columns, i.e., go from "long" data to "wide" data.

```{.python}
weather = pd.read_csv('https://bcdanl.github.io/data/weather.csv')
```


## (2)

The weather data include minimum (`tmin`) and maximum (`tmax`) temperatures recorded for each day (`d1`, `d2`, ... , `d31`) of the month (`month`).
  - The `element` column contains variables that may need to be pivoted wider to become new columns, 
  - The day variables may need to be melted into row values.
  

Let’s first fix the day values.
```{.python}
weather_melt = weather.melt(
  id_vars=["id", "year", "month", "element"],
  var_name="day",
  value_name="temp",
)
```



## (3)

Next, we need to pivot up the variables stored in the element column.

```{.python}
weather_tidy = weather_melt.pivot_table(
    index = ['id', 'year', 'month', 'day'],
    columns = 'element',
    values = 'temp'
)
```



## (4)
We can also flatten the hierarchical columns.
```{.python}
weather_tidy_flat = weather_tidy.reset_index()
```



## (5)
For `day` variable, we can replace 'd' with "". 
  - Then, convert `day` from string to integer.
  - Then, sort `weather_tidy_flat` by `['year', 'month', 'day']`.
  
```{.python}
weather_tidy_flat['day'] = weather_tidy_flat.day.str.replace('d', "")
weather_tidy_flat['day'] = weather_tidy_flat['day'].astype(int)
weather_tidy_flat = weather_tidy_flat.sort_values(by = ['year', 'month', 'day'])
```



::: 


# Data Types {background-color="#1c4982"}


## Data Types

Let's consider the built-in `tips` DataFrame from the `seaborn` library.


To get a list of the data types stored in each column of our DataFrame, we call the `dtypes` attribute.

```{.python}
import seaborn as sns
tips = sns.load_dataset("tips")
tips.dtypes
```

- The `category` data type represents categorical variables. 
  - It differs from the generic `object` data type that stores arbitrary Python objects (usually *strings*).



## Data Types
### Converting Types 

```{.python}
## convert the category sex column into a string dtype
tips['sex_str'] = tips['sex'].astype(str)

## convert total_bill into a string
tips['total_bill'] = tips['total_bill'].astype(str)

## convert it back to a float
tips['total_bill'] = tips['total_bill'].astype(float)

## convert sex into a string
tips['sex'] = tips['sex'].astype(str)

## convert it back to a category
tips['sex'] = tips['sex'].astype(category)
```



## Data Types
### `to_numeric()` method 

::: {.panel-tabset}
## (1)
When converting variables into numeric values (e.g., int, float), we can also use the Pandas `to_numeric()` function.
  - It handles non-numeric values better.

```{.python}

## subset the tips data
tips_sub_miss = tips.head(10).copy()
## assign some 'missing' values
tips_sub_miss.loc[[1, 3, 5, 7], 'total_bill'] = 'missing'
```


## (2)

```{.python}
## this will cause an error
tips_sub_miss['total_bill'].astype(float)

## this will cause an error
pd.to_numeric(tips_sub_miss['total_bill'])
```


## (3)


The `to_numeric()` function has a parameter called `errors` that governs what happens when the function encounters a value that it is unable to convert to a numeric value.
  - `'raise'`: Default. 
  - `'coerce'`: Invalid parsing will be set as `NaN`
  - `'ignore'`: Invalid parsing will return the input as is.
 

## (4)

```{.python}
tips_sub_miss["total_bill"] = pd.to_numeric(
    tips_sub_miss["total_bill"], errors="ignore"
)

tips_sub_miss["total_bill"]=pd.to_numeric(
    tips_sub_miss["total_bill"], errors="coerce"
)

```



::: 




# Data Assembly {background-color="#1c4982"}


## Data Assembly
###  

Sometimes, we better combine various DataFrames together to analyze a set of data.

  - The data may have been split up into separate DataFrames to reduce the amount of redundant information.

  - e.g., DataFrame for county-level data and DataFrame for geographic information, such as longitude and latitude





## Data Concatenation
### 

Concatenation can be thought of as appending a row or column to our data. 
  - This approach is possible if our data was split into parts or if we performed a calculation that we want to append to our existing data set.
- Let's consider the following example DataFrames:
```{.python}
df1 = pd.read_csv('https://bcdanl.github.io/data/concat_1.csv')
df2 = pd.read_csv('https://bcdanl.github.io/data/concat_2.csv')
df3 = pd.read_csv('https://bcdanl.github.io/data/concat_3.csv')
```


- We will be working with `.index` and `.columns` in this Section.
```{.python}
df1.index
df1.columns
df1.values
```


## Data Concatenation
### Add Rows 


Concatenating the DataFrames on top of each other uses the `concat()` function. 
  - All of the DataFrames to be concatenated are passed in a `list`.
  
```{.python}
row_concat = pd.concat([df1, df2, df3])
row_concat
```

-  The row names (i.e., the row indices) are simply a stacked version of the original row indices.
```{.python}
row_concat.iloc[3, :]
```


## Data Concatenation
### Add Rows 

- Let's consider a new Series and concatenate it with `df1`:
```{.python}
## create a new row of data
new_row_series = pd.Series(['n1', 'n2', 'n3', 'n4'])
new_row_series


## attempt to add the new row to a dataframe
df = pd.concat([df1, new_row_series])
df
```

-  Not only did our code not append the values as a row, but it also created a new column completely misaligned with everything else.
  - Why?



## Data Concatenation
### Add Rows 


To fix the problem, we need turn our series into a DataFrame. 
  - This data frame contains one row of data, and the column names are the ones the data will bind to.

```{.python}
new_row_df = pd.DataFrame(
  ## note the double brackets to create a "row" of data
  data =[["n1", "n2", "n3", "n4"]],
  columns =["A", "B", "C", "D"],
)

df = pd.concat([df1, new_row_df])
```



## Data Concatenation
###  Ignore the Index 

- We can use the `ignore_index` parameter to reset the row index after the concatenation if we simply want to concatenate or append data together.

```{.python}
row_concat_i = pd.concat([df1, df2, df3], ignore_index=True)
```



## Data Concatenation
###  Add Columns 


Concatenating columns is very similar to concatenating rows. 
  - The main difference is the `axis` parameter in the `concat` function. 
  - The default value of `axis` is `0` (or `axis = "index"`), so it will concatenate data in a row-wise fashion. 
  - If we pass `axis = 1` (or `axis = "columns"`) to the function, it will concatenate data in a column-wise manner.
  
  
```{.python}
col_concat = pd.concat([df1, df2, df3], axis = "columns")
```



## Data Concatenation
###  Add Columns 

- Adding a single column to a dataframe can be done directly without using any specific  Pandas function.

```{.python}
col_concat['new_col_list'] = ['n1', 'n2', 'n3', 'n4']
```


- We can reset the column indices so we do not have duplicated column names.

```{.python}
pd.concat([df1, df2, df3], axis="columns", ignore_index=True)
```




## Data Concatenation
### Concatenate with Different Indices 

What would happen when the row and column indices are not aligned?

- Let’s modify our DataFrames for the next few examples.
```{.python}
## rename the columns of our dataframes
df1.columns = ['A', 'B', 'C', 'D']
df2.columns = ['E', 'F', 'G', 'H']
df3.columns = ['A', 'C', 'F', 'H']
```

-  If we try to concatenate these DataFrames as we did, the DataFrames now do much more than simply stack one on top of the other.
```{.python}
row_concat = pd.concat([df1, df2, df3])
```




## Data Concatenation
### Concatenate with Different Indices 

- We can set `join = 'inner'` to keep only the columns that are shared among the data sets.
```{.python}
pd.concat([df1, df2, df3], join ='inner')
```


- If we use the DataFrames that have columns in common, only the columns that all of them share will be returned.
```{.python}
pd.concat([df1, df3], join ='inner',  ignore_index =False)
```




## Data Concatenation
### Concatenate with Different Indices 


- Let’s modify our DataFrames further.
```{.python}
## re-indexing the rows of our dataframes
df1.index = [0, 1, 2, 3]
df2.index = [4, 5, 6, 7]
df3.index = [0, 2, 5, 7]
```




## Data Concatenation
### Concatenate with Different Indices 


- When we concatenate along `axis="columns"` `(axis=1`), the new DataFrames will be added in a column-wise fashion and matched against their respective row indices.

```{.python}
col_concat = pd.concat([df1, df2, df3], axis="columns")
```


- Just as we did when we concatenated in a row-wise manner, we can choose to keep the results only when there are matching indices by using `join="inner"`.
```{.python}
pd.concat([df1, df3], axis ="columns", join='inner')
```





## Relational data

Why is one data set sometimes scattered across multiple files?
  - The size of the files can be huge.
  - The data collection process can be scattered across time and space.
 
 
Sometimes we may have two or more DataFrames (tables) that we want to combine based on common data values.
  -  This task is known in the database world as performing a “join.”
  -  We can do this with the `.merge()` method in Pandas.




## Relational data


-  The variables that are used to connect each pair of tables are called **keys**.

```{r, echo = F, eval = T, out.width='100%', fig.align='center'}
text_tbl <- data.frame(
  Pandas = c("left", "right", "outer", "inner"),
  SQL = c("left outer",
"right outer", 
"full outer",
"inner"),
  Description = c("Keep all the observations from the left",  
                  "Keep all the observations from the right", 
                  "Keep all the observations from both left and right",
                  "Keep only the observations whose key values exist in both")
  )


# Create a DT datatable without search box and 'Show entries' dropdown
DT::datatable(text_tbl, rownames = FALSE,
              options = list(
  dom = 't', # This sets the DOM layout without the search box ('f') and 'Show entries' dropdown ('l')
  paging = FALSE, # Disable pagination
  columnDefs = list(list(
    targets = "_all", # Applies to all columns
    orderable = FALSE # Disables sorting
  ))
), callback = htmlwidgets::JS("
  // Change header background and text color
  $('thead th').css('background-color', '#1c4982');
  $('thead th').css('color', 'white');

  // Loop through each row and alternate background color
  $('tbody tr').each(function(index) {
    if (index % 2 == 0) {
      $(this).css('background-color', '#d1dae6'); // Light color for even rows
    } else {
      $(this).css('background-color', '#9fb2cb'); // Dark color for odd rows
    }
  });

  // Set text color for all rows
  $('tbody tr').css('color', 'black');

  // Add hover effect
  $('tbody tr').hover(
    function() {
      $(this).css('background-color', '#607fa7'); // Color when mouse hovers over a row
    }, 
    function() {
      var index = $(this).index();
      if (index % 2 == 0) {
        $(this).css('background-color', '#d1dae6'); // Restore even row color
      } else {
        $(this).css('background-color', '#9fb2cb'); // Restore odd row color
      }
    }
  );
")
)

```




## Relational data
### Merges

```{r, echo=FALSE, eval = T, out.width = '20%', fig.align='center'}
knitr::include_graphics("lec_figs/join-setup.png")
```

:::: {.columns}
::: {.column width="50%"}
```{.python}
x = pd.DataFrame({
    'key': [1, 2, 3],
    'val_x': ['x1', 'x2', 'x3']
})

```
:::
::: {.column width="50%"}
```{.python}
y = pd.DataFrame({
    'key': [1, 2, 4],
    'val_y': ['y1', 'y2', 'y3']
})
```
:::
::::

- The colored column represents the "key" variable.
- The grey column represents the "value" column.



## Merges

::: {.panel-tabset}
## inner
- An **inner join** matches pairs of observations whenever their keys are equal:

```{r, echo=FALSE, eval = T, out.width = '50%', fig.align='center'}
knitr::include_graphics("lec_figs/join-inner.png")
```

```{.python}
## the default value for 'how' is 'inner'
## so it doesn't actually need to be specified
merge_inner = pd.merge(x, y, on='key', how='inner')
merge_inner_x = x.merge(y, on='key', how='inner')
merge_inner_x_how = x.merge(y, on='key')
```




## left

- A **left join** keeps all observations in `x`.

```{r, echo=FALSE, eval = T, out.width = '50%', fig.align='center'}
knitr::include_graphics("lec_figs/join-left.png")
```

```{.python}
merge_left = pd.merge(x, y, on='key', how='left')
merge_left_x = x.merge(y, on='key', how='left')
```

- The most commonly used join is the left join.



## right

- A **right join** keeps all observations in `y`.

```{r, echo=FALSE, eval = T, out.width = '50%', fig.align='center'}
knitr::include_graphics("lec_figs/join-right.png")
```

```{.python}
merge_right = pd.merge(x, y, on='key', how='right')
merge_right_x = x.merge(y, on='key', how='right')
```




## outer full

- A **full join** keeps all observations in `x` and `y`.

```{r, echo=FALSE, eval = T, out.width = '50%', fig.align='center'}
knitr::include_graphics("lec_figs/join-full.png")
```

```{.python}
merge_outer = pd.merge(x, y, on='key', how='outer')
merge_outer_x = x.merge(y, on='key', how='outer')
```


::: 





## Merges
### Duplicate keys

::: {.panel-tabset}
## one-to-many
- One data frame has duplicate keys (a one-to-many relationship). 
```{r, echo=FALSE, eval = T, out.width = '30%', fig.align='center'}
knitr::include_graphics("lec_figs/join-one-to-many.png")
```

:::: {.columns}
::: {.column width="50%"}
```{r, echo = T, eval = F}
x = pd.DataFrame({
    'key': [1, 2, 3],
    'val_x': ['x1', 'x2', 'x3'] })
```

:::
::: {.column width="50%"}
```{r, echo = T, eval = F}
y = pd.DataFrame({
    'key': [1, 2, 4],
    'val_y': ['y1', 'y2', 'y3'] })
one_to_many = x.merge(y, on='key', how='left')
```

:::
::::



## many-to-many
- Both data frames have duplicate keys (many-to-many relationship).
```{r, echo=FALSE, eval = T, out.width = '27.5%', fig.align='center'}
knitr::include_graphics("lec_figs/join-many-to-many.png")
```


:::: {.columns}
::: {.column width="50%"}
```{r, echo = T, eval = F}
x = pd.DataFrame({
  'key': [1, 2, 2, 3],
  'val_x': ['x1', 'x2', 'x3', 'x4'] })

```

:::

::: {.column width="50%"}
```{r, echo = T, eval = F}
y = pd.DataFrame({
  'key': [1, 2, 2, 3],
  'val_y': ['y1', 'y2', 'y3', 'y4'] })
many_to_many = x.merge(y, on='key', how='left')

```
:::
::::


::: 





## Merges
### Defining the key columns

- If the left and right columns do not have the same name for the key columns, we can use the `left_on` and `right_on` parameters instead.

:::: {.columns}
::: {.column width="50%"}
```{r, echo = T, eval = F}
x = pd.DataFrame({
  'key_x': [1, 2, 3],
  'val_x': ['x1', 'x2', 'x3']
})
```
:::

::: {.column width="50%"}
```{r, echo = T, eval = F}
y = pd.DataFrame({
  'key_y': [1, 2],
  'val_y': ['y1', 'y2'] })

keys_xy = x.merge(y, 
                  left_on='key_x', 
                  right_on = 'key_y', 
                  how='left')
```
:::
::::







<script>
document.addEventListener('wheel', function(event) {
    if (event.deltaY > 0) {
        Reveal.next(); // Scroll down to go to the next slide
    } else {
        Reveal.prev(); // Scroll up to go to the previous slide
    }
}, false);
</script>