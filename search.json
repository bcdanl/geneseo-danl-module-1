[
  {
    "objectID": "listing-danl-m1-qa.html",
    "href": "listing-danl-m1-qa.html",
    "title": "DANL Module 1 - Lecture Discussion",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\nLecture 7 - Discussion and Q & A\n\n\nMarch 19, 2023\n\n\n\n\nLecture 6 - Discussion and Q & A\n\n\nMarch 12, 2023\n\n\n\n\nLecture 5 - Discussion and Q & A\n\n\nMarch 5, 2023\n\n\n\n\nLecture 4 - Discussion and Q & A\n\n\nFebruary 27, 2023\n\n\n\n\nLecture 3 - Discussion and Q & A\n\n\nFebruary 20, 2023\n\n\n\n\nLecture 2 - Discussion and Q & A\n\n\nFebruary 13, 2023\n\n\n\n\nLecture 1 - Discussion and Q & A\n\n\nFebruary 6, 2023\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html",
    "href": "danl-hw/danl-m1-hw-1.html",
    "title": "Homework Assignment 1",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q1a",
    "href": "danl-hw/danl-m1-hw-1.html#q1a",
    "title": "Homework Assignment 1",
    "section": "Q1a",
    "text": "Q1a\n\nCalculate the simple difference between the probability of survival when passengers are first-class and the probability of survival when they are not.\n\n\n# Count the number of passengers in each class and return the count in descending order\ntitanic_1[['pclass']].value_counts()\n\n# Count the number of passengers who survived and who didn't and return the count in descending order\ntitanic_1[['survived']].value_counts()\n\n# Add a new column 'd' to the titanic_1 dataframe and set all values to 0\ntitanic_1['d'] = 0\n\n# For rows where the 'pclass' column is '1st class', set the value of the 'd' column to 1\ntitanic_1.loc[titanic_1['pclass']=='1st class', 'd'] = 1\n\n# Add a new column 'survived_d' to the titanic_1 DataFrame and set all values to 0\ntitanic_1['survived_d'] = 0\n\n# For rows where the 'survived' column is 'yes', set the value of the 'survived_d' column to 1\ntitanic_1.loc[titanic_1['survived']=='yes', 'survived_d'] = 1\n\n# Compute the mean of 'survived_d' for rows where 'd' is 0\ne_y0 = titanic_1.loc[titanic_1['d']==0, 'survived_d'].mean()\n\n# Compute the mean of 'survived_d' for rows where 'd' is 1\ne_y1 = titanic_1.loc[titanic_1['d']==1, 'survived_d'].mean()\n\n# Compute the SDP (so called treatment effect) by subtracting the mean of 'survived_d' where 'd' is 0 from the mean where 'd' is 1\nSDP = e_y1 - e_y0\n\n# Return the value of SDP\nSDP\n\n0.3152436786584735"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q1b",
    "href": "danl-hw/danl-m1-hw-1.html#q1b",
    "title": "Homework Assignment 1",
    "section": "Q1b",
    "text": "Q1b\n\nHow much does the probability of survival increase for first-class passengers relative to those who are not first-class passengers?\nSDP tells us what would happen to the probability of survival if non-first-class passengers were first-class.\n\nIn other words, SDP means the effect of being the first-class on the probability of survival from the Titanic Disaster."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q1c",
    "href": "danl-hw/danl-m1-hw-1.html#q1c",
    "title": "Homework Assignment 1",
    "section": "Q1c",
    "text": "Q1c\n\nConsider the probability of survival in titanic_2.csv.\n\n\ntitanic_2 = pd.read_csv('https://bcdanl.github.io/data/titanic_2.csv')\n\n\ntitanic_2.head()\n\n\n\n\n\n\n\n\npclass\nsurvived\nsex\nage\n\n\n\n\n0\n1st class\nyes\nfemale\n29.0000\n\n\n1\n1st class\nyes\nmale\n0.9167\n\n\n2\n1st class\nno\nfemale\n2.0000\n\n\n3\n1st class\nno\nmale\n30.0000\n\n\n4\n1st class\nno\nfemale\n25.0000\n\n\n\n\n\n\n\n\ntitanic_2.describe()\n\n\n\n\n\n\n\n\nage\n\n\n\n\ncount\n1046.000000\n\n\nmean\n29.881135\n\n\nstd\n14.413500\n\n\nmin\n0.166700\n\n\n25%\n21.000000\n\n\n50%\n28.000000\n\n\n75%\n39.000000\n\n\nmax\n80.000000\n\n\n\n\n\n\n\n\nAfter stratifying on gender and age, what happens to the difference in the probabilities of survival between first-class passengers and non-first-class passengers.\nExplain in your own words what stratifying on gender and age did for this difference in probabilities of survival between first-class passengers and non-first-class passengers.\n\n\n# Get count of passengers by pclass\ntitanic_2[['pclass']].value_counts()\n\n# Get count of passengers who survived or not\ntitanic_2[['survived']].value_counts()\n\n# Get count of passengers by gender\ntitanic_2[['sex']].value_counts()\n\n# Get count of passengers by age\ntitanic_2[['age']].value_counts()\n\nage    \n24.0000    47\n22.0000    43\n21.0000    41\n30.0000    40\n18.0000    39\n           ..\n20.5000     1\n11.5000     1\n0.6667      1\n0.4167      1\n80.0000     1\nName: count, Length: 98, dtype: int64\n\n\n\ntitanic_2.groupby('sex').describe()\n\n\n\n\n\n\n\n\nage\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n\nfemale\n388.0\n28.687071\n14.576995\n0.1667\n19.0\n27.0\n38.0\n76.0\n\n\nmale\n658.0\n30.585233\n14.280571\n0.3333\n21.0\n28.0\n39.0\n80.0\n\n\n\n\n\n\n\n\n# Create a new column 'd' and set its value to 0 for all rows\ntitanic_2['d'] = 0\n\n# Set the value of column 'd' to 1 for rows where pclass is '1st class'\ntitanic_2.loc[titanic_2['pclass']=='1st class', 'd'] = 1\n\n# Create a new column 'survived_d' and set its value to 0 for all rows\ntitanic_2['survived_d'] = 0\n\n# Set the value of column 'survived_d' to 1 for rows where survived is 'yes'\ntitanic_2.loc[titanic_2['survived']=='yes', 'survived_d'] = 1\n\n# Create a new column 'sex_d' and set its value to 0 for all rows\ntitanic_2['sex_d'] = 0\n\n# Set the value of column 'sex_d' to 1 for rows where sex is 'male'\ntitanic_2.loc[titanic_2['sex']=='male', 'sex_d'] = 1\n\n# Create a new column 'AgeGroup' and set its value to 0 for all rows\ntitanic_2['AgeGroup'] = 0\n\n# Set the value of column 'AgeGroup' to 1 for rows where age is greater than or equal to 18\ntitanic_2.loc[titanic_2['age'] &gt;= 18, 'AgeGroup'] = 1\n\n\ntitanic_2.groupby('AgeGroup').describe()\n\n\n\n\n\n\n\n\nage\nd\n...\nsurvived_d\nsex_d\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nAgeGroup\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n154.0\n9.101732\n6.015429\n0.1667\n3.0\n9.0\n15.75\n17.0\n418.0\n0.129187\n...\n1.0\n1.0\n418.0\n0.638756\n0.480937\n0.0\n0.0\n1.0\n1.0\n1.0\n\n\n1\n892.0\n33.468610\n12.244544\n18.0000\n24.0\n30.0\n41.00\n80.0\n892.0\n0.301570\n...\n1.0\n1.0\n892.0\n0.645740\n0.478557\n0.0\n0.0\n1.0\n1.0\n1.0\n\n\n\n\n2 rows × 32 columns\n\n\n\n\ntitanic_2.groupby(['sex', 'AgeGroup']).describe()\n\n\n\n\n\n\n\n\n\nage\nd\n...\nsurvived_d\nsex_d\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsex\nAgeGroup\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfemale\n0\n72.0\n9.015047\n5.974978\n0.1667\n3.00\n9.0\n15.0\n17.0\n150.0\n0.126667\n...\n1.0\n1.0\n150.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n316.0\n33.169304\n12.016747\n18.0000\n23.75\n30.0\n40.0\n76.0\n316.0\n0.395570\n...\n1.0\n1.0\n316.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nmale\n0\n82.0\n9.177845\n6.086437\n0.3333\n3.00\n9.0\n16.0\n17.0\n267.0\n0.131086\n...\n0.0\n1.0\n267.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n1\n576.0\n33.632812\n12.375016\n18.0000\n24.00\n30.0\n41.0\n80.0\n576.0\n0.250000\n...\n0.0\n1.0\n576.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n\n\n4 rows × 32 columns\n\n\n\n\n# Create a new column 's' and set its value to 0 for all rows\ntitanic_2['s'] = 0 \n\n# Set the value of column 's' based on gender and age group\ntitanic_2.loc[(titanic_2.sex_d == 0) & (titanic_2.AgeGroup == 1), 's'] = 1\ntitanic_2.loc[(titanic_2.sex_d == 0) & (titanic_2.AgeGroup == 0), 's'] = 2\ntitanic_2.loc[(titanic_2.sex_d == 1) & (titanic_2.AgeGroup == 1), 's'] = 3\ntitanic_2.loc[(titanic_2.sex_d == 1) & (titanic_2.AgeGroup == 0), 's'] = 4\n\n# Get the number of observations where d is 0\nobs = titanic_2.loc[titanic_2.d == 0].shape[0]\n\n# Define a function to calculate weighted average effect\ndef weighted_avg_effect(df):\n  \n  # Calculate the difference in survival rates between the treatment and control groups\n    diff = ( df[ df.d == 1 ].survived_d.mean() - \n             df[ df.d == 0 ].survived_d.mean() )\n  \n  # Calculate the weight assigned to the treatment group\n    weight = df[ df.d == 0 ].shape[0] / obs\n    \n  # Calculate the weighted average effect\n    return diff * weight\n\n\n# Apply the weighted_avg_effect function to each group in the data frame grouped by the s variable\nSDP2 = titanic_2.groupby('s').apply( weighted_avg_effect )\n\n# Calculate the weighted average effect of treatment for the entire population\nWSDP = SDP2.sum()\n\nWSDP\n\n0.24695099450754543\n\n\n\nThe probability of survival for the first-class passengers can be different across gender and age groups.\n\nIn other words, the effect of being the first-class on the probability of survival from the Titanic Disaster can be different across genders and age groups.\n\nWSDP takes into account the difference in the effect of being first-class across gender and age groups by weighting.\n\n\nWSDP - SDP\n\n-0.0682926841509281\n\n\n\nThe probability of survival for first-class after taking into account gender and age (WSDP) is less than that (SDP) which does presumably assume that characteristics of passengers such as gender and ages is related with the probability of survival."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q2a",
    "href": "danl-hw/danl-m1-hw-1.html#q2a",
    "title": "Homework Assignment 1",
    "section": "Q2a",
    "text": "Q2a\n\nHow many players have been recorded?\n\n\n# the number of unique players\nq2a = nhl1617['id_player'].nunique()  \n\nq2a\n\n888"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q2b.",
    "href": "danl-hw/danl-m1-hw-1.html#q2b.",
    "title": "Homework Assignment 1",
    "section": "Q2b.",
    "text": "Q2b.\n\nA column points (“P”) is missing in the data. The number of points of a player is defined as the sum of his goals (“G”) and assists (“A”).\nAdd the point column “P” to your DataFrame.\n\n\n# create a new column called 'P' in the nhl1617 dataframe that is the sum of the 'G' and 'A' columns\nq2b = nhl1617.assign(P = nhl1617['G'] + nhl1617['A'])\n\nq2b\n\n\n\n\n\n\n\n\nid_player\nBorn\nCity\nCntry\nNat\nHt\nWt\nLast_Name\nFirst_Name\nPosition\nTeam\nGP\nG\nA\nTOI\nTOI_GP\nP\n\n\n\n\n0\n1\n30.04.1988\nHamilton\nCAN\nCAN\n69\n170\nAbbott\nSpencer\nLW\nCHI\n1\n0\n0\n514\n8.57\n0\n\n\n1\n2\n25.02.1987\nMuskegon\nUSA\nUSA\n74\n218\nAbdelkader\nJustin\nLW/RW\nDET\n64\n7\n14\n63969\n16.65\n21\n\n\n2\n3\n23.09.1993\nStockholm\nSWE\nSWE\n71\n196\nAberg\nPontus\nLW\nNSH\n15\n1\n1\n11102\n12.33\n2\n\n\n3\n4\n01.12.1991\nJohnston\nUSA\nUSA\n70\n208\nAcciari\nNoel\nC\nBOS\n29\n2\n3\n18047\n10.23\n5\n\n\n4\n5\n30.04.1992\nMorristown\nUSA\nUSA\n72\n202\nAgostino\nKenny\nLW\nSTL\n7\n1\n2\n5366\n12.78\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n883\n884\n18.04.1993\nHuddinge\nSWE\nSWE\n74\n215\nZibanejad\nMika\nC/RW\nNYR\n56\n14\n23\n57362\n17.07\n37\n\n\n884\n885\n01.09.1987\nToronto\nCAN\nCAN\n71\n180\nZolnierczyk\nHarry\nLW\nNSH\n24\n2\n2\n12776\n8.87\n4\n\n\n885\n886\n01.09.1987\nOslo\nNOR\nNOR\n67\n179\nZuccarello\nMats\nRW/C/LW\nNYR\n80\n15\n44\n90378\n18.83\n59\n\n\n886\n887\n16.01.1992\nNewport Beach\nUSA\nUSA\n71\n187\nZucker\nJason\nLW/RW\nMIN\n79\n22\n25\n72455\n15.28\n47\n\n\n887\n888\n15.05.1995\nSt. Petersburg\nRUS\nRUS\n73\n224\nZykov\nValentin\nLW\nCAR\n2\n1\n0\n750\n6.25\n1\n\n\n\n\n888 rows × 17 columns"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q2c.",
    "href": "danl-hw/danl-m1-hw-1.html#q2c.",
    "title": "Homework Assignment 1",
    "section": "Q2c.",
    "text": "Q2c.\n\nWho is the top scorer in terms of points?\n\n\n# Sort the DataFrame q2b by the column 'P' in descending order\nq2c = q2b.sort_values(by= 'P', ascending=False)\n\nq2c\n\n\n\n\n\n\n\n\nid_player\nBorn\nCity\nCntry\nNat\nHt\nWt\nLast_Name\nFirst_Name\nPosition\nTeam\nGP\nG\nA\nTOI\nTOI_GP\nP\n\n\n\n\n509\n510\n13.01.1997\nRichmond Hill\nCAN\nCAN\n73\n200\nMcDavid\nConnor\nC\nEDM\n82\n30\n70\n103967\n21.13\n100\n\n\n149\n150\n07.08.1987\nCole Harbour\nCAN\nCAN\n71\n200\nCrosby\nSidney\nC\nPIT\n75\n44\n45\n89450\n19.88\n89\n\n\n389\n390\n19.11.1988\nBuffalo\nUSA\nUSA\n71\n177\nKane\nPatrick\nRW/C\nCHI\n82\n34\n55\n105263\n21.40\n89\n\n\n21\n22\n23.11.1987\nGävle\nSWE\nSWE\n73\n213\nBackstrom\nNicklas\nC\nWSH\n82\n23\n63\n89839\n18.27\n86\n\n\n426\n427\n17.06.1993\nMaykop\nRUS\nRUS\n71\n178\nKucherov\nNikita\nRW\nT.B\n74\n40\n45\n86320\n19.43\n85\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n512\n513\n22.05.1994\nHamilton\nCAN\nCAN\n74\n203\nMcEneny\nEvan\nD\nVAN\n1\n0\n0\n908\n15.13\n0\n\n\n520\n521\n22.02.1993\nLangley\nCAN\nCAN\n74\n214\nMcNeill\nMark\nC/RW\nDAL\n1\n0\n0\n829\n13.82\n0\n\n\n781\n782\n22.04.1994\nSaskatoon\nCAN\nCAN\n72\n204\nStephenson\nChandler\nC\nWSH\n4\n0\n0\n2131\n8.88\n0\n\n\n523\n524\n10.12.1992\nPlantation\nUSA\nUSA\n78\n225\nMegna\nJaycob\nD\nANA\n1\n0\n0\n920\n15.33\n0\n\n\n0\n1\n30.04.1988\nHamilton\nCAN\nCAN\n69\n170\nAbbott\nSpencer\nLW\nCHI\n1\n0\n0\n514\n8.57\n0\n\n\n\n\n888 rows × 17 columns"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q2d.",
    "href": "danl-hw/danl-m1-hw-1.html#q2d.",
    "title": "Homework Assignment 1",
    "section": "Q2d.",
    "text": "Q2d.\n\nHow many Russian (non-goalie) players had some ice time in there 2016/2017 regular season?\nHint: Nationality of a player can be found in “Nat”. Russians are indicated by “RUS”.\n\n\n# the number of rows in the 'nhl1617' dataframe where the value in the 'Nat' column is equal to 'RUS', and returns the count as output.\nq2d = len( nhl1617[nhl1617['Nat'] == 'RUS'] )\n\nq2d\n\n38"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q2e.",
    "href": "danl-hw/danl-m1-hw-1.html#q2e.",
    "title": "Homework Assignment 1",
    "section": "Q2e.",
    "text": "Q2e.\n\nWhat are their names?\n\n\n# Select rows where the 'Nat' column equals 'RUS', then select only the 'Last_Name' and 'First_Name' columns\nq2e = nhl1617[ nhl1617['Nat'] == 'RUS' ][ ['Last_Name', 'First_Name'] ]\n\nq2e\n\n\n\n\n\n\n\n\nLast_Name\nFirst_Name\n\n\n\n\n11\nAnisimov\nArtem\n\n\n27\nBarbashev\nIvan\n\n\n87\nBuchnevich\nPavel\n\n\n90\nBurmistrov\nAlex\n\n\n207\nEmelin\nAlexei\n\n\n272\nGoldobin\nNikolay\n\n\n293\nGrigorenko\nMikhail\n\n\n303\nGurianov\nDenis\n\n\n385\nKalinin\nSergey\n\n\n386\nKamenev\nVladislav\n\n\n426\nKucherov\nNikita\n\n\n429\nKulemin\nNikolay\n\n\n430\nKulikov\nDmitry\n\n\n433\nKuznetsov\nEvgeny\n\n\n473\nLyubimov\nRoman\n\n\n480\nMalkin\nEvgeni\n\n\n486\nMarchenko\nAlexey\n\n\n489\nMarkov\nAndrei\n\n\n557\nNamestnikov\nVladislav\n\n\n566\nNesterov\nNikita\n\n\n595\nOrlov\nDmitry\n\n\n600\nOvechkin\nAlex\n\n\n607\nPanarin\nArtemi\n\n\n646\nProvorov\nIvan\n\n\n657\nRadulov\nAlex\n\n\n707\nScherbak\nNikita\n\n\n724\nSergachev\nMikhail\n\n\n749\nSlepyshev\nAnton\n\n\n763\nSoshnikov\nNikita\n\n\n799\nSvechnikov\nEvgeny\n\n\n802\nTarasenko\nVladimir\n\n\n819\nTolchinsky\nSergey\n\n\n824\nTryamkin\nNikita\n\n\n828\nTyutin\nFedor\n\n\n875\nYakupov\nNail\n\n\n878\nZadorov\nNikita\n\n\n879\nZaitsev\nNikita\n\n\n887\nZykov\nValentin"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q2f.",
    "href": "danl-hw/danl-m1-hw-1.html#q2f.",
    "title": "Homework Assignment 1",
    "section": "Q2f.",
    "text": "Q2f.\n\nWho performed best among the Russian players in terms of points (“P”)?\n\n\n# Select only the rows where 'Nat' column is 'RUS'\n# Then sort these rows in descending order based on the values in the 'P' column\nq2f = q2b[q2b['Nat'] == 'RUS'].sort_values(by='P', ascending = False)\n\nq2f\n\n\n\n\n\n\n\n\nid_player\nBorn\nCity\nCntry\nNat\nHt\nWt\nLast_Name\nFirst_Name\nPosition\nTeam\nGP\nG\nA\nTOI\nTOI_GP\nP\n\n\n\n\n426\n427\n17.06.1993\nMaykop\nRUS\nRUS\n71\n178\nKucherov\nNikita\nRW\nT.B\n74\n40\n45\n86320\n19.43\n85\n\n\n802\n803\n13.12.1991\nYaroslavl\nRUS\nRUS\n72\n219\nTarasenko\nVladimir\nRW\nSTL\n82\n39\n36\n90872\n18.47\n75\n\n\n607\n608\n30.10.1991\nKorkino\nRUS\nRUS\n71\n170\nPanarin\nArtemi\nLW/C\nCHI\n82\n31\n43\n95798\n19.47\n74\n\n\n480\n481\n31.07.1986\nMagnitogorsk\nRUS\nRUS\n75\n195\nMalkin\nEvgeni\nC/RW\nPIT\n62\n33\n39\n69263\n18.62\n72\n\n\n600\n601\n17.09.1985\nMoscow\nRUS\nRUS\n75\n239\nOvechkin\nAlex\nLW/RW\nWSH\n82\n33\n36\n90361\n18.37\n69\n\n\n433\n434\n19.05.1992\nChelyabinsk\nRUS\nRUS\n74\n192\nKuznetsov\nEvgeny\nC/LW\nWSH\n82\n19\n40\n83410\n16.95\n59\n\n\n657\n658\n05.07.1986\nNizhny Tagil\nRUS\nRUS\n74\n205\nRadulov\nAlex\nRW\nMTL\n76\n18\n36\n83405\n18.30\n54\n\n\n11\n12\n24.05.1988\nYaroslavl\nRUS\nRUS\n76\n198\nAnisimov\nArtem\nC/LW\nCHI\n64\n22\n23\n68529\n17.85\n45\n\n\n489\n490\n20.12.1978\nVoskresensk\nRUS\nRUS\n72\n200\nMarkov\nAndrei\nD\nMTL\n62\n6\n30\n81230\n21.83\n36\n\n\n879\n880\n29.10.1991\nMoscow\nRUS\nRUS\n74\n195\nZaitsev\nNikita\nD\nTOR\n82\n4\n32\n108332\n21.85\n36\n\n\n595\n596\n23.07.1991\nNovokuznetsk\nRUS\nRUS\n71\n212\nOrlov\nDmitry\nD\nWSH\n82\n6\n27\n96107\n19.53\n33\n\n\n646\n647\n13.01.1997\nYaroslavl\nRUS\nRUS\n73\n201\nProvorov\nIvan\nD\nPHI\n82\n6\n24\n108132\n21.98\n30\n\n\n557\n558\n22.11.1992\nZhukovskiy\nRUS\nRUS\n71\n180\nNamestnikov\nVladislav\nC/LW\nT.B\n74\n10\n18\n65645\n14.78\n28\n\n\n293\n294\n16.05.1994\nKhabarovsk\nRUS\nRUS\n75\n209\nGrigorenko\nMikhail\nC\nCOL\n75\n10\n13\n63401\n14.08\n23\n\n\n429\n430\n14.07.1986\nMagnitogorsk\nRUS\nRUS\n73\n225\nKulemin\nNikolay\nLW/RW\nNYI\n72\n12\n11\n59691\n13.82\n23\n\n\n87\n88\n17.04.1995\nCherepovets\nRUS\nRUS\n74\n193\nBuchnevich\nPavel\nRW/LW\nNYR\n41\n8\n12\n32616\n13.25\n20\n\n\n566\n567\n28.03.1993\nChelyabinsk\nRUS\nRUS\n71\n191\nNesterov\nNikita\nD\nMTL/T.B\n48\n4\n13\n46901\n16.28\n17\n\n\n90\n91\n21.10.1991\nKazan\nRUS\nRUS\n73\n180\nBurmistrov\nAlex\nC/RW\nARI/WPG\n49\n5\n11\n39266\n13.35\n16\n\n\n828\n829\n19.07.1983\nIzhevsk\nRUS\nRUS\n74\n221\nTyutin\nFedor\nD\nCOL\n69\n1\n12\n78405\n18.93\n13\n\n\n27\n28\n14.12.1995\nMoscow\nRUS\nRUS\n72\n180\nBarbashev\nIvan\nC\nSTL\n30\n5\n7\n21224\n11.78\n12\n\n\n749\n750\n13.05.1994\nPenza\nRUS\nRUS\n74\n218\nSlepyshev\nAnton\nLW\nEDM\n41\n4\n6\n27342\n11.12\n10\n\n\n878\n879\n16.04.1995\nMoscow\nRUS\nRUS\n77\n230\nZadorov\nNikita\nD\nCOL\n56\n0\n10\n63960\n19.03\n10\n\n\n207\n208\n25.04.1986\nTogliatti\nRUS\nRUS\n74\n218\nEmelin\nAlexei\nD\nMTL\n76\n2\n8\n97227\n21.32\n10\n\n\n763\n764\n14.10.1993\nNizhny Tagil\nRUS\nRUS\n71\n190\nSoshnikov\nNikita\nRW\nTOR\n56\n5\n4\n36450\n10.70\n9\n\n\n875\n876\n06.10.1993\nNizhnekamsk\nRUS\nRUS\n71\n195\nYakupov\nNail\nRW/LW\nSTL\n40\n3\n6\n25553\n10.67\n9\n\n\n824\n825\n30.08.1994\nYekaterinburg\nRUS\nRUS\n79\n265\nTryamkin\nNikita\nD\nVAN\n66\n2\n7\n66291\n16.73\n9\n\n\n486\n487\n02.01.1992\nMoscow\nRUS\nRUS\n75\n210\nMarchenko\nAlexey\nD\nDET/TOR\n41\n1\n7\n41700\n16.95\n8\n\n\n473\n474\n06.01.1992\nTver\nRUS\nRUS\n74\n207\nLyubimov\nRoman\nLW\nPHI\n47\n4\n2\n27011\n9.58\n6\n\n\n430\n431\n29.10.1990\nLipetsk\nRUS\nRUS\n73\n204\nKulikov\nDmitry\nD\nBUF\n47\n2\n3\n61766\n21.90\n5\n\n\n385\n386\n17.03.1991\nOmsk\nRUS\nRUS\n75\n200\nKalinin\nSergey\nC/RW\nN.J\n43\n2\n2\n33008\n12.63\n4\n\n\n272\n273\n07.10.1995\nMoscow\nRUS\nRUS\n71\n185\nGoldobin\nNikolay\nRW/LW\nS.J/VAN\n14\n3\n0\n9551\n11.37\n3\n\n\n707\n708\n30.12.1995\nMoscow\nRUS\nRUS\n74\n190\nScherbak\nNikita\nRW\nMTL\n3\n1\n0\n2048\n11.38\n1\n\n\n819\n820\n03.02.1995\nMoscow\nRUS\nRUS\n68\n170\nTolchinsky\nSergey\nLW\nCAR\n2\n0\n1\n1377\n11.48\n1\n\n\n887\n888\n15.05.1995\nSt. Petersburg\nRUS\nRUS\n73\n224\nZykov\nValentin\nLW\nCAR\n2\n1\n0\n750\n6.25\n1\n\n\n724\n725\n25.06.1998\nNizhnekamsk\nRUS\nRUS\n75\n215\nSergachev\nMikhail\nD\nMTL\n4\n0\n0\n2910\n12.13\n0\n\n\n799\n800\n31.10.1996\nYuzhno-Sakhalinsk\nRUS\nRUS\n75\n205\nSvechnikov\nEvgeny\nRW/LW\nDET\n2\n0\n0\n1577\n13.15\n0\n\n\n303\n304\n07.06.1997\nTogliatti\nRUS\nRUS\n75\n200\nGurianov\nDenis\nRW\nDAL\n1\n0\n0\n786\n13.10\n0\n\n\n386\n387\n12.08.1996\nOrsk\nRUS\nRUS\n74\n194\nKamenev\nVladislav\nLW/C\nNSH\n2\n0\n0\n1207\n10.07\n0"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q2g.",
    "href": "danl-hw/danl-m1-hw-1.html#q2g.",
    "title": "Homework Assignment 1",
    "section": "Q2g.",
    "text": "Q2g.\n\nHow many points (“P”) did he have?\n\n\nq2g = (\n  q2b[q2b['Nat'] == 'RUS'] \n  .sort_values(by='P', ascending=False)\n  .head(1)[['Last_Name', 'First_Name', 'P']]\n  )\n  \nq2g\n\n\n\n\n\n\n\n\nLast_Name\nFirst_Name\nP\n\n\n\n\n426\nKucherov\nNikita\n85\n\n\n\n\n\n\n\n\nThe above code creates a new DataFrame called q2g, which:\n\nFilters q2b for players whose nationality is ‘RUS’\nSorts the filtered DataFrame by the ‘P’ column in descending order\nSelects the top row of the sorted DataFrame using the .head(1) method\nFilters the columns ‘Last_Name’, ‘First_Name’, and ‘P’ from the selected row"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q2h.",
    "href": "danl-hw/danl-m1-hw-1.html#q2h.",
    "title": "Homework Assignment 1",
    "section": "Q2h.",
    "text": "Q2h.\n\nHow well did he perform in the entire league? Put differently, what was his rank in terms of points?\n\n\nq2h = (\n     q2b.assign(ranking = q2b['P'].rank(method = 'min', ascending=False))  # Create a new column 'ranking' based on the values in column 'P'\n        .sort_values(by='P', ascending=False)  # Sort the DataFrame by 'P' column in descending order\n        .loc[q2b['Nat'] == 'RUS']  # Filter the rows where the 'Nat' column is 'RUS'\n        .head(1)[['ranking', 'Last_Name', 'First_Name', 'P']]  # Select the top row and specific columns\n        )\n\nq2h\n\n\n\n\n\n\n\n\nranking\nLast_Name\nFirst_Name\nP\n\n\n\n\n426\n5.0\nKucherov\nNikita\n85"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q2i.",
    "href": "danl-hw/danl-m1-hw-1.html#q2i.",
    "title": "Homework Assignment 1",
    "section": "Q2i.",
    "text": "Q2i.\n\nFind the top ten scorers (in terms of points) and print them including their number of point and their respective team.\n\n\nq2i = ( q2b.assign( ranking = q2b['P'].rank(ascending=False) )   # add a new column 'ranking' based on the 'P' column's rank\n           .sort_values(by='P', ascending=False)                  # sort by 'P' column in descending order\n           )\n\n( q2i.loc[q2i['ranking'] &lt;= 10]                                    # select rows where 'ranking' is less than or equal to 10\n            [['ranking', 'Last_Name', 'First_Name', 'P', 'Team']]  # select specific columns\n            )\n\n\n\n\n\n\n\n\nranking\nLast_Name\nFirst_Name\nP\nTeam\n\n\n\n\n509\n1.0\nMcDavid\nConnor\n100\nEDM\n\n\n149\n2.5\nCrosby\nSidney\n89\nPIT\n\n\n389\n2.5\nKane\nPatrick\n89\nCHI\n\n\n21\n4.0\nBackstrom\nNicklas\n86\nWSH\n\n\n426\n5.5\nKucherov\nNikita\n85\nT.B\n\n\n485\n5.5\nMarchand\nBrad\n85\nBOS\n\n\n704\n7.0\nScheifele\nMark\n82\nWPG\n\n\n184\n8.0\nDraisaitl\nLeon\n77\nEDM\n\n\n91\n9.0\nBurns\nBrent\n76\nS.J\n\n\n802\n10.0\nTarasenko\nVladimir\n75\nSTL"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q2j.",
    "href": "danl-hw/danl-m1-hw-1.html#q2j.",
    "title": "Homework Assignment 1",
    "section": "Q2j.",
    "text": "Q2j.\n\nWhat are the three countries with the most players originating from?\n\n\n# Create a dataframe of count of players by nationality\nq2j = q2b[['Nat']].value_counts().reset_index()\n\n# Rename the columns of the dataframe\nq2j.columns = ['Nat', 'counts']\n\n# Create a new column 'ranking' with the ranking of each nationality by count\nq2j = ( q2j.assign( ranking = q2j['counts']\n                             .rank(method = 'dense', ascending=False) )\n           \n           # Filter the dataframe to only include the top 3 nationalities by count\n           .query('ranking &lt;= 3' ) \n      )"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q3a.",
    "href": "danl-hw/danl-m1-hw-1.html#q3a.",
    "title": "Homework Assignment 1",
    "section": "Q3a.",
    "text": "Q3a.\n\nFor each type of mine, calculate the total coal production for each pair of state-year.\n\n\n# Create a new column 'production' which is the sum of two other columns\ncoal['production'] = coal['production_underground'] + coal['production_surface']\n\n# Group the data by 'state' and 'year' columns and calculate the sum of 'production', 'production_underground', and 'production_surface'\nq3a = (\n       coal[ ['state', 'year', 'production',\n             'production_underground',\n             'production_surface'] ]\n       .groupby(['state', 'year'])\n       .sum()\n       )\n\nq3a\n\n\n\n\n\n\n\n\n\nproduction\nproduction_underground\nproduction_surface\n\n\nstate\nyear\n\n\n\n\n\n\n\nAlabama\n2011\n19071\n10878\n8193\n\n\n2012\n19320\n12569\n6751\n\n\n2013\n18620\n13515\n5105\n\n\n2014\n16363\n12517\n3846\n\n\n2015\n13191\n9897\n3294\n\n\n...\n...\n...\n...\n...\n\n\nWyoming\n2014\n395665\n3370\n392295\n\n\n2015\n375773\n3090\n372683\n\n\n2016\n297218\n1167\n296051\n\n\n2017\n316454\n1716\n314738\n\n\n2018\n304188\n2210\n301978\n\n\n\n\n197 rows × 3 columns"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q3b.",
    "href": "danl-hw/danl-m1-hw-1.html#q3b.",
    "title": "Homework Assignment 1",
    "section": "Q3b.",
    "text": "Q3b.\n\nFind the top 5 coal-producing states for each year.\n\n\n# Creates a new dataframe q3b\nq3b = (\n       # Reset the index of q3a\n       q3a.reset_index()\n       # Sort the values of q3a by the production column in descending order\n       .sort_values(['production'], ascending = False)\n       # Group q3a by year and get the head with the largest production for each year\n       .groupby('year')\n       .head()\n       # Sort q3b first by year in ascending order and then by production in descending order\n       .sort_values(['year','production'], ascending = [True, False])\n       )\n\nq3b\n\n\n\n\n\n\n\n\nstate\nyear\nproduction\nproduction_underground\nproduction_surface\n\n\n\n\n189\nWyoming\n2011\n438673\n3043\n435630\n\n\n181\nWest Virginia\n2011\n130186\n80403\n49783\n\n\n61\nKentucky\n2011\n108768\n65250\n43518\n\n\n141\nPennsylvania\n2011\n59183\n47318\n11865\n\n\n157\nTexas\n2011\n45903\n0\n45903\n\n\n190\nWyoming\n2012\n401442\n4637\n396805\n\n\n182\nWest Virginia\n2012\n115247\n76281\n38966\n\n\n62\nKentucky\n2012\n90865\n58200\n32665\n\n\n142\nPennsylvania\n2012\n54719\n45042\n9677\n\n\n40\nIllinois\n2012\n48485\n42835\n5650\n\n\n191\nWyoming\n2013\n387924\n4443\n383481\n\n\n183\nWest Virginia\n2013\n106868\n74105\n32763\n\n\n63\nKentucky\n2013\n80379\n54621\n25758\n\n\n143\nPennsylvania\n2013\n54007\n45165\n8842\n\n\n41\nIllinois\n2013\n52148\n46447\n5701\n\n\n192\nWyoming\n2014\n395665\n3370\n392295\n\n\n184\nWest Virginia\n2014\n107566\n77074\n30492\n\n\n64\nKentucky\n2014\n77337\n52809\n24528\n\n\n144\nPennsylvania\n2014\n60913\n52913\n8000\n\n\n42\nIllinois\n2014\n57969\n52714\n5255\n\n\n193\nWyoming\n2015\n375773\n3090\n372683\n\n\n185\nWest Virginia\n2015\n91174\n71098\n20076\n\n\n65\nKentucky\n2015\n61426\n43379\n18047\n\n\n43\nIllinois\n2015\n56100\n51973\n4127\n\n\n145\nPennsylvania\n2015\n50031\n43894\n6137\n\n\n194\nWyoming\n2016\n297218\n1167\n296051\n\n\n186\nWest Virginia\n2016\n76116\n61316\n14800\n\n\n146\nPennsylvania\n2016\n45718\n41385\n4333\n\n\n44\nIllinois\n2016\n43423\n41258\n2165\n\n\n66\nKentucky\n2016\n42868\n32713\n10155\n\n\n195\nWyoming\n2017\n316454\n1716\n314738\n\n\n187\nWest Virginia\n2017\n87800\n69175\n18625\n\n\n147\nPennsylvania\n2017\n49084\n43586\n5498\n\n\n45\nIllinois\n2017\n48204\n44905\n3299\n\n\n67\nKentucky\n2017\n41786\n31512\n10274\n\n\n196\nWyoming\n2018\n304188\n2210\n301978\n\n\n188\nWest Virginia\n2018\n89876\n69366\n20510\n\n\n148\nPennsylvania\n2018\n49882\n44633\n5249\n\n\n46\nIllinois\n2018\n49564\n46073\n3491\n\n\n68\nKentucky\n2018\n39569\n30964\n8605"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html#q3c.",
    "href": "danl-hw/danl-m1-hw-1.html#q3c.",
    "title": "Homework Assignment 1",
    "section": "Q3c.",
    "text": "Q3c.\n\nVisualize the yearly trend of the total coal production from each type of mine.\n\n\n# selecting the columns 'year', 'production_underground', and 'production_surface' from the original DataFrame coal, grouping them by year, and summing the values of each group.\nq3c = (\n  coal[['year','production_underground', 'production_surface']]\n    .groupby(['year']).sum()\n    )\n\n\n# create a line plot for the 'production_underground' column of the q3c DataFrame.\nq3c['production_underground'].plot()\n\n&lt;Axes: xlabel='year'&gt;\n\n\n\n\n\n\n# create a line plot for the 'production_surface' column of the q3c DataFrame.\nq3c['production_surface'].plot()\n\n&lt;Axes: xlabel='year'&gt;\n\n\n\n\n\n\n# create a line plot for the 'production_underground' and 'production_surface' columns of the q3c DataFrame on the same graph.\nq3c.plot()\n\n&lt;Axes: xlabel='year'&gt;"
  },
  {
    "objectID": "listing-danl-m1-cw.html",
    "href": "listing-danl-m1-cw.html",
    "title": "DANL Module 1 - Classwork",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nClasswork 7\n\n\nDates and Times\n\n\nMarch 19, 2023\n\n\n\n\nClasswork 6\n\n\nData Concatenates and Merges\n\n\nMarch 12, 2023\n\n\n\n\nClasswork 5\n\n\nTidy Data\n\n\nMarch 5, 2023\n\n\n\n\nClasswork 4\n\n\nExploratory Data Analysis\n\n\nFebruary 27, 2023\n\n\n\n\nClasswork 3\n\n\nGroup Operations\n\n\nFebruary 20, 2023\n\n\n\n\nClasswork 2\n\n\nPandas Basics\n\n\nFebruary 13, 2023\n\n\n\n\nClasswork 1\n\n\nPython Basics\n\n\nFebruary 6, 2023\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-02.html",
    "href": "danl-qa/danl-m1-qa-02.html",
    "title": "Lecture 2 - Discussion and Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-05.html",
    "href": "danl-qa/danl-m1-qa-05.html",
    "title": "Lecture 5 - Discussion and Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-06.html",
    "href": "danl-qa/danl-m1-qa-06.html",
    "title": "Lecture 6 - Discussion and Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geneseo Data Analytics Microcredential, Spring 2024",
    "section": "",
    "text": "Welcome! 👋\n\\(-\\) Explore, Learn, and Grow with the DANL Microcredential! 🌟"
  },
  {
    "objectID": "index.html#bullet-lecture",
    "href": "index.html#bullet-lecture",
    "title": "Geneseo Data Analytics Microcredential, Spring 2024",
    "section": "\\(\\bullet\\,\\) Lecture 🚀",
    "text": "\\(\\bullet\\,\\) Lecture 🚀\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nLecture 7\n\n\nMarch 19, 2023\n\n\n\n\nLecture 6\n\n\nMarch 12, 2023\n\n\n\n\nLecture 5\n\n\nMarch 5, 2023\n\n\n\n\nLecture 4\n\n\nFebruary 27, 2023\n\n\n\n\nLecture 3\n\n\nFebruary 20, 2023\n\n\n\n\nLecture 2\n\n\nFebruary 13, 2023\n\n\n\n\nLecture 1\n\n\nFebruary 6, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-q-a",
    "href": "index.html#bullet-q-a",
    "title": "Geneseo Data Analytics Microcredential, Spring 2024",
    "section": "\\(\\bullet\\,\\) Q & A ❓",
    "text": "\\(\\bullet\\,\\) Q & A ❓\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nLecture 7 - Discussion and Q & A\n\n\nMarch 19, 2023\n\n\n\n\nLecture 6 - Discussion and Q & A\n\n\nMarch 12, 2023\n\n\n\n\nLecture 5 - Discussion and Q & A\n\n\nMarch 5, 2023\n\n\n\n\nLecture 4 - Discussion and Q & A\n\n\nFebruary 27, 2023\n\n\n\n\nLecture 3 - Discussion and Q & A\n\n\nFebruary 20, 2023\n\n\n\n\nLecture 2 - Discussion and Q & A\n\n\nFebruary 13, 2023\n\n\n\n\nLecture 1 - Discussion and Q & A\n\n\nFebruary 6, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-classwork",
    "href": "index.html#bullet-classwork",
    "title": "Geneseo Data Analytics Microcredential, Spring 2024",
    "section": "\\(\\bullet\\,\\) Classwork ⌨️",
    "text": "\\(\\bullet\\,\\) Classwork ⌨️\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nClasswork 7\n\n\nMarch 19, 2023\n\n\n\n\nClasswork 6\n\n\nMarch 12, 2023\n\n\n\n\nClasswork 5\n\n\nMarch 5, 2023\n\n\n\n\nClasswork 4\n\n\nFebruary 27, 2023\n\n\n\n\nClasswork 3\n\n\nFebruary 20, 2023\n\n\n\n\nClasswork 2\n\n\nFebruary 13, 2023\n\n\n\n\nClasswork 1\n\n\nFebruary 6, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-homework",
    "href": "index.html#bullet-homework",
    "title": "Geneseo Data Analytics Microcredential, Spring 2024",
    "section": "\\(\\bullet\\,\\) Homework 💻",
    "text": "\\(\\bullet\\,\\) Homework 💻\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nHomework Assignment 6\n\n\nMarch 12, 2023\n\n\n\n\nHomework Assignment 5\n\n\nMarch 5, 2023\n\n\n\n\nHomework Assignment 4\n\n\nFebruary 27, 2023\n\n\n\n\nHomework Assignment 3\n\n\nFebruary 20, 2023\n\n\n\n\nHomework Assignment 2\n\n\nFebruary 13, 2023\n\n\n\n\nHomework Assignment 1\n\n\nFebruary 6, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-1",
    "title": "Lecture 7",
    "section": "Missing Data",
    "text": "Missing Data\nRarely will we be given a data set without any missing values.\nPandas usually displays missing values as NaN. - NaN is the actual representation of “Not a Number” values. - I would prefer calling it “Not Available” (NA), for which some other programming languages as well as Pandas use to represent missing values."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-2",
    "title": "Lecture 7",
    "section": "Missing Data",
    "text": "Missing Data\n\nThe NaN value in Pandas comes from Numpy.\n\nfrom numpy import NaN\nNaN == True\nNaN == 0\nNaN == \"\nNaN == NaN"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-3",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-3",
    "title": "Lecture 7",
    "section": "Missing Data",
    "text": "Missing Data\n\nPandas has functions to test for missing values, isnull().\nPandas also has functions for testing non-missing values, notnull().\n\nimport pandas as pd\npd.isnull(NaN)\npd.notnull(NaN)\npd.notnull(210)\npd.notnull('missing')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from",
    "title": "Lecture 7",
    "section": "Where Can Missing Values Come From?",
    "text": "Where Can Missing Values Come From?\nLoad Data\n\nread_csv()na_valueskeep_default_na\n\n\n\nWhen we load the data, Pandas automatically finds the missing data cell and give us a DataFrame with the NaN value in the appropriate cell.\nIn the read_csv() function, three parameters are related to reading missing values: na_values and keep_default_na.\n\n\n\nThe na_values parameter allows us to specify additional missing or NaN values.\n\nWe can pass in either a Python str (i.e., string) or a list-like object to be automatically coded as missing values when the file is read.\n\npath = 'https://bcdanl.github.io/data/survey_visited.csv'\nsurvey_visited_0 = pd.read_csv(path)\nsurvey_visited_1 = pd.read_csv(path, na_values = [\"MSK-4\"])\n\n\nThe keep_default_na parameter is a bool (i.e., True or False boolean) that allows us to specify whether any additional values need to be considered as missing.\n\nkeep_default_na = False will only use the missing values specified in na_values.\n\nsurvey_visited_2 = pd.read_csv(path, keep_default_na = False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-1",
    "title": "Lecture 7",
    "section": "Where Can Missing Values Come From?",
    "text": "Where Can Missing Values Come From?\nMerged Data\n\nLet’s merge survey_visited_0 with survey.\n\npath = 'https://bcdanl.github.io/data/survey_survey.csv'\nsurvey = pd.read_csv(path)\nsurvey\n\nvs = survey_visited_0.merge(survey, left_on='ident', right_on='taken')\nvs"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-2",
    "title": "Lecture 7",
    "section": "Where Can Missing Values Come From?",
    "text": "Where Can Missing Values Come From?\nReindexing\n\n(1)(2)\n\n\nAnother way to introduce missing values into our data is to reindex our dataframe.\n\nThis is useful when we want to add new indices to your dataframe, but still want to retain its original values.\nA common usage is when the index represents some time interval, and we want to add more dates.\n\ngapminder = pd.read_csv('https://bcdanl.github.io/data/gapminder.tsv', sep='\\t')\nlife_exp = gapminder.groupby(['year'])['lifeExp'].mean()\n\n\n\nWe can reindex the dataframe by using the .reindex() method.\n\n## subset\ny2000 = life_exp[life_exp.index &gt; 2000]\n\n## reindexing\ny2000.reindex(range(2000, 2010))"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data",
    "title": "Lecture 7",
    "section": "Working With Missing Data",
    "text": "Working With Missing Data\nFind and Count Missing Data\n\n.count()np.count_nonzero().value_counts(dropna=False).sum()\n\n\nOne way to look at the number of missing values is to count() them.\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv')\n## count the number of non-missing values\nebola.count()\nQ. Count the number of non-missing values for each variable in ebola.\n\n\nIf we want to count the total number of missing values in our DataFrame, or count the number of missing values for a particular column, we can use the np.count_nonzero() function from numpy in conjunction with the .isnull() method.\nnp.count_nonzero(ebola.isnull())\nnp.count_nonzero(ebola['Cases_Guinea'].isnull())\n\n\nAnother way to get missing data counts is to use the .value_counts() method, giving a frequency table of values in a Series. - If we use the dropna = False, we can also get a missing value count.\ncnts = ebola['Cases_Guinea'].value_counts(dropna=False)\ncnts\n\nThe results are sorted so we can subset the count vector to just look at the missing values.\n\ncnts.loc[pd.isnull(cnts.index)]\n\n\n\nIn Python, True values equate to the integer value 1, and False values equate to the integer value 0.\n\nWe can use this behavior to get the number of missing values by summing up a boolean vector with the .sum() method.\n\n\nebola.Cases_Guinea.isnull().sum()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-1",
    "title": "Lecture 7",
    "section": "Working With Missing Data",
    "text": "Working With Missing Data\nClean Missing Data\n\nCleaning NaN.fillna().fillna(method=‘ffill’).fillna(method=‘bfill’).interpolate().dropna()Calculationskipna\n\n\nThere are many different ways we can deal with missing data. 1. We can replace the missing value with another value, 2. We can fill in the cells with the missing value using existing value, 3. We can drop the observations with missing values from our DataFrame.\n\n\n\nWe can use the .fillna() method to recode the missing values to another value.\n\n## fill the missing values to 0\nebola0 = ebola.fillna(0)\n\n\n\nWe can use built-in methods to fill forward (method = ffill).\n\nWhen we fill data forward, the last known value (from top to bottom) is used for the next missing value.\n\n\nebola_f = ebola.fillna(method='ffill')\n\n\n\nWe can also use built-in methods to fill backward (method = bfill).\n\nWhen we fill data backward, the newest value (from top to bottom) is used to replace the missing data.\n\n\nebola_b = ebola.fillna(method='bfill')\n\n\n\nInterpolation uses existing values to fill in missing values.\n\nBy default, .interpolate() treats the missing values as if they should be equally spaced apart.\n\nebola_linear = ebola.interpolate()\nThe .interpolate() method behaves kind of in a forward fill fashion.\n\n\n\nIf we want to keep the observations with only non-missing values, we can use .dropna()\nebola_dropna = ebola.dropna()\nWe are left with just one row of data!\n\n\nSuppose we wanted to look at the case counts for multiple regions.\n\n\nebola[\"Cases_multiple\"] = (\n  ebola[\"Cases_Guinea\"]\n  + ebola[\"Cases_Liberia\"]\n  + ebola[\"Cases_SierraLeone\"]\n)\n\nebola_subset = ebola.loc[:,\n    [\"Cases_Guinea\", \n     \"Cases_Liberia\", \n     \"Cases_SierraLeone\",\n     \"Cases_multiple\"] ]\n\n\n\n\n.mean() and .sum() can ignore missing values. - These functions will typically have a skipna parameter that will still calculate a value by skipping over the missing values.\nebola.Cases_Guinea.sum(skipna = True) ## default\nebola.Cases_Guinea.sum(skipna = False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-2",
    "title": "Lecture 7",
    "section": "Working With Missing Data",
    "text": "Working With Missing Data\nPandas Built-In NA Missing\nPandas 1.0 introduced a built-in &lt;NA&gt; value (pd.NA).\n\neconomists = pd.DataFrame(\n  {\n    \"Name\": [\"John Forbes Nash\", \"William Nordhaus\"],\n    \"Occupation\": [\"Mathematician\", \"Climate Economist\"],\n    \"Born\": [\"1928-06-13\", \"1941-05-31\"],\n    \"Died\": [\"2015-05-23\", \"\"],\n    \"Age\": [86, 81]\n  }\n)\n\neconomists.loc[1, \"Age\"] = pd.NA"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nPython’s datetime Object\nOne of the bigger reasons for using Pandas is its ability to work with timeseries data.\n\nWe can use datetime to get the current date and time.\n\nfrom datetime import datetime\nnow = datetime.now()\n\nWe can also create our own datetime manually.\n\n\nt1 = datetime.now()\nt2 = datetime(2000,1,1)\n\ndiff = t1 - t2\ntype(diff)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-1",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nConverting to datetime\n\ndata.to_datatime()formatparse_dates\n\n\n\nLet’s load up our Ebola data set and convert the Date column into a proper datetime object.\n\nimport pandas as pd\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv')\nebola.info()\n\nThe Date column is encoded as a generic string object.\n\n\n\n\nWe can use .to_datatime() to create a new column, date_dt, that converts the Date column into a datetime.\n\nebola['date_dt'] = pd.to_datetime(ebola['Date'])\n\n\n\nThe to_datetime() method has a parameter called format that allows us to manually specify the format of the date.\n\n\n\nThe read_csv() function has several parameters about datetime. - We can parse the Date column directly by specifying the column we want in the parse_dates parameter.\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv', parse_dates=[\"Date\"])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-2",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nExtracting Date Components\nNow that we have a datetime object, we can extract various parts of the date, such as year, month, or day.\nLet’s consider the example datetime object.\nd = pd.to_datetime('2021-12-14')\ntype(d)\nd.year\nd.month\nd.day\nd.quarter"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-3",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-3",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nExtracting Date Components in DataFrame\n\nWe can extract various parts of the datetime column in DataFrame by accessing datetime methods using the .dt accessor.\n\nebola['date_dt'] = pd.to_datetime(ebola['Date'])\n\nebola = ebola.assign(\n    year = ebola[\"date_dt\"].dt.year,\n    month = ebola[\"date_dt\"].dt.month,\n    day = ebola[\"date_dt\"].dt.day\n)\n\nebola.info() ## what are the data types of year, month, and day?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-4",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-4",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nDate Ranges\n\n(1)(2)(3)\n\n\n\nIn our Ebola data set, we do not have an observation for every day in the date range.\n\nThis is quite common.\n\n\nebola = pd.read_csv(\n'https://bcdanl.github.io/data/country_timeseries.csv', parse_dates=[\"Date\"]\n)\n\nHere, 2015-01-01 is missing.\n\n\n\n\nIt’s common practice to create a date range to .reindex() a data set.\n\nWe can use the date_range().\n\n\nhead_range = pd.date_range(start='2014-12-31', end='2015-01-05')\nebola_5 = ebola.head()\nebola_5.index = ebola_5['Date']\nebola_5 = ebola_5.reindex(head_range)\n\n\nebola = pd.read_csv(\n  \"https://bcdanl.github.io/data/country_timeseries.csv\",\n  index_col=\"Date\",\n  parse_dates=[\"Date\"],\n)\n\nnew_idx = pd.date_range(ebola.index.min(), ebola.index.max())\nnew_idx = reversed(new_idx) ## to reverse new_idx\nebola = ebola.reindex(new_idx)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-1",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\n\n\nIn a tidy data.frame,\n\nA variable is in a column.\nAn observation is in a row.\nA value are in a cell.\n\n\nTidy data is a framework to structure data sets so they can be easily analyzed and visualized."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-2",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Values, Not Variables\nData can have columns that contain values instead of variables.\n\nLet’s consider data on income and religion in the United States from the Pew Research Center\n\nimport pandas as pd\npew = pd.read_csv('https://bcdanl.github.io/data/pew.csv')\n\nNot every column here is a variable.\n\nThe values that relate to income are spread across multiple columns.\n\n\nFor data analysis, the pew can be reshaped so that we have religion, income, and count variables."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-3",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Values, Not Variables\n## Show only the first few columns\npew.iloc[:,0:5]\nThe form of the data like pew is known as “wide” data. - To turn it into the “long” tidy data format, we will have to melt our DataFrame."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-4",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-4",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nPandas DataFrames have a method called .melt() that will reshape the DataFrame into a tidy format and it takes a few parameters: - id_vars is a container (list, tuple, ndarray) that represents the variables that will remain as is. - value_vars identifies the columns you want to melt down (or unpivot). - By default, it will melt all the columns not specified in the id_vars parameter. - var_name is a string for the new column name when the value_vars is melted down. - By default, it will be called variable. - value_name is a string for the new column name that represents the values for the var_name. - By default, it will be called value."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-5",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-5",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Values, Not Variables\n\n(1)(2)\n\n\n## we do not need to specify a value_vars since we want to pivot\n## all the columns except for the 'religion' column\npew_long = pew.melt(id_vars='religion')\npew_long\n\n## The .melt() method also exists as a pandas function, pd.melt()\n## The below line of code is the equivalent one:\n## melt function\npew_long = pd.melt(pew, id_vars='religion')\n\n\n\nWe can change the defaults so that the melted/unpivoted columns are named.\n\npew_long = pew.melt(\n  id_vars =\"religion\", var_name=\"income\", value_name =\"count\"\n)\n\npew_long"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-6",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-6",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nKeep Multiple Columns Fixed\n\n(1)(2)\n\n\nNot every data set will have one column to hold still while we unpivot the rest of the columns. - Let’s consider the Billboard data set.\nbillboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')\nEach week has its own column. - What if we want to create a faceted plot of the weekly ratings?\n\n\n## use a list to reference more than 1 variable\nbillboard_long = billboard.melt(\n  id_vars = [\"year\", \"artist\", \"track\", \"time\", \"date.entered\"],\n  var_name = \"week\",\n  value_name = \"rating\",\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-7",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-7",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Multiple Variables\n\n(1)(2)(3)(4)(5)(6)(2)(3)(4)(5)\n\n\nSometimes columns in a data set may represent multiple variables. - Let’s look at the Ebola data set.\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv')\nebola.columns\nebola.iloc[ :5, [0, 1, 2, 10] ]\n\n\nThe column names Cases_Guinea and Deaths_Guinea actually contain two variables. - The individual status (cases and deaths, respectively) as well as the country name, Guinea. - The data is also arranged in a wide format that needs to be reshaped (with the .melt() method). - First, let’s fix the problem by melting the data into long format.\nebola_long = ebola.melt(id_vars=['Date', 'Day'])\n\n\n\nIn this case, we can split the column of interest based on the underscore, _.\n\nWe can use the .split() method that takes a string and “splits” it up based on a given delimiter.\nTo get access to the string methods, we need to use the .str. attribute.\n\n\n## split the column based on a delimiter\nvariable_split = ebola_long.variable.str.split('_')\nvariable_split\ntype(variable_split); type(variable_split[0])\n\n\n\nNow that the column has been split into various pieces, the next step is to assign those pieces to a new column.\n\nTo do so, we can use the .get() string method to “get” the index we want for each row.\n\n\nstatus_values = variable_split.str.get(0)\ncountry_values = variable_split.str.get(1)\n\n\n\nNow that we have the vectors we want, we can add them to our DataFrame.\n\nebola_long['status'] = status_values\nebola_long['country'] = country_values\nebola_long\n\n\n\nIn the .split() method, there is a parameter expand that defaults to False.\n\nWhen we set it to True, it will return a DataFrame where each result of the split is in separate columns. ```{.python} ## reset our ebola_long data ebola_long = ebola.melt(id_vars =[‘Date’, ‘Day’]) ## split the column by _ into a dataframe using expand variable_split = ebola_long.variable.str.split(’_’, expand=True)\n\n\nebola_long[[‘status’, ‘country’]] = variable_split\n\n\n\n::: \n\n\n\n## Tidy Data\n### Variables in Both Rows and Columns \n\n::: {.panel-tabset}\n## (1)\n\nWhat happens if a column of data actually holds two variables instead of one variable?\n  -  In this case, we will have to `pivot` the variable into separate columns, i.e., go from \"long\" data to \"wide\" data.\n\n```{.python}\nweather = pd.read_csv('https://bcdanl.github.io/data/weather.csv')\n\n\nThe weather data include minimum (tmin) and maximum (tmax) temperatures recorded for each day (d1, d2, … , d31) of the month (month). - The element column contains variables that may need to be pivoted wider to become new columns, - The day variables may need to be melted into row values.\nLet’s first fix the day values.\nweather_melt = weather.melt(\n  id_vars=[\"id\", \"year\", \"month\", \"element\"],\n  var_name=\"day\",\n  value_name=\"temp\",\n)\n\n\nNext, we need to pivot up the variables stored in the element column.\nweather_tidy = weather_melt.pivot_table(\n    index = ['id', 'year', 'month', 'day'],\n    columns = 'element',\n    values = 'temp'\n)\n\n\nWe can also flatten the hierarchical columns.\nweather_tidy_flat = weather_tidy.reset_index()\n\n\nFor day variable, we can replace ‘d’ with ““. - Then, convert day from string to integer. - Then, sort weather_tidy_flat by ['year', 'month', 'day'].\nweather_tidy_flat['day'] = weather_tidy_flat.day.str.replace('d', \"\")\nweather_tidy_flat['day'] = weather_tidy_flat['day'].astype(int)\nweather_tidy_flat = weather_tidy_flat.sort_values(by = ['year', 'month', 'day'])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-1",
    "title": "Lecture 6",
    "section": "Data Types",
    "text": "Data Types\nLet’s consider the built-in tips DataFrame from the seaborn library.\nTo get a list of the data types stored in each column of our DataFrame, we call the dtypes attribute.\nimport seaborn as sns\ntips = sns.load_dataset(\"tips\")\ntips.dtypes\n\nThe category data type represents categorical variables.\n\nIt differs from the generic object data type that stores arbitrary Python objects (usually strings)."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-2",
    "title": "Lecture 6",
    "section": "Data Types",
    "text": "Data Types\nConverting Types\n## convert the category sex column into a string dtype\ntips['sex_str'] = tips['sex'].astype(str)\n\n## convert total_bill into a string\ntips['total_bill'] = tips['total_bill'].astype(str)\n\n## convert it back to a float\ntips['total_bill'] = tips['total_bill'].astype(float)\n\n## convert sex into a string\ntips['sex'] = tips['sex'].astype(str)\n\n## convert it back to a category\ntips['sex'] = tips['sex'].astype(category)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-3",
    "title": "Lecture 6",
    "section": "Data Types",
    "text": "Data Types\nto_numeric() method\n\n(1)(2)(3)(4)\n\n\nWhen converting variables into numeric values (e.g., int, float), we can also use the Pandas to_numeric() function. - It handles non-numeric values better.\n\n## subset the tips data\ntips_sub_miss = tips.head(10).copy()\n## assign some 'missing' values\ntips_sub_miss.loc[[1, 3, 5, 7], 'total_bill'] = 'missing'\n\n\n## this will cause an error\ntips_sub_miss['total_bill'].astype(float)\n\n## this will cause an error\npd.to_numeric(tips_sub_miss['total_bill'])\n\n\nThe to_numeric() function has a parameter called errors that governs what happens when the function encounters a value that it is unable to convert to a numeric value. - 'raise': Default. - 'coerce': Invalid parsing will be set as NaN - 'ignore': Invalid parsing will return the input as is.\n\n\ntips_sub_miss[\"total_bill\"] = pd.to_numeric(\n    tips_sub_miss[\"total_bill\"], errors=\"ignore\"\n)\n\ntips_sub_miss[\"total_bill\"]=pd.to_numeric(\n    tips_sub_miss[\"total_bill\"], errors=\"coerce\"\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-assembly-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-assembly-1",
    "title": "Lecture 6",
    "section": "Data Assembly",
    "text": "Data Assembly\n\nSometimes, we better combine various DataFrames together to analyze a set of data.\n\nThe data may have been split up into separate DataFrames to reduce the amount of redundant information.\ne.g., DataFrame for county-level data and DataFrame for geographic information, such as longitude and latitude"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\n\nConcatenation can be thought of as appending a row or column to our data. - This approach is possible if our data was split into parts or if we performed a calculation that we want to append to our existing data set. - Let’s consider the following example DataFrames:\ndf1 = pd.read_csv('https://bcdanl.github.io/data/concat_1.csv')\ndf2 = pd.read_csv('https://bcdanl.github.io/data/concat_2.csv')\ndf3 = pd.read_csv('https://bcdanl.github.io/data/concat_3.csv')\n\nWe will be working with .index and .columns in this Section.\n\ndf1.index\ndf1.columns\ndf1.values"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-1",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\nConcatenating the DataFrames on top of each other uses the concat() function. - All of the DataFrames to be concatenated are passed in a list.\nrow_concat = pd.concat([df1, df2, df3])\nrow_concat\n\nThe row names (i.e., the row indices) are simply a stacked version of the original row indices.\n\nrow_concat.iloc[3, :]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-2",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\n\nLet’s consider a new Series and concatenate it with df1:\n\n## create a new row of data\nnew_row_series = pd.Series(['n1', 'n2', 'n3', 'n4'])\nnew_row_series\n\n\n## attempt to add the new row to a dataframe\ndf = pd.concat([df1, new_row_series])\ndf\n\nNot only did our code not append the values as a row, but it also created a new column completely misaligned with everything else.\nWhy?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-3",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\nTo fix the problem, we need turn our series into a DataFrame. - This data frame contains one row of data, and the column names are the ones the data will bind to.\nnew_row_df = pd.DataFrame(\n  ## note the double brackets to create a \"row\" of data\n  data =[[\"n1\", \"n2\", \"n3\", \"n4\"]],\n  columns =[\"A\", \"B\", \"C\", \"D\"],\n)\n\ndf = pd.concat([df1, new_row_df])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-4",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-4",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nIgnore the Index\n\nWe can use the ignore_index parameter to reset the row index after the concatenation if we simply want to concatenate or append data together.\n\nrow_concat_i = pd.concat([df1, df2, df3], ignore_index=True)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-5",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-5",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Columns\nConcatenating columns is very similar to concatenating rows. - The main difference is the axis parameter in the concat function. - The default value of axis is 0 (or axis = \"index\"), so it will concatenate data in a row-wise fashion. - If we pass axis = 1 (or axis = \"columns\") to the function, it will concatenate data in a column-wise manner.\ncol_concat = pd.concat([df1, df2, df3], axis = \"columns\")"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-6",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-6",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Columns\n\nAdding a single column to a dataframe can be done directly without using any specific Pandas function.\n\ncol_concat['new_col_list'] = ['n1', 'n2', 'n3', 'n4']\n\nWe can reset the column indices so we do not have duplicated column names.\n\npd.concat([df1, df2, df3], axis=\"columns\", ignore_index=True)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-7",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-7",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\nWhat would happen when the row and column indices are not aligned?\n\nLet’s modify our DataFrames for the next few examples.\n\n## rename the columns of our dataframes\ndf1.columns = ['A', 'B', 'C', 'D']\ndf2.columns = ['E', 'F', 'G', 'H']\ndf3.columns = ['A', 'C', 'F', 'H']\n\nIf we try to concatenate these DataFrames as we did, the DataFrames now do much more than simply stack one on top of the other.\n\nrow_concat = pd.concat([df1, df2, df3])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-8",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-8",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\n\nWe can set join = 'inner' to keep only the columns that are shared among the data sets.\n\npd.concat([df1, df2, df3], join ='inner')\n\nIf we use the DataFrames that have columns in common, only the columns that all of them share will be returned.\n\npd.concat([df1, df3], join ='inner',  ignore_index =False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-9",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-9",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\n\nLet’s modify our DataFrames further.\n\n## re-indexing the rows of our dataframes\ndf1.index = [0, 1, 2, 3]\ndf2.index = [4, 5, 6, 7]\ndf3.index = [0, 2, 5, 7]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-10",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-10",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\n\nWhen we concatenate along axis=\"columns\" (axis=1), the new DataFrames will be added in a column-wise fashion and matched against their respective row indices.\n\ncol_concat = pd.concat([df1, df2, df3], axis=\"columns\")\n\nJust as we did when we concatenated in a row-wise manner, we can choose to keep the results only when there are matching indices by using join=\"inner\".\n\npd.concat([df1, df3], axis =\"columns\", join='inner')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data",
    "title": "Lecture 6",
    "section": "Relational data",
    "text": "Relational data\nWhy is one data set sometimes scattered across multiple files? - The size of the files can be huge. - The data collection process can be scattered across time and space.\nSometimes we may have two or more DataFrames (tables) that we want to combine based on common data values. - This task is known in the database world as performing a “join.” - We can do this with the .merge() method in Pandas."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-1",
    "title": "Lecture 6",
    "section": "Relational data",
    "text": "Relational data\n\nThe variables that are used to connect each pair of tables are called keys."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-2",
    "title": "Lecture 6",
    "section": "Relational data",
    "text": "Relational data\nMerges\n\n\n\nx = pd.DataFrame({\n    'key': [1, 2, 3],\n    'val_x': ['x1', 'x2', 'x3']\n})\n\ny = pd.DataFrame({\n    'key': [1, 2, 4],\n    'val_y': ['y1', 'y2', 'y3']\n})\n\n\n\nThe colored column represents the “key” variable.\nThe grey column represents the “value” column."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-1",
    "title": "Lecture 6",
    "section": "Merges",
    "text": "Merges\n\ninnerleftrightouter full\n\n\n\nAn inner join matches pairs of observations whenever their keys are equal:\n\n\n\n\n\n\n\n\n\n\n## the default value for 'how' is 'inner'\n## so it doesn't actually need to be specified\nmerge_inner = pd.merge(x, y, on='key', how='inner')\nmerge_inner_x = x.merge(y, on='key', how='inner')\nmerge_inner_x_how = x.merge(y, on='key')\n\n\n\nA left join keeps all observations in x.\n\n\n\n\n\n\n\n\n\n\nmerge_left = pd.merge(x, y, on='key', how='left')\nmerge_left_x = x.merge(y, on='key', how='left')\n\nThe most commonly used join is the left join.\n\n\n\n\nA right join keeps all observations in y.\n\n\n\n\n\n\n\n\n\n\nmerge_right = pd.merge(x, y, on='key', how='right')\nmerge_right_x = x.merge(y, on='key', how='right')\n\n\n\nA full join keeps all observations in x and y.\n\n\n\n\n\n\n\n\n\n\nmerge_outer = pd.merge(x, y, on='key', how='outer')\nmerge_outer_x = x.merge(y, on='key', how='outer')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-2",
    "title": "Lecture 6",
    "section": "Merges",
    "text": "Merges\nDuplicate keys\n\none-to-manymany-to-many\n\n\n\nOne data frame has duplicate keys (a one-to-many relationship).\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = pd.DataFrame({\n    'key': [1, 2, 3],\n    'val_x': ['x1', 'x2', 'x3'] })\n\n\n\ny = pd.DataFrame({\n    'key': [1, 2, 4],\n    'val_y': ['y1', 'y2', 'y3'] })\none_to_many = x.merge(y, on='key', how='left')\n\n\n\n\n\n\nBoth data frames have duplicate keys (many-to-many relationship).\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = pd.DataFrame({\n  'key': [1, 2, 2, 3],\n  'val_x': ['x1', 'x2', 'x3', 'x4'] })\n\n\n\ny = pd.DataFrame({\n  'key': [1, 2, 2, 3],\n  'val_y': ['y1', 'y2', 'y3', 'y4'] })\nmany_to_many = x.merge(y, on='key', how='left')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-3",
    "title": "Lecture 6",
    "section": "Merges",
    "text": "Merges\nDefining the key columns\n\nIf the left and right columns do not have the same name for the key columns, we can use the left_on and right_on parameters instead.\n\n\n\n\nx = pd.DataFrame({\n  'key_x': [1, 2, 3],\n  'val_x': ['x1', 'x2', 'x3']\n})\n\n\n\ny = pd.DataFrame({\n  'key_y': [1, 2],\n  'val_y': ['y1', 'y2'] })\n\nkeys_xy = x.merge(y, \n                  left_on='key_x', \n                  right_on = 'key_y', \n                  how='left')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-libraries",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-libraries",
    "title": "Lecture 2",
    "section": "pandas Libraries",
    "text": "pandas Libraries\n\nPython is a general-purpose programming language and is not specialized for numerical or statistical computation.\nA library is a collection of packages that includes a bunch of related Python codes.\n\n\n\npandas is a Python library that provides high-performance data structures and data analysis tools:\n\nData manipulation and analysis\nDataFrame and Series objects\nExport and import data from files and web\nHandling of missing data"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#importing-pandas-libraries",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#importing-pandas-libraries",
    "title": "Lecture 2",
    "section": "Importing pandas Libraries",
    "text": "Importing pandas Libraries\n\nWe refer to code of libraries by using the Python import statement.\n\nThis makes the code and variables in the imported module available to our programming codes.\nWe can use the as keyword when importing the libraries using their canonical names.\n\n\nimport pandas as pd"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas",
    "title": "Lecture 2",
    "section": "Pandas",
    "text": "Pandas\nLoad Data Sets\n\nWhen given a data set, we first load it and begin looking at its structure and contents.\nThe simplest way of looking at a data set is to look at and subset specific rows and columns.\nWe can see what type of information is stored in each column, and can start looking for patterns by aggregating descriptive statistics."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-1",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-1",
    "title": "Lecture 2",
    "section": "Pandas",
    "text": "Pandas\nLoad Data Sets\n\nWith the library loaded we can use the read_csv() function to load a CSV data file.\nIn order to access the read_csv() function from pandas, we use something called “dot” notation.\n\nWe write pd.read_csv() to say: within the pandas library we just loaded, look inside for the read_csv() function.\n\n\n# *.tsv is a file of tab-separated values.\n# we can use the sep parameter and indicate a tab with \\t\ndf  = pd.read_csv('https://bcdanl.github.io/data/gapminder.tsv', sep='\\t')\ndf"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas---load-data-sets",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas---load-data-sets",
    "title": "Lecture 2",
    "section": "Pandas - Load Data Sets",
    "text": "Pandas - Load Data Sets\n\ntype().shape.columns.dtypes\n\n\n\nWe can check to see if we are working with a Pandas Dataframe by using the built-in type() function (i.e., it comes directly from Python, not a separate library such as Pandas).\n\ntype(df)\n\nThe type() function is handy when we begin working with many different types of Python objects and need to know what object we are currently working on.\n\n\n\n\nEvery DataFrame object has a .shape attribute that will give us the number of rows and columns of the DataFrame.\n\ndf.shape\n#  It’s written as df.shape and not df.shape()\n\nSince .shape is an attribute of the DataFrame object, and not a function or method of the DataFrame object, it does not have round parentheses after the period.\n\n\n\n\nTo get a gist of what information the data set contains, we look at the column names.\nThe column names are given using the .column attribute of the DataFrame object.\n\ndf.columns\n\nQ. What is the type of the column names?\n\n\n\n\nEach column (i.e., Series) has to be the same type, whereas each row can contain mixed types.\nWe can use the .dtypes attribute or the .info() method to see the type of each column in DataFrame.\n\ndf.dtypes\ndf.info()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSelect and Subset Columns by Name\n\nhead()[ ][ [] ]\n\n\n\nWe can use the .head() method of a DataFrame to look at the first 5 rows of our data.\n\nThis is useful to see if our data loaded properly, and to get a better sense of the columns and contents.\n\n\ndf.head()\n\n\n\nIf we want only a specific column from our data, we can access the data using square brackets, [ ].\n\n# just get the country column and save it to its own variable\ncountry_df = df['country']\n# show the first 5 observations\ncountry_df.head()\n\n\n\nIn order to specify multiple columns by the column name, we need to pass in a Python list between the square brackets.\n\n# Looking at country, continent, and year\nsubset = df[['country', 'continent', 'year']]\nsubset"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-1",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-1",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSingle Value Returns DataFrame or Series\n\nSeriesDataFrame\n\n\n\nWhen we first select a single column, a Series object comes out.\n\ncountry_df = df['country']\ntype(country_df)\ncountry_df\n\n\n\nCompare df['country'] with df[ ['country'] ]\n\ncountry_df_list = df[ ['country'] ]\ntype(country_df_list)\ncountry_df_list\n\nIf we use a list to subset, we will always get a DataFrame object back."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-2",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-2",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nUsing Dot Notation to Pull a Column of Values\n\nThere is a shorthand notation where we can pull the column by treating it as a DataFrame attribute, or using dot (.).\n\ndf['country']\ndf.country\n\nWe do have to be mindful of what our columns are named if we want to use the dot notation.\n\nIf there is a column named shape, the df.shape will return the number of rows and columns\nIf our column name has spaces or special characters, we will not be able to use the dot notation to select that column of values."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-3",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-3",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubset Rows\n\nRows can be subset in multiple ways, by row name or row index."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-4",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-4",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubset Rows by index Label: .loc[]\n\nIndex.loc[]last row.tail()multiple rows\n\n\n\nLet’s take a look at our gapminder data:\n\ndf\n\nWe can see on the left side of the printed DataFrame, what appear to be row numbers.\n\nThis column-less row of values is the “index” label of the DataFrame.\n\nBy default, Pandas fills in the index labels with the row numbers starting from 0.\n\n\n\n\nWe can use the .loc[] attribute on the DataFrame to subset rows based on the index label.\n\n:::: {.columns} ::: {.column width=“50%”}\n# get the first row\ndf.loc[0]\n\n# get the 100th row\ndf.loc[99]\n\n# get the last row\ndf.loc[ -1]\n\nDoes df.loc[ -1] work?\n\n\n\n\n\nHere, passing -1 to the .loc[] causes an error.\n\nIt is actually looking for the row index label (i.e., row number -1), which does not exist in our example DataFrame.\n\n\n# use the first value given from shape to get the number of rows\nnumber_of_rows = df.shape[0]\n# subtract 1 from the value since we want the last index value\nlast_row_index = number_of_rows - 1\n\n# finally do the subset using the index of the last row\ndf.loc[last_row_index]\n\n\n\nWe can use the .tail() method to return the last n = 1 row, instead of the default 5.\n\ndf.tail(n = 1)\n\nUsing .tail() and .loc[] prints out the results differently.\n\ntype( df.tail(n = 1) )\ntype( df.loc[last_row_index] )\n\n\n\nWe can filter multiple rows.\n\ndf.loc[ [0, 99, 999] ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-5",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-5",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubset Rows by Row Number: .iloc[]\n\n.iloc[] does the same thing as .loc[], but is used to subset by the row index number.\nIn our current example, gapminder DataFrame, .iloc[] and .loc[] behave exactly the same way because the index labels are the same as the row numbers.\n\n\n\n# get the 2nd row\ndf.iloc[1]\n\n# get the 100th row\ndf.iloc[99]\n\n# get the last row\ndf.iloc[ -1]. # does it work?\n\n# get the first, 100th, and 1000th row\ndf.iloc[ [0, 99, 999] ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-6",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-6",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nMix It Up\n\nWe can use .loc[] and .iloc[] to obtain rows, columns, or both.\nThe general syntax for .loc[] and .iloc[] uses square brackets with a comma.\ndf.loc[ [rows], [columns] ]\ndf.iloc[ [rows], [columns] ]\nTo just subset columns, we can use the slicing methods.\n\nIf we are subsetting columns, we are getting all the rows for the specified column.\nSo, we need a method to capture all the rows.\nIf we have just a colon (:) when using slicing methods, it “slices” all the values in that axis."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-7",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-7",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSelecting Columns\n\nWe can write df.loc[:, [columns]] or df.iloc[:, [columns]] to subset the column(s).\n\n# subset columns with loc\nsubset_1 = df.loc[:, ['year', 'pop']]\nsubset_1\n\n# subset columns with iloc\n# iloc will allow us to use integers -1 to select the last column\nsubset_2 = df.iloc[:, [2, 4, -1]]\nsubset_2\n\n# do the followings work?\nsubset = df.loc[:, [2, 4, -1]] \nsubset = df.iloc[:, ['year', 'pop']]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-8",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-8",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with range()\n\nWe can use the built-in range() function to create a range of integer values.\n\nBy default, range(start, stop) creates all integer values between the beginning and the end (inclusive left, exclusive right).\nFYI, we can also pass in a 3rd parameter into range(start, stop, step), step, that allows us to change how to increment between the start and stop values (defaults to step=1).\n\n\n# create a range of integers from 0 to 4 inclusive\nsmall_range = list( range(5) )   # equivalent to range(0, 5)\nsmall_range\n# subset the dataframe with the range\nsubset = df.iloc[:, small_range]\nsubset"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-9",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-9",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with range()\n\nLet’s consider one more example with range():\n\n#  create a range from 3 to 5 inclusive\nsmall_range = list( range(3, 6) )\n\nsubset = df.iloc[:, small_range]\nsubset\n\nQ. What happens when you specify a range() that’s beyond the number of columns you have?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-10",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-10",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with Slicing :\n\nUnlike the range() function, the slicing method separates the values with the colon within the square bracket, [start : end : step ].\nLet’s see the columns of the DataFrame:\n\ndf.columns\n\nSee how range() and : are used to slice the first 3 columns.\n\nsmall_range = list( range(3) )\nsubset_1 = df.iloc[ : , small_range ]\n\nsubset_2 = df.iloc[ : , :3 ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-11",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-11",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with Slicing :\nQ. Let’s slice the columns 3 to 5 inclusive using (1) the range() function and (2) the slicing method.\nQ. What happens if we use the slicing method with 2 colons, but leave a value out? For example:\n df.iloc[: , 0 : 6 :   ]\n df.iloc[: , 0 :   : 2 ]\n df.iloc[: ,   : 6 : 2 ]\n df.iloc[: ,   :   : 2 ]\n df.iloc[: ,   :   :   ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes",
    "title": "Lecture 4",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nModifying Columns with .assign()\n\n(0)(1)(2)\n\n\n\nLet’s create a new set of columns that contain the datetime representations of the object (string) dates.\n\n## format the 'Born' column as a datetime\nborn_datetime = pd.to_datetime( scientists['Born'] )\ndied_datetime = pd.to_datetime( scientists['Died'] )\n\nscientists['born_dt'], scientists['died_dt'] = (\n  born_datetime,\n  died_datetime\n)\n\nscientists['age_days'] =  scientists['died_dt'] - scientists['born_dt']\n\n\n\nLet’s create the age_year_assign column using .assign().\n\nscientists = scientists.assign(\n  ## new columns on the left of the equal sign\n  ## how to calculate values on the right of the equal sign\n  ## separate new columns with a comma\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = scientists['age_days'].astype('timedelta64[Y]')\n)\n\n\n\nIn the previous panel, we had to use age_days to create age_year_assign.\n\nTo be able to use age_days_assign when calculating age_year_assign in the previous panel, we need to know about lambda functions.\n\n\nscientists = scientists.drop( ['age_days'], axis = 1)  ## to drop age_days\nscientists = scientists.assign(\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = lambda some_df: \n    some_df['age_days_assign'].astype('timedelta64[Y]') \n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-1",
    "title": "Lecture 4",
    "section": "Pandas Data Structures Basics",
    "text": "Pandas Data Structures Basics\nLambda functions\n\n(1)(2)(3)(4)(5)\n\n\n\nPython has support for so-called anonymous or lambda functions.\n\nLambda functions are a way of writing functions consisting of a single statement, the result of which is the return value.\nA syntax for a lambda function is lambda ARGUMENTS : EXPRESSION\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\nshort_function(2)\nequiv_anon(2)\n\n\n\nA lambda function can have multiple arguments:\n\nfn_two = lambda a, b : a * b\nfn_two(1, 2)\n\nfn_three = lambda a, b, c : a + b + c\nfn_two(1, 2, 3)\n\n\n\nThe power of lambda is better shown when we use them as an anonymous function inside another function.\n\nSay we have a function definition that takes one argument, and that argument will be multiplied with an unknown number:\n\n\ndef myfunc(n):\n  return lambda a : a * n\n\n\n\n\n\nUse that function definition to make a function that always doubles the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_doubler = myfunc(2)\nmy_doubler(10)\n\n\nUse the same function definition to make a function that always triples the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_tripler = myfunc(3)\nmy_tripler(10)\n\n\n\n\n\nHere is the example of applying a lambda function to single variable in DataFrame using .assign()\n\nvalues= [['Rohan',182],['Elvish',100],['Deepak',198],\n         ['Soni',160],['Radhika',140],['Vansh',180]]\ndf = pd.DataFrame(values, columns = ['name','tot_marks'])\n \n## Applying lambda function to find percentage of 'tot_marks' column\n## using df.assign()\ndf = df.assign( percentage = \n  lambda some_name: (some_name['tot_marks'] /200 * 100) )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe",
    "title": "Lecture 4",
    "section": "The DataFrame",
    "text": "The DataFrame\nSubsetting Multiple Rows and Columns"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-1",
    "title": "Lecture 4",
    "section": "The DataFrame",
    "text": "The DataFrame\nBoolean Subsetting\n\nSeriesDataFrameQ.\n\n\n\nWe can not only subset values using labels and indices, but also supply a vector of boolean values.\nBoolean subsetting of numeric Series works as follows:\n\nSeries[ Series &gt; VALUE  ]\nSeries[ Series == VALUE  ]\nSeries[ Series &lt; VALUE  ]\n\n\n\n\n\nBoolean subsetting of DataFrames works like boolean subsetting a Series.\n\nDataFrame[ DataFrame['VARIABLE_NAME'] &gt; VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] == VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] &lt; VALUE  ]\n\n\nimport pandas as pd\nscientists = read_csv('data/scientists.csv')  \n\n## What if we want to subset our ages by identifying those above the mean?\nscientists.loc[ scientists['Age'] &gt; scientists['Age'].mean() ]\n\n\n\nConsider the two Series, area and pop:\n\narea = pd.Series({'California': 423967, 'Texas': 695662,\n                  'New York': 141297, 'Florida': 170312,\n                  'Illinois': 149995})\npop = pd.Series({'California': 38332521, 'Texas': 26448193,\n                 'New York': 19651127, 'Florida': 19552860,\n                 'Illinois': 12882135})\n\nUse boolean subsetting to find states whose population density is greater than 100 or less than 50."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-2",
    "title": "Lecture 4",
    "section": "The DataFrame",
    "text": "The DataFrame\nSubsetting Rows with .query()\n\nWe can also subset rows based on a condition using DataFrame.query():\n\n\nData.query() (1)(2)(3)(4)\n\n\ndf = pd.DataFrame(\n    ## array with numbers from 0 to 35, 6 rows and 6 columns\n    data = np.reshape( range(36), (6, 6) ),\n    index = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"],\n    columns = [\"col\" + str(i) for i in range(6)],\n    dtype = float,\n)\ndf[\"col6\"] = [\"apple\", \"orange\", \"pineapple\", \"mango\", \"kiwi\", \"lemon\"]\ndf\n\n\n\nSelect rows if a value of col6 is “kiwi” or “pineapple”:\n\ndf.query(\"col6 == 'kiwi' or col6 == 'pineapple'\")\n\nSelect rows if a value of col0 is greater than 6:\n\ndf.query(\"col0 &gt; 6\")\n\n\n\nSelect flights that departed on January 1:\n\n## download NY_flights.csv from the Files section in Canvas\nflights = pd.read_csv(\"data/NY_flights.csv\")\nflights.head()\n\nflights.query(\"month == 1 and day == 1\")\n\n\n\nSelect penguins whose bill_length_mm is smaller than 1.8 * bill_depth_mm.\n\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\npenguins.query('bill_length_mm &lt; bill_depth_mm*1.8')\n\n\n\nObjects not in DataFrame can be referenced with an @ character like@a + b.\n\noutside_var = 21\npenguins.query('bill_depth_mm &gt; @outside_var')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe",
    "title": "Lecture 4",
    "section": "The Series and the DataFrame",
    "text": "The Series and the DataFrame\nSorting and Ranking\n\nLet’s consider .sort_index() and .sort_values() methods:\n\n\nSeries\n\n\n\nSeries.sort_index(ascending=False) sorts Series by index in descending order.\nSeries.sort_values(ascending=False) sorts Series by value in descending order.\n\nrev_ages = ages.sort_index(ascending =False) \nsorted_ages = ages.sort_values(ascending =False) \n] ## DataFrame - DataFrame.sort_index(ascending=False) sorts DataFrame by index in descending order. - DataFrame.sort_values(by = \"VAR\", ascending=False) sorts DataFrame by value of VAR in descending order.\nrev_ages = scientists.sort_index(ascending =False) \nsorted_df = scientists.sort_values(by = 'Age', \n                                   ascending =False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe-1",
    "title": "Lecture 4",
    "section": "The Series and the DataFrame",
    "text": "The Series and the DataFrame\nSorting and Ranking\n\nIn Pandas, there are a variety of ranking functions with .rank().\n\n\nexamplemethod (1)method (2)na_optionpct\n\n\n\nConsider the following DataFrame.\n\nimport numpy as np\ndf = pd.DataFrame(data={'Animal': ['fox', 'Kangaroo', 'deer',\n                                   'spider', 'snake'],\n                        'Number_legs': [4, 2, 4, 8, np.nan]})\ndf\n\n\n\n.rank(method = \"min\", ascending=False) does give the largest values the smallest ranks.\n.rank(method = \"dense\", ascending=False) does give the largest values the smallest ranks without any gaps between ranks when breaking ties.\n.rank(method = \"average\", ascending=False) calculates the average rank for each unique value.\n\n\n\ndf['default_rank'] = df['Number_legs'].rank(ascending = False)  ## method = 'average'\ndf['min_rank'] = df['Number_legs'].rank(method='min', ascending = False)\ndf['dense_rank'] = df['Number_legs'].rank(method='dense', ascending = False)\ndf\n\n\n\nna_option = keep assigns NaN rank to NaN values (default).\nna_option = top assign smallest rank to NaN values if ascending\nna_option = bottom assign highest rank to NaN values if ascending\n\ndf['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')\ndf['NA_top'] = df['Number_legs'].rank(na_option='top')\ndf\n\n\n\npct = True displays the returned rankings in percentile form.\n\n\ndf['pct_rank'] = df['Number_legs'].rank(pct=True)\ndf"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-1",
    "title": "Lecture 4",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDropping Values\n\nColumnsRows\n\n\n\nTo drop a column, we can select columns to drop with the .drop() method with axis = 1 or axis = \"columns\" on our dataframe.\n\n## all the current columns in our data\nscientists.columns\n\n## drop the shuffled age column\n## we provide the axis=1 argument to drop column-wise\nscientists_dropped = scientists.drop( ['Age'], axis =\"columns\")\nscientists_dropped.columns\n\n\n\nTo drop rows, we can select rows by index to drop with the .drop() method with axis = 0, which is default.\n\n## all the current columns in our data\nscientists.columns\n\n## drop rows by their indices\nscientists_rows_dropped = scientists.drop( [2, 4, 6] )\nscientists_rows_dropped"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-2",
    "title": "Lecture 4",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nModifying Columns with .assign()\n\n(0)(1)(2)\n\n\n\nLet’s create a new set of columns that contain the datetime representations of the object (string) dates.\n\n## format the 'Born' column as a datetime\nborn_datetime = pd.to_datetime( scientists['Born'] )\ndied_datetime = pd.to_datetime( scientists['Died'] )\n\nscientists['born_dt'], scientists['died_dt'] = (\n  born_datetime,\n  died_datetime\n)\n\nscientists['age_days'] =  scientists['died_dt'] - scientists['born_dt']\n\n\n\nLet’s create the age_year_assign column using .assign().\n\nscientists = scientists.assign(\n  ## new columns on the left of the equal sign\n  ## how to calculate values on the right of the equal sign\n  ## separate new columns with a comma\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = scientists['age_days'].astype('timedelta64[Y]')\n)\n\n\n\nIn the previous panel, we had to use age_days to create age_year_assign.\n\nTo be able to use age_days_assign when calculating age_year_assign in the previous panel, we need to know about lambda functions.\n\n\nscientists = scientists.drop( ['age_days'], axis = 1)  ## to drop age_days\nscientists = scientists.assign(\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = lambda some_df: \n    some_df['age_days_assign'].astype('timedelta64[Y]') \n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-2",
    "title": "Lecture 4",
    "section": "Pandas Data Structures Basics",
    "text": "Pandas Data Structures Basics\nLambda functions\n\n(1)(2)(3)(4)(5)\n\n\n\nPython has support for so-called anonymous or lambda functions.\n\nLambda functions are a way of writing functions consisting of a single statement, the result of which is the return value.\nA syntax for a lambda function is lambda ARGUMENTS : EXPRESSION\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\nshort_function(2)\nequiv_anon(2)\n\n\n\nA lambda function can have multiple arguments:\n\nfn_two = lambda a, b : a * b\nfn_two(1, 2)\n\nfn_three = lambda a, b, c : a + b + c\nfn_two(1, 2, 3)\n\n\n\nThe power of lambda is better shown when we use them as an anonymous function inside another function.\n\nSay we have a function definition that takes one argument, and that argument will be multiplied with an unknown number:\n\n\ndef myfunc(n):\n  return lambda a : a * n\n\n\n\n\n\nUse that function definition to make a function that always doubles the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_doubler = myfunc(2)\nmy_doubler(10)\n\n\nUse the same function definition to make a function that always triples the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_tripler = myfunc(3)\nmy_tripler(10)\n\n\n\n\n\nHere is the example of applying a lambda function to single variable in DataFrame using .assign()\n\nvalues= [['Rohan',182],['Elvish',100],['Deepak',198],\n         ['Soni',160],['Radhika',140],['Vansh',180]]\ndf = pd.DataFrame(values, columns = ['name','tot_marks'])\n \n## Applying lambda function to find percentage of 'tot_marks' column\n## using df.assign()\ndf = df.assign( percentage = \n  lambda some_name: (some_name['tot_marks'] /200 * 100) )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-1",
    "title": "Lecture 4",
    "section": "Exporting and Importing Data",
    "text": "Exporting and Importing Data\nComma-Separated Values (CSV)\n\nComma-separated values (CSV) are the most flexible data storage type.\n\nFor each row, the column information is separated with a comma.\n\nTo export Series or DataFrame as a csv file, we use the to_csv() method.\n\n## index =False  does not write the row names in the CSV output\nscientists.to_csv('output/scientists_df_no_index.csv', \n                   index =False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-2",
    "title": "Lecture 4",
    "section": "Exporting and Importing Data",
    "text": "Exporting and Importing Data\nExcel\n\nThe more of your work you can do in Python and/or R, the easier it will be to scale up to larger projects, catch and fix mistakes, and collaborate.\nHowever, Excel’s popularity and market share are unrivaled.\nTo export Series or DataFrame as an .xlsx file, we use the to_excel() method.\n\n## saving a DataFrame into Excel format\nscientists.to_excel(\n  \"output/scientists_df.xlsx\",\n  sheet_name = \"scientists\",\n  index = False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nHere we discuss how to use summary statistics and visualization to explore your data in a systematic way, a task that statisticians call exploratory data analysis (EDA).\nEDA is an iterative cycle. We:\n\nGenerate questions about your data.\nSearch for answers by visualizing, transforming, and modelling our data.\nUse what we learn to refine our questions and/or generate new questions.\n\nHere, we focus on the visualization part."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-1",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nTidy data.frame\n\n\nIn a tidy DataFrame,\n\nA variable is in a column.\nAn observation is in a row.\nA value are in a cell."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-2",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nTidy data.frame\n\nA variable is a quantity, quality, or property that we can measure/count.\nAn observation is a set of measurements made under similar conditions (e.g, similar unit of entity, time, and/or geography).\n\nWe usually make all of the measurements in an observation at the same time and on the same object.\nAn observation will contain several values, each associated with a different variable.\nWe sometimes refer to an observation as a data point.\n\nThe value of a variable may change from measurement to measurement."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-3",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-3",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nCategorical/Discrete vs. Continuous Variables\n\nA discrete/categorical variable is a variable whose value is obtained by counting and is whole numbers.\n\nNumber of red marbles in a jar\nNumber of heads when flipping three coins\nStudents’ letter grade\nUS state/county\n\nA continuous variable is a variable whose value is obtained by measuring and can have a decimal or fractional value.\n\nHeight/weight of students\nTime it takes to get to school\nFuel efficiency of a vehicle (e.g., miles per gallon)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-4",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-4",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nMaking Discoveries from a Data Set\n\nDistributionRelationshipStepsTypesSummary Stat.\n\n\n\nWhat type of variation occurs within a variable?\nVariation is the tendency of the values of a variable to change from measurement to measurement.\n\nWe can see variation easily in real life; if we measure any continuous variable twice, we will be likely to get two different values.\nWhich values are the most common? Why?\nWhich values are rare? Why? Does that match your expectations?\nCan we see any unusual patterns? What might explain them?\n\n\n\n\n\nCo-variation is the tendency for the values of two or more variables to vary together in a related way.\nWhat type of co-variation occurs between variables?\n\n\n\n\nFigure out whether variables of interests are categorical or continuous.\nThink which geometric objects, aesthetic mappings, and faceting are appropriate to visualize distributions and relationships.\nIf needed, transform a given DataFrame (e.g., subset of observations, new variables, summarized data) and try new visualizations.\n\n\n\n\nA distribution of a categorical variable (e.g., bar charts and more)\nA distribution of a continuous variable (e.g., histograms and more)\nA relationship between two categorical variables (e.g., bar charts and more)\nA relationship between two continuous variables (e.g., scatter plots and more)\nA relationship between a categorical variable and a continuous variable (e.g., boxplots and more)\nA time trend of a categorical variable (e.g., bar plots and more)\nA time trend of a continuous variable (e.g., line plots and more)\n\n\n\n\nUse skim(DataFrame) or .describe() to know:\n\nMean (Average, Expected Value);\nStandard Deviation (SD)\nMinimum, First Quartile (Q1), Median (Q2), Third Quartile (Q3), and Maximum."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization",
    "title": "Lecture 4",
    "section": "Data Visualization",
    "text": "Data Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraphs and charts let us explore and learn about the structure of the information we have in DataFrame.\nGood data visualizations make it easier to communicate our ideas and findings to other people."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#seaborn",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#seaborn",
    "title": "Lecture 4",
    "section": "seaborn",
    "text": "seaborn\n\nseaborn is a Python data visualization library based on matplotlib.\n\nIt allows us to easily create beautiful but complex graphics using a simple interface.\nIt also provides a general improvement in the default appearance of matplotlib-produced plots, and so I recommend using it by default.\n\n\nimport seaborn as sns\nsns.set_theme(rc={'figure.dpi': 600, \n                  'figure.figsize': (5, 3.75)})   ## better quality"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-1",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nGetting started with seaborn\n\nLet’s get the names of DataFrames provided by the seaborn library:\n\nimport seaborn as sns\nprint( sns.get_dataset_names() )\n\nLet’s use the titanic and tips DataFrames:\n\ntitanic = sns.load_dataset('titanic')\ntitanic.head()\ntips = sns.load_dataset('tips')\ntips.head()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-2",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nBar Chart\n\nA bar chart is used to plot the frequency of the different categories.\n\nIt is useful to visualize how values of a categorical variable are distributed.\nA variable is categorical if it can only take one of a small set of values.\n\nWe use sns.countplot() function to plot a bar chart:\n\n\n\nsns.countplot(data = titanic,\n              x =  'sex')\n\n\nMapping\n\ndata: DataFrame.\nx: Name of a categorical variable (column) in DataFrame"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-3",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-3",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nBar Chart\n\nWe can further break up the bars in the bar chart based on another categorical variable.\n\nThis is useful to visualize the relationship between the two categorical variables.\n\n\n\n\nsns.countplot(data = titanic,\n              x = 'sex'\n              hue = 'survived')\n\n\nMapping\n\nhue: Name of a categorical variable"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-4",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-4",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nHistogram\n\nA histogram is a continuous version of a bar chart.\n\nIt is used to plot the frequency of the different values.\nIt is useful to visualize how values of a continuous variable are distributed.\n\nWe use sns.histplot() function to plot a histogram: :::: {.columns} ::: {.column width=“50%”}\n\nsns.histplot(data = titanic,\n             x =  'age', \n             bins = 5)\n:::\n\n\nMapping\n\nbins: Number of bins\n\n\n\n::::"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-5",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-5",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nHistogram\n\nA boxplot computes a summary of the distribution and then display a specially formatted box.\n\nIt is useful to visualize how values of a continuous variable are distributed across different values of another (categorical) variable.\n\nWe use sns.histplot() function to plot a histogram: :::: {.columns} ::: {.column width=“50%”}\n\nsns.boxplot(data = tips,\n             y='total_bill')\n:::\n\nsns.boxplot(data = tips,\n            x='time', y='total_bill')\n\n::::"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-6",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-6",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nScatter plot\n\nA scatter plot is used to display the relationship between two continuous variables.\n\nWe can see co-variation as a pattern in the scattered points.\n\nWe use sns.scatterplot() function to plot a scatter plot:\n\n\n\nsns.scatterplot(data = tips,\n                x = 'total_bill', \n                y = 'tip')\n\n\nMapping\n\nx: Name of a continuous variable on the horizontal axis\ny: Name of a continuous variable on the vertical axis"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-7",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-7",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nScatter plot\n\nTo the scatter plot, we can add a hue-VARIABLE mapping to display how the relationship between two continuous variables varies by VARIABLE.\nSuppose we are interested in the following question:\n\nQ. Does a smoker and a non-smoker have a difference in tipping behavior?\n\n\nsns.scatterplot(data = tips,\n                x = 'total_bill', \n                y = 'tip',\n                hue = 'smoker')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-8",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-8",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nFitted line\n\nFrom the scatter plot, it is often difficult to clearly see the relationship between two continuous variables.\n\nsns.lmplot() adds a line that fits well into the scattered points.\nOn average, the fitted line describes the relationship between two continuous variables.\n\n\nsns.lmplot(data = tips,\n           x = 'total_bill', \n           y = 'tip')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-9",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-9",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nTransparency with alpha\n\nIn a scatter plot, adding transparency with alpha helps address many data points on the same location.\n\nWe can map alpha to number between 0 and 1.\n\n\n\n\nsns.scatterplot(x = 'total_bill', \n                y = 'tip',\n                hue = 'smoker',\n                alpha = .25)\n\nsns.lmplot(data = tips,\n           x = 'total_bill', \n           y = 'tip',\n           scatter_kws = {'alpha' : 0.2})"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-10",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-10",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nScatter plot\n\nTo the scatter plot, we can add a hue-VARIABLE mapping to display how the relationship between two continuous variables varies by VARIABLE.\nUsing the fitted lines, let’s answer the following question:\n\nQ. Does a smoker and a non-smoker have a difference in tipping behavior?\n\n\nsns.scatterplot(data = tips,\n                x = 'total_bill', \n                y = 'tip',\n                hue = 'smoker')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-11",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-11",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nLine cahrt\n\nA line chart is used to display the trend in a continuous variable or the change in a continuous variable over other variable.\n\nIt draws a line by connecting the scattered points in order of the variable on the x-axis, so that it highlights exactly when changes occur.\n\nWe use sns.lineplot() function to plot a line plot:\n\n\n\npath_csv = 'https://bcdanl.github.io/data/dji.csv'\ndow = pd.read_csv(path_csv, index_col=0, parse_dates=True)\nsns.lineplot(data = dow,\n             x =  'Date', \n             y =  'Close')\n\n\nMapping\n\nx: Name of a continuous variable (often time variable) on the horizontal axis\ny: Name of a continuous variable on the vertical axis"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-12",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-12",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nLine cahrt\n\nFor line charts, we often need to group or connect observations to visualize the number of distinct lines.\n\nhealthexp = ( sns.load_dataset(\"healthexp\")\n             .sort_values([\"Country\", \"Year\"])\n             .query(\"Year &lt;= 2020\") )\nhealthexp.head()\n\nsns.lineplot(data = dow,\n             x =  'Year', \n             y =  'Life_Expectancy',\n             color = 'Country')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-13",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-13",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nFaceting\n\nFaceting allows us to plot subsets (facets) of our data across subplots.\n\n\nStep 1. FacetGrid()Step2. FacetGrid().map()\n\n\n\nFirst, we create a FacetGrid() object with the data we will be using and define how it will be subset with the row and col arguments:\n\ng = sns.FacetGrid(\n      data = titanic,\n      row='class',\n      col='sex')\n\n\n\nSecodn, we use the FacetGrid().map() method to run a plotting function on each of the subsets, passing along any necessary arguments.\n\ng.map(sns.histplot, 'age', kde=True)  ## kde: kernel density (probability density function)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#pandas-data-structures-basics-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#pandas-data-structures-basics-1",
    "title": "Lecture 3",
    "section": "Pandas Data Structures Basics",
    "text": "Pandas Data Structures Basics\nCreate Your Own Data\n\nKnowing how to create Series or DataFrames without loading data from a file is a useful skill.\nSeries is a one-dimensional container.\nA Series is very similar to a Python list, except that each element must be the same dtype (object, int64, float64, or datetime64).\n\nThis is the same behavior as the NumPy array (ndarray).\nIf a column contains the number 1 and the sequence of letters “pizza”, the entire dtype of the column will be a string (which is object).\n\nA DataFrame can be thought of as a dictionary of Series objects.\n\nEach key is the column name and the value is the Series."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-1",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nNumPy Array\n\nPython library NumPy introduces an N-dimensional array object, or ndarray.\n\nPandas implicitly uses ndarray, so here let’s see what ndarray is.\nThe easiest way to create an array is to use the .array() method.\n\n\n\n\nimport pandas as np\ndata1 = [6, 7.5, 8, 0, 1]\narr1 = np.array(data1) \narr1\narr2.ndim\narr2.shape\n\ndata2 = [ [1, 2, 3, 4], \n          [5, 6, 7, '8'] ]\narr2 = np.array(data2)\narr2\narr2.ndim\narr2.shape"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-2",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a Series\n\nA Series is a data structure in pandas.\n\nContaining a sequence of values and a corresponding labels, called the index,\nA Series displays the index on the left and the values on the right,\nThe default index consists of the integers 0 through N-1.\n\npd.Series() creates a one-dimensional container including values and an index.\n\nimport pandas as pd\ns = pd.Series( ['banana', 42] )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-3",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a Series\n\nThe “row number” is shown on the left of the Series.\nThis is actually the index for the Series.\nIt is similar to the row name and row index for DataFrame.\n\n## manually assign index values to a series\n## by passing a Python list\ns = pd.Series(\n  data =[\"Wes McKinney\", \"Creator of Pandas\"],\n  index =[\"Person\", \"Who\"],\n)\n\ns"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-4",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a Series\n\nPandas Series can also be created from ndarrys, tuples, and dictionaries.\nQ. Use dictdata to create a Pandas Series.\n\ndictdata = {\n    \"Name\": [\"William Nordhaus\", \"Ronald Coase\"],\n    \"Occupation\": [\"Economist\", \"Economist\"],\n    \"Born\": [\"1941-05-31\", \"1910-12-29\"],\n    \"Died\": [\"\", \"2013-09-02\"],\n    \"Age\": [81, 102],\n  }"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-5",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-5",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a DataFrame\n\nA DataFrame can be thought of as a dictionary of Series objects.\n\nDictionaries are one of the most common ways of creating a DataFrame.\nThe key represents the column name, and the values are the contents of the column.\n\n\neconomists = pd.DataFrame(\n {  \"Name\": [\"William Nordhaus\", \"Ronald Coase\"],\n    \"Occupation\": [\"Economist\", \"Economist\"],\n    \"Born\": [\"1941-05-31\", \"1910-12-29\"],\n    \"Died\": [\"\", \"2013-09-02\"],\n    \"Age\": [81, 102]  } )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\n\nLet’s re-create our example DataFrame.\n\n## create our example dataframe with a row index label\neconomists = pd.DataFrame(\n  data = {\n    \"Occupation\": [\"Economist\", \"Economist\"],\n    \"Born\": [\"1941-05-31\", \"1910-12-29\"],\n    \"Died\": [\"\", \"2013-09-02\"],\n    \"Age\": [81, 102] },\n  index =  [\"William Nordhaus\", \"Ronald Coase\"],\n  columns =[\"Occupation\", \"Born\", \"Died\", \"Age\"] \n)\n\nQ. Select an economist from economists by the row index label to get a Series."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-1",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nSeries Attributes\n\nWhen a series is printed (i.e., the string representation), the index is printed as the first “column”, and the values are printed as the second “column”.\nThere are many attributes and methods associated with a Series object.\n\nfirst_row = economists.loc['William Nordhaus']\ntype(first_row)\nfirst_row.index\nfirst_row.values\ntype(first_row.values)\nfirst_row.shape\nfirst_row.size\nfirst_row.dtypes"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-2",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nAttributes vs. Methods\n\nAttributes can be thought of as features of an object (in this example, our object is a Series).\nMethods can be thought of as some calculation or operation that is performed on an object.\n\nMethods or functions have round parentheses (()), while attributes do not.\n\nThe subsetting syntax .loc[] and .iloc[] consists of all attributes."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-3",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nSeries Methods\n\nLet’s first get a series of the Age column from our economists DataFrame.\n\nages = economists['Age']\n\nWhen we have a vector of numbers, there are common calculations we can perform.\n\nages.mean()\nages.min()\nages.max()\nages.std()\n\n.mean(), .min(), .max(), and .std() are also methods in np.ndarray."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#workflow",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#workflow",
    "title": "Lecture 3",
    "section": "Workflow",
    "text": "Workflow\nWorking Directory\n\nIn Spyder, we can set the working directory.\n\nWindows: Tools &gt; Preferences &gt; Working directory &gt; The following directory\nMac: python &gt; Preferences &gt; Working directory &gt; The following directory\n\nDownload the CSV file, scientists.csv from the Files section in our Canvas.\n\nCreate the data folder in your working directory.\nThen move the scientists.csv file to the data folder in your working directory."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#descriptive-statistics",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#descriptive-statistics",
    "title": "Lecture 3",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n.describe() Methods\n\nLet’s load the CSV file, scientists.csv.\n\n## If we set the working directory, we do not need to use the absolute path\nscientists = pd.read_csv('data/scientists.csv')  \nages = scientists['Age']\n\nThe .describe() method calculates multiple descriptive statistics for numeric variables.\n\nscientists.describe()\nages.describe()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-4",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nBoolean Subsetting on Series\n\nWe can not only subset values using labels and indices, but also supply a vector of boolean values.\nBoolean subsetting of numeric Series works as follows:\n\nSeries[ Series &gt; VALUE  ]\nSeries[ Series == VALUE  ]\nSeries[ Series &lt; VALUE  ]\n\nQ. What if we wanted to subset our ages by identifying those above the mean?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-5",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-5",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nBoolean Subsetting on Series\n\nLet’s look at what ages &gt; ages.mean() returns.\n\nages &gt; ages.mean()\n\ncond = ages &gt; ages.mean()\nages[cond]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-6",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-6",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nMany of the methods that work on Series (and also DataFrames) are “vectorized”, meaning that they work on the entire vector simultaneously.\nages &gt; ages.mean() returns a vector without any for loops.\n\n\nsame lengthw/ scalarsw/ different lengthw/ different length (e.g.,)\n\n\n\nIf we perform an operation between two vectors of the same length, the resulting vector will be an element-by-element calculation of the vectors.\n\nages + ages\nages * ages\n\n\n\nWhen we perform an operation on a vector using a scalar, the scalar will be recycled across all the elements in the vector.\n\nages + 100\nages * 2\n\n\n\nWhen we are working with vectors of different lengths, the behavior will depend on the type() of the vectors.\n\nWith a Series, the vectors will perform an operation matched by the index.\nWith other types(), the shapes must match.\n\n\n\n\nages + pd.Series( [1, 100] )\nages + np.array( [1, 100] )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-7",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-7",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nWhat’s convenient in Pandas is how data alignment is almost always automatic. - If possible, things will always align themselves with the index label when actions are performed.\nLet’s consider .sort_index() method:\n\nSeries.sort_index(ascending=False) sorts Series by index in descending order.\n\nrev_ages = ages.sort_index(ascending =False) \nrev_ages\nQ. What is ages + rev_ages?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nParts of a DataFrame\n\nThe DataFrame is the most common Pandas object.\n\nIt can be thought of as Python’s way of storing spreadsheet-like data.\nMany of the features of the Series data structure carry over into the DataFrame.\n\nThere are 3 main parts of a Pandas DataFrame object:\n\n\n.index, (2) .columns, and (3) .values\n\n\n\nscientists.index\nscientists.columns\nscientists.values"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-1",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nBoolean Subsetting on DataFrames\n\nBoolean subsetting of DataFrames works like boolean subsetting a Series.\n\nDataFrame[ DataFrame['VARIABLE_NAME'] &gt; VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] == VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] &lt; VALUE  ]\n\n\n## boolean vectors will subset rows\nscientists.loc[ scientists['Age'] &gt; scientists['Age'].mean() ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-2",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nSubsetting Multiple Rows and Columns"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-3",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nPandas supports broadcasting because the Series and DataFrame objects are built on top of the numpy library.\n\nBroadcasting describes what happens when performing operations between array-like objects.\nThese behaviors depend on the type of object, its length, and any labels associated with the object."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-4",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nw/ scalarsw/ Series\n\n\n\nWhen we perform an action on a dataframe with a scalar, it will try to apply the operation on each cell of the dataframe.\n\nscientists * 2\n\n\n\nBy default, arithmetic operations between DataFrames and Series match the index of the Series on the DataFrame’s columns,\nThe operations will be broadcasted along the rows.\n\npd.Series([10]) + scientists[['Age']]\npd.Series([10], index = ['Age']) + scientists[['Age']]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nAdd Additional Columns\n\nThe type of the Born and Died columns is object, meaning they are strings or a sequence of characters.\n\nscientists.dtypes\n\nWe can convert the strings to a proper datetime type so we can perform common date and time operations.\n\n## format the 'Born' column as a datetime\nborn_datetime = pd.to_datetime( scientists['Born'] )\n\ndied_datetime = pd.to_datetime( scientists['Died'] )\n\nMore examples with datetimes would be discussed later in March or April."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-1",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nAdd Additional Columns\n\nWe can create a new set of columns that contain the datetime representations of the object (string) dates.\n\nscientists['born_dt'], scientists['died_dt'] = (\n  born_datetime,\n  died_datetime\n)\n\nscientists.head()\nscientists.shape\nscientists.dtypes"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-2",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDirectly Change a Column\n\n(1)(2)(3)(4)(5)\n\n\n\nLet’s look at the original Age values.\n\nscientists['Age']\n\n\n\nLet’s shuffle the values using .sample().\n\n## the frac=1 tells pandas to randomly select 100% of the values\n## the random_state makes the randomization the same each time\nscientists['Age'].sample(frac=1, random_state = 210)\n\n\n\nLet’s assign scientists['Age'] to the shuffled one.\n\nscientists['Age'] = (\n  scientists['Age']\n  .sample(frac=1, random_state = 210)\n)\nscientists['Age']\n\nHow is scientists['Age']?\n\n\n\n\nWe tried to randomly shuffle the values, but when we assigned the values back into the dataframe, it reverted back to the original order.\n\nThat’s because Pandas will try to automatically join on the .index values on many operations, for this example to get around this problem we need to remove that .index information.\nOne way of doing that, is to assign just the .values of the shuffled values that does not have any .index value associated with it.\n\n\n\n\nscientists['Age'] = (\n  scientists['Age']\n  .sample(frac=1, random_state = 210)\n  .values    ## remove the index so it doesn't auto align the values\n)\n\nscientists['Age']\n\nHow is scientists['Age'] now?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-3",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDirectly Change a Column\n\nHere is how we re-calculate the real ages.\n\n## subtracting dates will give us number of days\nscientists['age_days'] =  scientists['died_dt'] - scientists['born_dt']\n\n## we can convert the value to just the year\n## using the astype method\nscientists['age_years'] = (\n  scientists['age_days']\n  .astype('timedelta64[Y]')\n)\n\nscientists"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-4",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nModifying Columns with .assign()\n\n(1)(2)lambda (1)lambda (2)\n\n\n\nLet’s redo the age_years column creation, but this time using .assign().\n\nscientists = scientists.assign(\n  ## new columns on the left of the equal sign\n  ## how to calculate values on the right of the equal sign\n  ## separate new columns with a comma\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = scientists['age_days'].astype('timedelta64[Y]')\n)\n\n\n\nWhen calculating age_year_assign, we did not use age_days_assign.\n\nTo be able to use age_days_assign when calculating age_year_assign in the previous panel, we need to know about lambda functions.\n\n\nscientists = scientists.assign(\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = lambda some_df: some_df['age_days_assign'].astype('timedelta64[Y]')\n)\n\nWe will cover lambda functions in detail later.\n\n\n\n\nPython has support for so-called anonymous or lambda functions.\n\nLambda functions are a way of writing functions consisting of a single statement, the result of which is the return value.\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\n\nshort_function(2)\nequiv_anon(2)\n\n\n\nThe power of lambda is better shown when we use them as an anonymous function inside another function.\n\nSay we have a function definition that takes one argument, and that argument will be multiplied with an unknown number:\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\n\nshort_function(2)\nequiv_anon(2)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-5",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-5",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDropping Values\n\nColumnsRows\n\n\n\nTo drop a column, we can select columns to drop with the .drop() method with axis = 1 or axis = \"columns\" on our dataframe.\n\n## all the current columns in our data\nscientists.columns\n\n## drop the shuffled age column\n## we provide the axis=1 argument to drop column-wise\nscientists_dropped = scientists.drop( ['Age'], axis =\"columns\")\nscientists_dropped.columns\n\n\n\nTo drop rows, we can select rows by index to drop with the .drop() method with axis = 0, which is default.\n\n## all the current columns in our data\nscientists.columns\n\n## drop rows by their indices\nscientists_rows_dropped = scientists.drop( [2, 4, 6] )\nscientists_rows_dropped"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-1",
    "title": "Lecture 1",
    "section": "Instructor",
    "text": "Instructor\nCurrent Appointment & Education\n\nName: Byeong-Hak Choe.\nAssistant Professor of Data Analytics and Economics, School of Business at SUNY Geneseo.\nPh.D. in Economics from University of Wyoming.\nM.S. in Economics from Arizona State University.\nM.A. in Economics from SUNY Stony Brook.\nB.A. in Economics & B.S. in Applied Mathematics from Hanyang University at Ansan, South Korea.\n\nMinor in Business Administration.\nConcentration in Finance."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-2",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-2",
    "title": "Lecture 1",
    "section": "Instructor",
    "text": "Instructor\nData Science and Climate Change\n\nChoe, B.H., 2021. “Social Media Campaigns, Lobbying and Legislation: Evidence from #climatechange/#globalwarming and Energy Lobbies.”\nQuestion: To what extent do social media campaigns compete with fossil fuel lobbying on climate change legislation?\nData include:\n\n5.0 million tweets with #climatechange/#globalwarming around the globe;\n12.0 million retweets/likes to those tweets;\n0.8 million Twitter users who wrote those tweets;\n1.4 million Twitter users who retweeted or liked those tweets;\n0.3 million US Twitter users with their location at a city level;\nFirm-level lobbying data (expenses, targeted bills, etc.)."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-1",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nEmail, Class & Office Hours\n\nEmail: bchoe@geneseo.edu\nClass Homepage:\n\nhttps://brightspace.geneseo.edu/\nhttp://bcdanl.github.io/module-1/\n\nOffice: South Hall 301\nOffice Hours:\n\nBy appointment via email"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-2",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-2",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Description\n\nThis course aims to provide overview of how one can collect, manipulate, process, clean, and crunch datasets with practical case studies.\nThis course will cover topics such as (1) loading, slicing, filtering, transforming, reshaping, and merging data, (2) summarizing and visualizing data, and (3) exploratory data analysis.\nWe will cover these topics to solve real-world data analysis problems with thorough, detailed examples."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-3",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-3",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Requirements\n\nLaptop: You should bring your own laptop (Mac or Windows) to the classroom.\n\nIt is recommended to have 2+ core CPU, 4+ GB RAM, and 500+ GB disk storage in your laptop for this course.\n\nHomework: There will be six homework assignments.\nExams: There will be one take-home exam."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-4",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-4",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Contents\n\nThere will be tentatively 28 class sessions:\n\n7 lectures\n1 take-home exam"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-5",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-5",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Contents"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-6",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-6",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nAssessments\n\nEach of the six homework assignments accounts for 10% of the total percentage grade.\nThe exam account for 30% of the total percentage grade.\nParticipation in discussions accounts for 10% of the total percentage grade.\n\n\\[\n\\begin{align}\n(\\text{Total Percentage Grade}) =\\quad\\, &0.60\\times(\\text{Total Homework Score})\\notag\\\\\n\\,+\\, &0.30\\times(\\text{Take-Home Exam Score})\\notag\\\\\n\\,+\\, &0.10\\times(\\text{Total Discussion Score})\\notag\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-github",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-github",
    "title": "Lecture 1",
    "section": "What is GitHub?",
    "text": "What is GitHub?\n\nhttps://github.com/ is a platform for storing, managing, and sharing code.\nCourse contents will be posted not only Brightspace but also our GitHub repositories (“repos”).\nOnline discussion and Q & A will be hosted on GitHub repository.\nClicking “Sign Up” or https://github.com/signup will direct you to the sign up page."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#getting-a-github-account",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#getting-a-github-account",
    "title": "Lecture 1",
    "section": "Getting a GitHub account",
    "text": "Getting a GitHub account\n\nGithub is useful for many reasons, but the main reason is how user friendly it makes uploading and sharing code.\nSharing and editing code for a large group is easy with Github\nIt also automatically maintains a history of all changes to the repo. You can see the history of committed changes, and even go into detail about what those changes entailed.\nYou can also revert to old versions of your repo, and all edits will be saved in this history."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-google-colab",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-google-colab",
    "title": "Lecture 1",
    "section": "What is Google Colab",
    "text": "What is Google Colab\n\nhttps://colab.research.google.com/ is analogous to Google Drive, but specifically for writing and executing Python code in your browser.\n\nThe base Colab link listed above leads to a Python notebook introducing Colab and how to use it.\n\nThis video also helps get started with Colab if you are unfamiliar with the format!\n\nhttps://www.youtube.com/watch?v=inN8seMm7UI"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-use-colab",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-use-colab",
    "title": "Lecture 1",
    "section": "Why use Colab?",
    "text": "Why use Colab?\n\nA key benefit of Colab is that it is entirely free to use and has many of the standard Python modules pre installed.\n\nIt allows for CPU or GPU usage, even for free users, and stores the files in Google’s servers so you can access your files from anywhere you can connect to the Internet.\n\nUsing Colab also means you can entirely avoid the process of installing Python and any dependencies onto your computer.\nColab notebooks don’t just contain Python code. They can contain text, images, and HTML!\nUltimately, they’re intuitive to use and let you jump right into the code and data analysis without needing to worry about the more cumbersome details needed to run Python notebooks on a personal computer."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#colab-has-github-integration",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#colab-has-github-integration",
    "title": "Lecture 1",
    "section": "Colab has GitHub integration!",
    "text": "Colab has GitHub integration!\n\nLet’s walk through the GitHub integration and look at how to use Colab with the first notebook for this course."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-1",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nObjectives\n\nIn this Section, we will discuss the basics of objects, types, operations, and imports.\n\nThese are the basic building blocks of almost all programming languages and will serve you well for your coding journey.\n\nEverything is an object, and every object has a type."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-2",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-2",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nVariables Are Names, Not Places\n\nThe most basic built-in data types that you’ll need to know about are: integers 10, floats 1.23, strings like this, booleans True, and nothing None.\nPython also has a built-in type called a list [10, 15, 20] that can contain anything, even different types\n\nlist_example = [10, 1.23, \"like this\", True, None]\nprint(list_example)\ntype(list_example)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-3",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-3",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nTypes\n\n\n\n\n\n\n\nPython’s basic data types\n\n\n\n\n\n\nThe second column (Type) contains the Python name of that type.\nThe third column (Mutable?) indicates whether the value can be changed after creation."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-4",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-4",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBrackets\n\nThere are several kinds of brackets in Python, including [], {}, and ().\n\n\n[]{}()\n\n\n\n[] is used to denote a list or to signify accessing a position using an index.\n\nvector = ['a', 'b']\nvector[0]\n\n\n\n{} is used to denote a set or a dictionary (with key-value pairs).\n\n{'a', 'b'}\n{'first_letter': 'a', 'second_letter': 'b'}\n\n\n\n() is used to denote a tuple, or the arguments to a function, e.g., function(x) where x is the input passed to the function, or to indicate the order operations are carried out.\n\nnum_tup = (1, 2, 3)\nsum(num_tup)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-5",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-5",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\n\nLists are a really useful way to work with lots of data at once.\n\nWe can also construct them by appending entries:\n\n\nlist_example = [10, 1.23, \"like this\", True, None]\nlist_example.append(\"one more entry\")\nprint(list_example)\n\nWe can access earlier entries using an index, which begins at 0 and ends at one less than the length of the list.\n\nprint(list_example[0])\nprint(list_example[-1])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-6",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-6",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\n\nSlicing can be even more elaborate than that because we can jump entries using a second colon.\n\n# range() produces a list of integers from the value to one less than the last\nlist_of_numbers = list(range(1, 11))\nstart = 1\nstop = -1\nstep = 2\nprint(list_of_numbers[ start : stop : step ])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-7",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-7",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\n\nAll of the basic operators you see in mathematics are available to use: + for addition, - for subtraction, * for multiplication, ** for powers, / for division, // for integer division, and % for modulo.\n\nThese work as you’d expect on numbers.\nThese operators are sometimes defined for other built-in data types too.\n\n\nWe can ‘sum’ strings (which really concatenates them):\n\n\nstring_one = \"This is an example \"\nstring_two = \"of string concatenation\"\nstring_full = string_one + string_two\nprint(string_full)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-8",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-8",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\n\n\n\nIt works for lists too:\n\nlist_one = [\"apples\", \"oranges\"]\nlist_two = [\"pears\", \"satsumas\"]\nlist_full = list_one + list_two\nprint(list_full)\n\n\nWe can multiply strings!\n\nstring = \"apples, \"\nprint(string * 3)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-9",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-9",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\n\nWe can combine the arithmetic operators with assignment by putting the operator before the =.\n\n\n\n\nx += 1 is equivalent to x = x + 1.\n\nx = 3\nx += 1\n\n\na -= 2 is equivalent to a = a - 2.\n\na = 2\na -= 2"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-10",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-10",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\nQ. Using Python operations only, what is \\[\\frac{2^5}{7 \\cdot (4 - 2^3)} \\quad\\text{?}\\]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-11",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-11",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nStrings\n\nFrom strings, we can access the individual characters via slicing and indexing.\n\nstring = \"cheesecake\"\nprint( string[-4:] )\n\nBoth lists and strings will allow us to use the len() command to get their length:\n\n\n\n\nx += 1 is equivalent to x = x + 1.\n\nstring = \"cheesecake\"\nprint( \"String has length:\" )\nprint( len(string) )\n\n\na -= 2 is equivalent to a = a - 2.\n\nlist_of_numbers = range(1, 20)\nprint( \"List of numbers has length:\" )\nprint( len(list_of_numbers) )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-12",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-12",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nBoolean data have either True or False value.\nThere are two types of operation that are associated with booleans: boolean operations.\n\nExisting booleans are combined, and condition operations, which create a boolean when executed."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-13",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-13",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nConditions are expressions that evaluate as booleans."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-14",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-14",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nThe == is an operator that compares the objects on either side and returns True if they have the same values\n\nboolean_condition1 = 10 == 20\nprint(boolean_condition1)\n\nboolean_condition2 = 10 == '10'\nprint(boolean_condition2)\nQ. What does not (not True) evaluate to?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-15",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-15",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nThe real power of conditions comes when we start to use them in more complex examples, such as if statements.\n\nname = \"Geneseo\"\nscore = 99\n\nif name == \"Geneseo\" and score &gt; 90:\n    print(\"Geneseo, you achieved a high score.\")\n\nif name == \"Geneseo\" or score &gt; 90:\n    print(\"You could be called Geneseo or have a high score\")\n\nif name != \"Geneseo\" and score &gt; 90:\n    print(\"You are not called Geneseo and you have a high score\")"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-16",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-16",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nGiven that == and != test for equality and not equal, respectively, you may be wondering what the keyword is is for.\n\nRemember that everything in Python is an object, and that values can be assigned to objects.\n== and != compare values, while is compare objects.\n\n\n\n\nname_list = [\"Ada\", \"Adam\"]\nname_list_two = [\"Ada\", \"Adam\"]\n\n# Compare values\nprint(name_list == name_list_two)\n\n# Compare objects\nprint(name_list is name_list_two)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-17",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-17",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nOne of the most useful conditional keywords is in.\n\nThis one must pop up ten times a day in most coders’ lives because it can pick out a variable or make sure something is where it’s supposed to be.\n\n\nname_list = [\"Lovelace\", \"Smith\", \"Hopper\", \"Babbage\"]\n\nprint(\"Lovelace\" in name_list)\n\nprint(\"Bob\" in name_list)\nQ. Check if “a” is in the string “Sun Devil Arena” using in. Is “a” in “Anyone”?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-18",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-18",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nOne conditional construct we’re bound to use at some point, is the if-else chain:\n\nscore = 98\n\nif score == 100:\n    print(\"Top marks!\")\nelif score &gt; 90 and score &lt; 100:\n    print(\"High score!\")\nelif score &gt; 10 and score &lt;= 90:\n    pass\nelse:\n    print(\"Better luck next time.\")"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-19",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-19",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nQ. Create a new if-else chain that prints “well done” if a score is over 90, “good” if between 40 and 90, and “bad luck” otherwise."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-20",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-20",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nWe can make multiple assignment or multiple boolean comparisons in a single line.\n\na, b = 3, 6\n\n1 &lt; a &lt; b &lt; 20"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-1",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-1",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nBuilt-in Aggregation Methods\n\n(1)(2)(3)(4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know we can calculate multiple summary statistics simultaneously with .describe().\n## group by continent and describe each group\ncontinent_describe = df.groupby('continent')[\"lifeExp\"].describe()\ncontinent_describe"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-2",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-2",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nAggregation Functions\nFunctions From Other Libraries\nWe can also use other libraries’ functions that are not listed in the previous tables. (e.g., numpy, scipy)\n## calculate the average life expectancy by continent\n## but use the np.mean function\ncont_le_agg = df.groupby('continent')[\"lifeExp\"].agg(np.mean)\nQ. Add a new variable, the log of lifeExp, using np.log to DataFrame df."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-3",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-3",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nAggregation Functions\n\n(1)(2)(3)(4)(5)\n\n\nSometimes we may want to perform a calculation that is not provided by Pandas or another library.\n\nLet’s create our own mean function with def.\n\ndef my_mean(values):\n  n = len(values)    ## get the total number of numbers for the denominator\n  sum = 0   ## start the sum at 0\n  for value in values:\n      sum += value   ## add each value to the running sum\n  return sum / n   ## return the summed values divided by the number of values\n\n\nWe can pass our custom function straight into the .agg() method with my_mean.\n## use our custom function into agg\nagg_my_mean = df.groupby('year')[\"lifeExp\"].agg(my_mean)\n\nagg_my_mean\n\n\nWe can write functions that take multiple parameters.\ndef my_mean_diff(values, diff_value):\n    \"\"\"Difference between the mean and diff_value\n    \"\"\"\n    n = len(values)\n    sum = 0\n    for value in values:\n        sum += value\n    mean = sum / n\n    return(mean - diff_value)\n\n\nUsing my_mean_diff, we will calculate the global average life expectancy, diff_value, and subtract it from each grouped value.\n## custom aggregation function with multiple parameters\nagg_mean_diff = (\n  df\n  .groupby(\"year\")[\"lifeExp\"]\n  .agg(my_mean_diff, diff_value = global_mean)\n)\n\n\nWhen we want to calculate multiple functions, we can pass the multiple functions into .agg()\n\n## calculate the count, mean, std of the lifeExp by continent\ngdf = (\n  df\n  .groupby(\"year\")\n  [\"lifeExp\"]\n  .agg([np.count_nonzero, np.mean, np.std])\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-4",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-4",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nUse a dict in .agg() on a Series\nWe can also pass .agg() a Python dictionary ({key : value}). - The results will differ depending on whether we are aggregating directly on a DataFrame or on a Series object. - When specifying a dict on a grouped DataFrame, the keys are the columns of the DataFrame, and the values are the functions used in the aggregated calculation.\ngdf_dict = df.groupby(\"year\").agg(\n  {\"lifeExp\": \"mean\",\n    \"pop\": \"median\",\n    \"gdpPercap\": \"median\"})"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-5",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-5",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nUse a dict in .agg() on a DataFrame\nTo have user-defined column names in the output of a grouped series calculation, we need to rename those columns.\n\n\ngdf = (\n  df\n  .groupby(\"year\")\n  [\"lifeExp\"]\n  .agg(\n    [\n      np.count_nonzero,\n      np.mean,\n      np.std,\n    ]\n  )\n\n  .rename(\n    columns={\n      \"count_nonzero\": \"count\",\n      \"mean\": \"avg\",\n      \"std\": \"std_dev\",\n    }\n  )\n  .reset_index() ## return a flat dataframe\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-6",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-6",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform\nWhen we transform data, we pass values from our dataframe into a function.\n\nThe function then “transforms” the data.\nUnlike .agg(), which can take multiple values and return a single (aggregated) value, .transform() takes multiple values and returns a one-to-one transformation of the values.\nThat is, it does not reduce the amount of data."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-7",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-7",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\n\nLet’s calculate the z-score of our life expectancy data by year. \\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\n\\(x\\) is a value in a variable\n\\(\\mu\\) is the average of a variable\n\\(\\sigma\\) is the standard deviation of a variable\n\n\\[\n\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n}(x - \\mu)^{2}}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-8",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-8",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\nThe following is a function, my_zscore(x), that calculates a z-score using Pandas methods.\ndef my_zscore(x):\n  '''Calculates the z-score of provided data\n  'x' is a vector or series of values\n  '''\n  return((x - x.mean()) / x.std())"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-9",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-9",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\nNow we can use this function to .transform() our data by group.\ntransform_z = df.groupby('year')[\"lifeExp\"].transform(my_zscore)\n\nNote that both df and transform_z have the same number of rows and data.\n\ndf.shape\ntransform_z.shape"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-10",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-10",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\nThe scipy library has its own zscore() function.\nLet’s use its zscore() function in a .groupby() .transform() and compare it to what happens when we do not use .groupby().\nfrom scipy.stats import zscore\n## calculate a grouped zscore\nsp_z_grouped = df.groupby('year')[\"lifeExp\"].transform(zscore)\n\n## calculate a nongrouped zscore\nsp_z_nogroup = zscore(df[\"lifeExp\"])\n\ntransform_z.head()\nsp_z_grouped.head()\nsp_z_nogroup[:5]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-11",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-11",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nagg, transform, and apply: when to use each with a groupby\nWith all of the different options available, it can be confusing to know when to use the different functions available for performing groupby operations, namely: .agg, .transform, and .apply.\nHere are the key points to remember: - Use .agg when using a groupby, but you want your groups to become the new index. - Use .transform when using a groupby, but you want to retain your original index. - Use .apply when using a groupby, but you want to perform operations that will leave neither the original index nor an index of groups."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-12",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-12",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\n.filter()\n.filter() allows us to split our data by keys, and then perform some kind of boolean subsetting on the data.\n\nLet’s work with the tips data set from seaborn:\n\nimport seaborn as sns\ntips = sns.load_dataset('tips')\n\n## note the number of rows in the original data\ntips.shape\n\n\n## look at the frequency counts for the table size\ntips['size'].value_counts()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-13",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-13",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\n.filter() (cont.)\nSuppose we want each group to consist of 30 or more observations. - To accomplish this goal, we can use the .filter() method on a grouped operation.\n## filter the data such that each group has more than 30 observations\ntips_filtered = (\n  tips\n  .groupby(\"size\")\n  .filter( lambda x : x[\"size\"].count() &gt;= 30 )\n)\n\ntips_filtered.shape\ntips_filtered['size'].value_counts()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-14",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-14",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nThe pandas.core.groupby.DataFrameGroupBy object\nThe .agg(), .transform(), and .filter() methods are commonly used after the .groupby().\ntips_10 = sns.load_dataset('tips').sample(10, random_state=42)\nprint(tips_10)\n\nWe can choose to save just the groupby object without running any other .agg(), .transform(), or .filter() method on it.\n\n\n\n## save just the grouped object\ngrouped = tips_10.groupby('sex')\ngrouped\n\n## see the actual groups of the groupby\n## it returns only the index\ngrouped.groups  ## row numbers?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-15",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-15",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nGroup Calculations Involving Multiple Variables\n\nWe have been usually performing .groupby() calculations on a single column.\n\nIf we specify the calculation we want right after the .groupby(), however, Python will perform the calculation on all the columns it can and silently drop the rest.\n\n\n## calculate the mean on relevant columns\navgs = grouped.mean()\navgs\n\n## list all the columns\ntips_10.columns  ## not all the columns reported a mean."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-16",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-16",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nSelecting a Group\nIf we want to extract a particular group, we can use the .get_group() method, and pass in the group that we want.\n\nFor example, if we wanted the Female values:\n\n## get the 'Female' group\nfemale = grouped.get_group('Female')\nfemale"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-17",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-17",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nIterating Through Groups\n\nAnother benefit of saving just the groupby object is that we can then iterate through the groups individually.\nWe can iterate through our grouped values just like any other container (e.g., list, dictionary) in Python using a for-loop.\n\nfor sex_group in grouped:\n    print(sex_group)\n    \n## we can't really get the 0 element from the grouped object\nprint(grouped[0])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-18",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-18",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nIterating Through Groups (cont.)\n\nLet’s modify the for loop to just show the first element, along with some of the things we get when we loop over the grouped object.\n\n\n\nfor sex_group in grouped:\n    type(sex_group) ## tuple\n    len(sex_group) ## length \n\n    ## get the first element\n    first_element = sex_group[0]\n    first_element\n    type(sex_group[0])\n\n    ## get the second element\n    second_element = sex_group[1]\n    second_element\n    type(second_element)\n\n    ## print what we have\n    print(f'what we have:')\n    print(sex_group)\n\n    ## stop after first iteration\n    break"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-19",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-19",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nIterating Through Groups (cont.)\n\nThe option of iterating through groups with for-loop is available for us if we need to iterate through the groups one at a time."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-20",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-20",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nMultiple Groups\n\nWe can add multiple variables to the .groupby()\n\n## mean by sex and time\nbill_sex_time = tips_10.groupby(['sex', 'time'])\n\ngroup_avg = bill_sex_time.mean()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-21",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-21",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nFlattening the Results (.reset_index())\n\nLet’s look at the type of the group_avg we just calculated.\n\ntype(group_avg)\n\nIf we look at the columns and the index, we get what we expect.\n\ngroup_avg.columns\ngroup_avg.index"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-22",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-22",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nFlattening the Results (.reset_index())\n\nIf we want to get a regular flat DataFrame back, we can call the .reset_index() method on the results.\n\ngroup_method = tips_10.groupby(['sex', 'time']).mean().reset_index()\ngroup_method\n\nAlternatively, we can use the as_index = False parameter in the .groupby() method (it is True by default).\n\ngroup_param = tips_10.groupby(['sex', 'time'], as_index=False).mean()\ngroup_param"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-23",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-23",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nSometimes, we may want to chain calculations after a .groupby() method. - We can always “flatten” the results and then execute another .groupby() statement, but that may not always be the most efficient way of performing the calculation.\nDownload epi_sim.zip from the Files section in Canvas.\nLet’s begin with this epidemiological simulation data on influenza cases in Chicago.\n## notice that we can even read a compressed zip file of a csv\nintv_df = pd.read_csv('data/epi_sim.zip')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-24",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-24",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nThe data set includes six columns: - ig_type: edge type (type of relationship between two nodes in the network, such as “school” and “work”) - intervened: time in the simulation at which an intervention occurred for a given person (pid) - pid: simulated person’s ID number - rep: replication run (each set of simulation parameters was run multiple times) - sid: simulation ID - tr: transmissibility value of the influenza virus"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-25",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-25",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nLet’s count the number of interventions for each replicate, intervention time, and treatment value. - Here, we are counting the ig_type arbitrarily. - We just need a value to get a count of observations for the groups.\ncount_only = (\n  intv_df\n  .groupby([\"rep\", \"intervened\", \"tr\"])\n  [\"ig_type\"]\n  .count()\n)\n\ncount_only"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-26",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-26",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nNow that we’ve done a .groupby() .count(), we can perform an additional .groupby() that calculates the average value. - However, our initial .groupby() method does not return a regular flat DataFrame.\ntype(count_only)\nThe results take the form of a multi-index Series."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-27",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-27",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nIf we want to do another .groupby() operation, we have to pass in the levels parameter to refer to the multi-index levels. - Here we pass in [0, 1, 2] for the first, second, and third index levels, respectively.\ncount_mean = count_only.groupby(level=[0, 1, 2]).mean()\ncount_mean.head()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-28",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-28",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nWe can combine all of these operations in a single command.\ncount_mean = (\n    intv_df\n    .groupby([\"rep\", \"intervened\", \"tr\"])[\"ig_type\"]\n    .count()\n    .groupby(level=[0, 1, 2])\n    .mean()\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-29",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-29",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nSee how the relationship between intervened and ig_type varies by rep and tr.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfig = sns.lmplot(\n   data = count_mean.reset_index(),\n   x = \"intervened\",\n   y = \"ig_type\",\n   hue = \"rep\",\n   col = \"tr\",\n   fit_reg = False,\n   palette = \"viridis\" )\n   \nplt.show()"
  },
  {
    "objectID": "listing-danl-m1-lec.html",
    "href": "listing-danl-m1-lec.html",
    "title": "DANL Module 1 - Lecture",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\nLecture 7\n\n\nMarch 19, 2023\n\n\n\n\nLecture 6\n\n\nMarch 12, 2023\n\n\n\n\nLecture 5\n\n\nMarch 5, 2023\n\n\n\n\nLecture 4\n\n\nFebruary 27, 2023\n\n\n\n\nLecture 3\n\n\nFebruary 20, 2023\n\n\n\n\nLecture 2\n\n\nFebruary 13, 2023\n\n\n\n\nLecture 1\n\n\nFebruary 6, 2023\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-07.html",
    "href": "danl-qa/danl-m1-qa-07.html",
    "title": "Lecture 7 - Discussion and Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-04.html",
    "href": "danl-qa/danl-m1-qa-04.html",
    "title": "Lecture 4 - Discussion and Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-01.html",
    "href": "danl-qa/danl-m1-qa-01.html",
    "title": "Lecture 1 - Discussion and Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-03.html",
    "href": "danl-qa/danl-m1-qa-03.html",
    "title": "Lecture 3 - Discussion and Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html",
    "href": "danl-hw/danl-m1-hw-2.html",
    "title": "Homework Assignment 2",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here.\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1a",
    "href": "danl-hw/danl-m1-hw-2.html#q1a",
    "title": "Homework Assignment 2",
    "section": "Q1a",
    "text": "Q1a\nWhat are the minimum, first quartile, median, thrid quartile, maximum, mean, and standard deviation of Close and Volume for each company?\n\n# Group stock data by company and get descriptive statistics for the Close and Volume columns\nq1a = stock.groupby('company')[['Close', 'Volume']].describe()\n\n# Rename columns to have the format 'column_name'_'statistic'\nq1a.columns = q1a.columns.get_level_values(0) + '_' +\\\n    q1a.columns.get_level_values(1)"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1b",
    "href": "danl-hw/danl-m1-hw-2.html#q1b",
    "title": "Homework Assignment 2",
    "section": "Q1b",
    "text": "Q1b\nFind the 10 largest values for Volume. What are the companies and dates associated with those 10 largest values for Volume?\n\n# Calculate dense ranking of stock volumes and add it as a new column to a copy of the stock dataframe\nranking = stock['Volume'].rank(method='dense', ascending=False)\nq1b = stock.copy()\nq1b['ranking'] = ranking\n\n# Sort the dataframe by ranking and select the top 10 stocks\nq1b = (\n  q1b\n  .sort_values(by='ranking')\n  .query('ranking &lt;= 10')\n)"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1c",
    "href": "danl-hw/danl-m1-hw-2.html#q1c",
    "title": "Homework Assignment 2",
    "section": "Q1c",
    "text": "Q1c\nCalculate the Z-scores of Open and Close for each company using apply().\n\nq1c = (\n  stock.set_index(['company', 'Date'])  # set company and Date as the index\n  .groupby('company', group_keys=False) #  to prevent pandas from keeping the group keys in the resulting DataFrame\n  .apply(lambda x: ( x - x.mean() ) / x.std() )  # standardize the Close and Volume for each company\n  .reset_index()                       # reset the index to default\n)"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1d",
    "href": "danl-hw/danl-m1-hw-2.html#q1d",
    "title": "Homework Assignment 2",
    "section": "Q1d",
    "text": "Q1d\nUse the transform() method on the stock data to represent all the values of Open, High, Low, Close, Adj Close, and Volume in terms of the first date in the data.\nTo do so, divide all values for each company by the values of the first date in the data for that company.\n\n# Set multi-index for DataFrame using 'company' and 'Date' columns\nq1d = stock.set_index(['company', 'Date'])\n\n# Divide each element of q1d by the first element of the corresponding 'company' group\nq1d = ( q1d / q1d.groupby('company').transform('first') )"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1e",
    "href": "danl-hw/danl-m1-hw-2.html#q1e",
    "title": "Homework Assignment 2",
    "section": "Q1e",
    "text": "Q1e\nProvide both seaborn code and a simple comment to describe the daily trend of normalized values of Close for each company in one plot. The normalized values of Close are the one calculated from Q1d.\n\ng = sns.lineplot(q1d, # create a line plot using Seaborn\n             x = 'Date', y = 'Close', # specify x and y axes\n             hue = 'company') # specify color grouping by company\n\nimport matplotlib.dates as mdates # import Matplotlib date module\ng.xaxis.set_major_locator(mdates.YearLocator()) # set x-axis ticks to show year\nplt.xticks(rotation=45) # rotate x-axis labels for better readability\n\n(array([-365.,    0.,  365.,  730., 1096., 1461., 1826., 2191., 2557.]),\n [Text(-365.0, 0, ''),\n  Text(0.0, 0, '2013-01-02'),\n  Text(365.0, 0, '2014-06-16'),\n  Text(730.0, 0, '2015-11-24'),\n  Text(1096.0, 0, '2017-05-10'),\n  Text(1461.0, 0, '2018-10-19'),\n  Text(1826.0, 0, '2020-04-03'),\n  Text(2191.0, 0, '2021-09-15'),\n  Text(2557.0, 0, '2023-03-01')])"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1f",
    "href": "danl-hw/danl-m1-hw-2.html#q1f",
    "title": "Homework Assignment 2",
    "section": "Q1f",
    "text": "Q1f\nCreate a box plot of Close for each company in one plot. Make a simple comment on the plot.\n\nsns.boxplot(stock, \n             x = 'company', y = 'Close')\n\n&lt;Axes: xlabel='company', ylabel='Close'&gt;"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q2a",
    "href": "danl-hw/danl-m1-hw-2.html#q2a",
    "title": "Homework Assignment 2",
    "section": "Q2a",
    "text": "Q2a\nHow many parties have provided or disbursed positive funding contributions to other countries or regions for their adaptation projects for every single year from 2011 to 2018?\n\nq2a = (\n  climate_finance\n       .query('Status == \"provided\" or Status == \"disbursed\"') # Select only the rows where status is \"provided\" or \"disbursed\"\n       .query('`Type of support` == \"adaptation\"') # Select only the rows where the type of support is \"adaptation\"\n       .groupby(['Party', 'Year']) # Group the data by party and year\n       .agg({'Contribution': 'sum'}) # Calculate the sum of contributions for each party and year\n       .reset_index() # Reset the index of the dataframe\n       .query('Contribution &gt; 0') # Select only the rows where the contribution is greater than 0\n       .groupby(['Party']) # Group the data by party\n       .size() # Count the number of rows for each party\n       .reset_index(name='n') # Reset the index of the dataframe and rename the \"size\" column to \"n\"\n       .query('n == 8') # Select only the rows where the value of \"n\" is 8\n       )\n\nq2a.shape[0] # Output the number of rows in the resulting dataframe\n      \nq2a\n\n\n\n\n\n\n\n\nParty\nn\n\n\n\n\n2\nCanada\n8\n\n\n10\nIceland\n8\n\n\n11\nIreland\n8\n\n\n15\nNetherlands\n8\n\n\n16\nNew Zealand\n8\n\n\n19\nPortugal\n8\n\n\n21\nSlovakia\n8\n\n\n24\nSwitzerland\n8"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q2b",
    "href": "danl-hw/danl-m1-hw-2.html#q2b",
    "title": "Homework Assignment 2",
    "section": "Q2b",
    "text": "Q2b\nFor each party, calculate the total funding contributions that were disbursed or provided for mitigation projects for each year.\n\nq2b = (\n  climate_finance\n       .query('Status == \"provided\" or Status == \"disbursed\"')\n       .query('`Type of support` == \"mitigation\"')\n       .groupby(['Party', 'Year'])\n       .agg({'Contribution': 'sum'})\n       .reset_index()\n       )\n\nq2b\n\n\n\n\n\n\n\n\nParty\nYear\nContribution\n\n\n\n\n0\nAustralia\n2013\n3832000.00\n\n\n1\nAustria\n2015\n91424141.76\n\n\n2\nBelgium\n2013\n54096469.42\n\n\n3\nBelgium\n2014\n29152327.24\n\n\n4\nBelgium\n2015\n15182584.02\n\n\n...\n...\n...\n...\n\n\n117\nSwitzerland\n2014\n81716104.00\n\n\n118\nSwitzerland\n2015\n94186259.00\n\n\n119\nSwitzerland\n2016\n97202955.00\n\n\n120\nSwitzerland\n2017\n93447999.02\n\n\n121\nSwitzerland\n2018\n92704380.68\n\n\n\n\n122 rows × 3 columns"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q2c",
    "href": "danl-hw/danl-m1-hw-2.html#q2c",
    "title": "Homework Assignment 2",
    "section": "Q2c",
    "text": "Q2c\nFor each party, calculate the ratio between adaptation contribution and mitigation contribution for each type of Status for each year.\n\n# Groupby Party, Year, Status, and Type of support and sum the contribution\nq2c_tmp = (\n    climate_finance\n    .groupby(['Party', 'Year', 'Status', 'Type of support'])\n    .agg({'Contribution': 'sum'})\n    .reset_index()    \n    ) \n\n# Filter out rows where the Contribution is 0 and keep only rows with a length of 2 groups\nq2c_tmp = q2c_tmp[ q2c_tmp['Contribution'] != 0 ]\nq2c_tmp = (\n  q2c_tmp\n  .groupby(['Party', 'Year', 'Status'])\n  .filter(lambda x: len(x) == 2)\n  )\n\n# Create a separate dataframe for adaptation and mitigation contributions\nq2ca = q2c_tmp[q2c_tmp['Type of support'] == 'adaptation']\nq2cm = q2c_tmp[q2c_tmp['Type of support'] == 'mitigation']\n\n# Drop the Type of support column, rename the Contribution column, and compute the adaptation to mitigation ratio\nq2c = (\n   q2ca\n   .drop('Type of support', axis=1)\n   .rename(columns={'Contribution': 'adaptation'})\n   .assign(mitigation = q2cm['Contribution'].values,\n           am_ratio = lambda x: x['adaptation'] / x['mitigation'])\n       )"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q2d",
    "href": "danl-hw/danl-m1-hw-2.html#q2d",
    "title": "Homework Assignment 2",
    "section": "Q2d",
    "text": "Q2d\nProvide both seaborn code and a simple comment to describe the distribution of the ratio between adaptation contribution and mitigation contribution, which is calculated in Q2c.\n\n# Graph 1\nsns.histplot(q2c, x='am_ratio', bins=100)\nplt.axvline(x=1, color='red', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x17b6d1ad0&gt;\n\n\n\n\n\n\n# Graph 2\nsns.histplot(q2c, x=np.log(q2c['am_ratio']), bins=100)\nplt.axvline(x=0, color='red', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x17d4dbf50&gt;\n\n\n\n\n\n\n# Graph 3\nsns.kdeplot(q2c, x='am_ratio')\nplt.axvline(x=1, color='red', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x17d055310&gt;\n\n\n\n\n\n\n# Graph 4\nsns.kdeplot(q2c, x=np.log(q2c['am_ratio']))\nplt.axvline(x=0, color='red', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x17d581f90&gt;"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q2e",
    "href": "danl-hw/danl-m1-hw-2.html#q2e",
    "title": "Homework Assignment 2",
    "section": "Q2e",
    "text": "Q2e\nProvide both seaborn code and a simple comment to describe how the distribution of Contribution varies by Type of support and Status.\n\n# Graph 1\ng = sns.FacetGrid(data=climate_finance, \n                  row='Status', \n                  hue='Type of support')\ng.map(sns.histplot, \n      'Contribution',\n      alpha = .25)\n\n\n\n\n\n# Log transformation\nclimate_finance['log_contribution'] = np.log(climate_finance['Contribution'])\n\n\n# Graph 2\ng = sns.FacetGrid(data=climate_finance, \n                  row='Status', \n                  hue='Type of support')\ng.map(sns.histplot, \n      'log_contribution',\n      alpha = .25)\n\n\n\n\n\n# Graph 3\ng = sns.FacetGrid(data=climate_finance, \n                  row='Status', \n                  hue='Type of support')\ng.map(sns.kdeplot, 'Contribution')\n\n\n\n\n\n# Graph 4\ng = sns.FacetGrid(data=climate_finance, \n                  row='Status', \n                  hue='Type of support')\ng.map(sns.kdeplot, 'log_contribution')"
  },
  {
    "objectID": "listing-danl-m1-hw.html",
    "href": "listing-danl-m1-hw.html",
    "title": "DANL Module 1 - Homework",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nHomework Assignment 1\n\n\nDANL Module 1\n\n\nJanuary 16, 2024\n\n\n\n\nHomework Assignment 2\n\n\nDANL Module 1\n\n\nJanuary 16, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]