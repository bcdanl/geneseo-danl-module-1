[
  {
    "objectID": "listing-danl-m1-qa.html",
    "href": "listing-danl-m1-qa.html",
    "title": "DANL 210 - Lecture Discussion",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\nLecture 7 - Q & A\n\n\nMarch 19, 2024\n\n\n\n\nLecture 6 - Q & A\n\n\nMarch 12, 2024\n\n\n\n\nLecture 5 - Q & A\n\n\nMarch 5, 2024\n\n\n\n\nLecture 4 - Q & A\n\n\nFebruary 27, 2024\n\n\n\n\nLecture 3 - Q & A\n\n\nFebruary 20, 2024\n\n\n\n\nLecture 2 - Q & A\n\n\nFebruary 13, 2024\n\n\n\n\nLecture 1 - Q & A\n\n\nFebruary 6, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html",
    "href": "danl-hw/danl-m1-hw-1.html",
    "title": "Homework Assignment 1",
    "section": "",
    "text": "Direction\n\nWrite a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files.\n\n\n\nLoad Libraries\n\nImport all the Python libraries we need here.\n\n\n\nQuestion 1\nWrite a Python code that uses a list comprehension to create a list of ages that are over 30 based on the ages list.\n\nAnswer\n\n\n\nQuestion 2\nWrite a Python code that uses (1) input() function with a message in the Console “Please enter a number less than 100:” and (2) if-else statement to print (a) “The value entered is greater than or equal to 75.” if the value entered from the Spyder Console is greater than or equal to 75, and (b) “The value entered is greater than or equal to 50.” and “The value entered is also less than 75.” if the value entered from the Spyder Console is less than 75 and greater than or equal to 50, and (c) “The value entered is less than 50.” otherwise.\n\nAnswer\n\n\n\nQuestion 3\nWrite a Python code that uses a for-loop statement to process each element of sequence to calculate the sum of all the numeric values in the sequence list.\n\nAnswer\n\n\n\nQuestion 4\nWrite a Python code that uses a for-loop statement and a if-else statement to create a Wordle dictionary for which a key is an alphabet and a value is a list of words.\n\nAnswer\n\n\n\nQuestion 5\nWrite a Python code that uses (1) a dictionary, Wordle_by_letter (2) sorted(), and (3) dictionary comprehension to create an alphabetically-ordered Wordle dictionary for which a key is an alphabet and a value is a list of words.\n\nAnswer\n\n\n\nQuestion 6\nThis Python code uses a function to look up code meanings for variables, region and happy, using dictionaries:\nWrite a Python code that uses the function code_lookup() and the print() function to print the followings:\n\nInterview region: East North Central\n\n\nHappiness level: Pretty happy\n\n\nAnswer\n\n\n\n\n\n\n Back to topReuse© 2024 Byeong-Hak Choe | This post is licensed under &lt;a href='http://creativecommons.org/licenses/by-nc-sa/4.0/' target='_blank'&gt;CC BY-NC-SA 4.0&lt;/a&gt;.CitationFor attribution, please cite this work as:\nChoe, Byeong-Hak. 2024. “Homework Assignment 1.” February\n6, 2024."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html",
    "href": "danl-hw/danl-m1-hw-2.html",
    "title": "Homework Assignment 2",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1a",
    "href": "danl-hw/danl-m1-hw-2.html#q1a",
    "title": "Homework Assignment 2",
    "section": "Q1a",
    "text": "Q1a\nWrite a Python code that uses the list, a, to create the following Numpy Array:\n\narray([0.1, 1.2, 2.3, 3.4, 4.5])\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1b",
    "href": "danl-hw/danl-m1-hw-2.html#q1b",
    "title": "Homework Assignment 2",
    "section": "Q1b",
    "text": "Q1b\nWrite a Python code that uses the list, a, to create the following pandas Series:\n\n########################\n# Index      0\n\n# 0         0.1\n# 1         1.2\n# 2         2.3\n# 3         3.4\n# 4         4.5\n\n# dtype: float64    \n########################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1c",
    "href": "danl-hw/danl-m1-hw-2.html#q1c",
    "title": "Homework Assignment 2",
    "section": "Q1c",
    "text": "Q1c\nWrite a Python code that uses the list, a, to create the following pandas Series:\n\n########################\n# Index      0\n\n# a         0.1\n# b         1.2\n# c         2.3\n# d         3.4\n# e         4.5\n\n# dtype: float64    \n########################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1d",
    "href": "danl-hw/danl-m1-hw-2.html#q1d",
    "title": "Homework Assignment 2",
    "section": "Q1d",
    "text": "Q1d\nWrite a Python code that uses (1) the Series created in Q1c and (2) Boolean indexing to get the following Series:\n\n########################\n# Index      0\n\n# c         2.3\n# d         3.4\n# e         4.5\n\n# dtype: float64    \n########################  \n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q2a",
    "href": "danl-hw/danl-m1-hw-2.html#q2a",
    "title": "Homework Assignment 2",
    "section": "Q2a",
    "text": "Q2a\nWrite a Python code that uses the list, hh_income, to assign the object, hh_income_array, to the following Numpy array:\n\n############################\n\n# array([[    10,  14629],\n#        [    20,  25600],\n#        [    30,  37002],\n#        [    40,  50000],\n#        [    50,  63179],\n#        [    60,  79542],\n#        [    70, 100162],\n#        [    80, 130000],\n#        [    90, 184292]])\n\n############################\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q2b",
    "href": "danl-hw/danl-m1-hw-2.html#q2b",
    "title": "Homework Assignment 2",
    "section": "Q2b",
    "text": "Q2b\nWrite a Python code that uses the print() function to report the dimensions of the ndarray and the number of elements in hh_income_array as follows:\n\nDimensions of the NumPy array, hh_income_array, is: (9, 2)\n\n\nNumber of elements in the NumPy array, hh_income_array, is: 18\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q3a.",
    "href": "danl-hw/danl-m1-hw-2.html#q3a.",
    "title": "Homework Assignment 2",
    "section": "Q3a.",
    "text": "Q3a.\nWrite a Python code that uses the NumPy array, c, to create the following DataFrame:\n\n############################\n\n# index     0    1\n# 0         1.0  2.0\n# 1         3.0  4.0\n\n############################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q3b.",
    "href": "danl-hw/danl-m1-hw-2.html#q3b.",
    "title": "Homework Assignment 2",
    "section": "Q3b.",
    "text": "Q3b.\nWrite a Python code that uses the NumPy array, c, to create the following DataFrame:\n\n############################\n\n# index     dogs    cats\n# 0         1.0     2.0\n# 1         3.0     4.0\n\n############################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q3c.",
    "href": "danl-hw/danl-m1-hw-2.html#q3c.",
    "title": "Homework Assignment 2",
    "section": "Q3c.",
    "text": "Q3c.\nWrite a Python code that uses the NumPy array, c, to create the following DataFrame:\n\n############################\n\n# index             dogs    cats\n# byeong-hak        1.0     2.0\n# your_first_name   3.0     4.0\n\n############################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4a",
    "href": "danl-hw/danl-m1-hw-2.html#q4a",
    "title": "Homework Assignment 2",
    "section": "Q4a",
    "text": "Q4a\nRead the data file, US_state_GDP.csv, as the object name, state_gdp, using (1) path_csv and (2) pd.read_csv() function.\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4b",
    "href": "danl-hw/danl-m1-hw-2.html#q4b",
    "title": "Homework Assignment 2",
    "section": "Q4b",
    "text": "Q4b\nWrite a Python code that uses the DataFrame, state_gdp, to create the DataFrame, whose first five rows are as follows:\n\n############################################\n\n# index    state_code           state\n\n# 0          AK                Alaska\n# 1          AL               Alabama\n# 2          AR              Arkansas\n# 3          AZ               Arizona\n# 4          CA            California\n\n############################################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4c",
    "href": "danl-hw/danl-m1-hw-2.html#q4c",
    "title": "Homework Assignment 2",
    "section": "Q4c",
    "text": "Q4c\nWrite a Python code that uses (1) the DataFrame, state_gdp, and (2) state_gdp.columns to create the DataFrame, whose first five rows are as follows:\n\n############################################\n\n#                    state  gdp_2009\n\n# 0                 Alaska     44215\n# 1                Alabama    149843\n# 2               Arkansas     89776\n# 3                Arizona    221405\n# 4             California   1667152\n\n############################################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4d",
    "href": "danl-hw/danl-m1-hw-2.html#q4d",
    "title": "Homework Assignment 2",
    "section": "Q4d",
    "text": "Q4d\nWrite a Python code to get the first three rows of the DataFrame, state_gdp:\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4e",
    "href": "danl-hw/danl-m1-hw-2.html#q4e",
    "title": "Homework Assignment 2",
    "section": "Q4e",
    "text": "Q4e\nWrite a Python code to get all the rows of the DataFrame, state_gdp, for which the value of gdp_growth_2010 is less than 0\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4f",
    "href": "danl-hw/danl-m1-hw-2.html#q4f",
    "title": "Homework Assignment 2",
    "section": "Q4f",
    "text": "Q4f\nWrite a Python code that uses state_gdp.loc[] to get the following DataFrame:\n\n\n############################################\n\n#       state  gdp_growth_2010\n\n# 0    Alaska             -1.7\n# 3   Arizona             -0.2\n# 33   Nevada             -0.4\n# 50  Wyoming             -1.3\n\n############################################\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4g",
    "href": "danl-hw/danl-m1-hw-2.html#q4g",
    "title": "Homework Assignment 2",
    "section": "Q4g",
    "text": "Q4g\nWrite a Python code that uses state_gdp.iloc[] to get the following DataFrame:\n\n\n############################################\n\n#    state_code     state\n\n# 10         GA   Georgia\n# 11         HI    Hawaii\n# 12         IA      Iowa\n# 13         ID     Idaho\n# 14         IL  Illinois\n\n############################################\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-5.html",
    "href": "danl-hw/danl-m1-hw-5.html",
    "title": "Homework Assignment 5",
    "section": "",
    "text": "Direction\n\nWrite a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here.\n\n\n\n\nQuestion 1\nConcatenate the two weeks of sales data into one DataFrame. Assign the week1 DataFrame a key of “Week 1” and the week2 DataFrame a key of “Week 2”.\n\n\n\nQuestion 2\nFind the customers who ate at the restaurant both weeks.\n\n\n\nQuestion 3\nFind the customers who ate at the restaurant both weeks and ordered the same item each week.\n\nHINT You can join data sets on multiple columns by passing the on parameter a list of columns.\n\n\n\n\nQuestion 4\nIdentify which customers came in only on Week 1 and only on Week 2.\n\n\n\nQuestion 5\nEach row in the week1 DataFrame identifies a customer who purchased a food item. For each row, pull in the customer’s information from the customers DataFrame.\n\n\n\n\n\n\n Back to topReuse© 2024 Byeong-Hak Choe | This post is licensed under &lt;a href='http://creativecommons.org/licenses/by-nc-sa/4.0/' target='_blank'&gt;CC BY-NC-SA 4.0&lt;/a&gt;.CitationFor attribution, please cite this work as:\nChoe, Byeong-Hak. 2024. “Homework Assignment 5.” March 5,\n2024."
  },
  {
    "objectID": "listing-danl-m1-cw.html",
    "href": "listing-danl-m1-cw.html",
    "title": "DANL 210 - Classwork",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nClasswork 7\n\n\nMissing Data; Time-series Data\n\n\nMarch 19, 2024\n\n\n\n\nClasswork 6\n\n\nMerging, Joining, and Concaternating DataFrames\n\n\nMarch 12, 2024\n\n\n\n\nClasswork 5\n\n\nReshaping and Pivoting DataFrames\n\n\nMarch 5, 2024\n\n\n\n\nClasswork 4\n\n\nGroup Operations with GroupBy Objects\n\n\nFebruary 27, 2024\n\n\n\n\nClasswork 3\n\n\nFiltering, Sorting, Ranking, and Visualizing DataFrames\n\n\nFebruary 20, 2024\n\n\n\n\nClasswork 2\n\n\nPandas Basics\n\n\nFebruary 13, 2024\n\n\n\n\nClasswork 1\n\n\nPython Basics\n\n\nFebruary 6, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-02.html",
    "href": "danl-qa/danl-m1-qa-02.html",
    "title": "Lecture 2 - Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-05.html",
    "href": "danl-qa/danl-m1-qa-05.html",
    "title": "Lecture 5 - Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-06.html",
    "href": "danl-qa/danl-m1-qa-06.html",
    "title": "Lecture 6 - Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DANL Module 1: Data Preparation and Management, 2024",
    "section": "",
    "text": "Welcome! 👋\n\\(-\\) Explore, Learn, and Grow with the DANL Microcredential! 🌟"
  },
  {
    "objectID": "index.html#bullet-lecture-slides",
    "href": "index.html#bullet-lecture-slides",
    "title": "DANL Module 1: Data Preparation and Management, 2024",
    "section": "\\(\\bullet\\,\\) Lecture Slides 🚀",
    "text": "\\(\\bullet\\,\\) Lecture Slides 🚀\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nLecture 7\n\n\nMarch 19, 2024\n\n\n\n\nLecture 6\n\n\nMarch 12, 2024\n\n\n\n\nLecture 5\n\n\nMarch 5, 2024\n\n\n\n\nLecture 4\n\n\nFebruary 27, 2024\n\n\n\n\nLecture 3\n\n\nFebruary 20, 2024\n\n\n\n\nLecture 2\n\n\nFebruary 13, 2024\n\n\n\n\nLecture 1\n\n\nFebruary 6, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-lecture-q-a",
    "href": "index.html#bullet-lecture-q-a",
    "title": "DANL Module 1: Data Preparation and Management, 2024",
    "section": "\\(\\bullet\\,\\) Lecture Q & A ❓",
    "text": "\\(\\bullet\\,\\) Lecture Q & A ❓\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nLecture 7 - Q & A\n\n\nMarch 19, 2024\n\n\n\n\nLecture 6 - Q & A\n\n\nMarch 12, 2024\n\n\n\n\nLecture 5 - Q & A\n\n\nMarch 5, 2024\n\n\n\n\nLecture 4 - Q & A\n\n\nFebruary 27, 2024\n\n\n\n\nLecture 3 - Q & A\n\n\nFebruary 20, 2024\n\n\n\n\nLecture 2 - Q & A\n\n\nFebruary 13, 2024\n\n\n\n\nLecture 1 - Q & A\n\n\nFebruary 6, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-classwork",
    "href": "index.html#bullet-classwork",
    "title": "DANL Module 1: Data Preparation and Management, 2024",
    "section": "\\(\\bullet\\,\\) Classwork ⌨️",
    "text": "\\(\\bullet\\,\\) Classwork ⌨️\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nClasswork 7\n\n\nMarch 19, 2024\n\n\n\n\nClasswork 6\n\n\nMarch 12, 2024\n\n\n\n\nClasswork 5\n\n\nMarch 5, 2024\n\n\n\n\nClasswork 4\n\n\nFebruary 27, 2024\n\n\n\n\nClasswork 3\n\n\nFebruary 20, 2024\n\n\n\n\nClasswork 2\n\n\nFebruary 13, 2024\n\n\n\n\nClasswork 1\n\n\nFebruary 6, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-homework",
    "href": "index.html#bullet-homework",
    "title": "DANL Module 1: Data Preparation and Management, 2024",
    "section": "\\(\\bullet\\,\\) Homework 💻",
    "text": "\\(\\bullet\\,\\) Homework 💻\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nHomework Assignment 6\n\n\nMarch 12, 2024\n\n\n\n\nHomework Assignment 5\n\n\nMarch 5, 2024\n\n\n\n\nHomework Assignment 4\n\n\nFebruary 27, 2024\n\n\n\n\nHomework Assignment 3\n\n\nFebruary 20, 2024\n\n\n\n\nHomework Assignment 2\n\n\nFebruary 13, 2024\n\n\n\n\nHomework Assignment 1\n\n\nFebruary 6, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-3.html",
    "href": "danl-cw/danl-m1-cw-3.html",
    "title": "Classwork 3",
    "section": "",
    "text": "Load Data\n\nimport pandas as pd\ndf_ny = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_pop.csv')\ndf_ny.head(10)\n\n\nVariable Description\n\nFIPS: ID number for a county\npincp: average personal income in a county X in year Y\npop_18_24: population 18 to 24 years\npop_25_over: population 25 years and over\n\n\n\n\nQ1a\n\nUse .sort_values() to find the top 5 rich counties in NY for each year.\n\nDo not use .apply().\n\n\n\n\nQ1b\n\nUse .rank() to find the top 5 rich counties in NY for each year.\n\nDo not use apply().\n\n\n\n\nQ1c\n\nUse apply() with a lambda function and .sort_values() to find the top 5 rich counties in NY for each year.\n\n\n\nQ1d\n\nWrite a function with def and .sort_values() that selects the top 5 pincp values.\nThen, use the defined function in apply() to find the top 5 rich counties in NY for each year.\n\n\n\ncf) SeriesGroupBy.nlargest()\n\n\nQ1e\n\nVisualize the yearly trend of the mean level of pincp.\n\n\n\n\n\n Back to topReuse© 2024 Byeong-Hak Choe | This post is licensed under &lt;a href='http://creativecommons.org/licenses/by-nc-sa/4.0/' target='_blank'&gt;CC BY-NC-SA 4.0&lt;/a&gt;.CitationFor attribution, please cite this work as:\nChoe, Byeong-Hak. 2024. “Classwork 3.” February 20, 2024."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html",
    "href": "danl-cw/danl-m1-cw-6.html",
    "title": "Classwork 6",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#load-libraries",
    "href": "danl-cw/danl-m1-cw-6.html#load-libraries",
    "title": "Classwork 6",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q1a",
    "href": "danl-cw/danl-m1-cw-6.html#q1a",
    "title": "Classwork 6",
    "section": "Q1a",
    "text": "Q1a\nWrite a Pandas code to join the two given DataFrames along rows and assign all data."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q1b",
    "href": "danl-cw/danl-m1-cw-6.html#q1b",
    "title": "Classwork 6",
    "section": "Q1b",
    "text": "Q1b\nWrite a Pandas code to join the two given DataFrames along columns and assign all data."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q1c",
    "href": "danl-cw/danl-m1-cw-6.html#q1c",
    "title": "Classwork 6",
    "section": "Q1c",
    "text": "Q1c\nConsider the following Pandas Series\nWrite a Pandas code to append rows to DataFrame student_data1 and display the combined data using DATAFRAME.append(SERIES, ignore_index = True)"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q2a",
    "href": "danl-cw/danl-m1-cw-6.html#q2a",
    "title": "Classwork 6",
    "section": "Q2a",
    "text": "Q2a\nMerge flights with weather.\n\npath = '/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/flights.csv'\nflights = pd.read_csv(path)\n\npath = '/Users/byeong-hakchoe/Google Drive/suny-geneseo/teaching-materials/lecture-data/weather.csv'\nweather = pd.read_csv(path)\n\nflights.columns\nweather.columns\n\nflights = flights.merge(weather,\n                        on = ['year', 'month', 'day', \n                              'hour', 'origin'],\n                        how = 'left')\nflights.columns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q2b",
    "href": "danl-cw/danl-m1-cw-6.html#q2b",
    "title": "Classwork 6",
    "section": "Q2b",
    "text": "Q2b\nFind the airline that has the longest positive dep_delay on average."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q2c",
    "href": "danl-cw/danl-m1-cw-6.html#q2c",
    "title": "Classwork 6",
    "section": "Q2c",
    "text": "Q2c\nFind the airline that has the largest proportion of flights with longer than 30-minute dep_delay."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html",
    "href": "danl-cw/danl-m1-cw-5.html",
    "title": "Classwork 5",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#load-libraries",
    "href": "danl-cw/danl-m1-cw-5.html#load-libraries",
    "title": "Classwork 5",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#load-dataframe",
    "href": "danl-cw/danl-m1-cw-5.html#load-dataframe",
    "title": "Classwork 5",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbillboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')\nny_pincp = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_wide.csv')\ncovid = pd.read_csv('https://bcdanl.github.io/data/covid19_cases.csv')"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q1a",
    "href": "danl-cw/danl-m1-cw-5.html#q1a",
    "title": "Classwork 5",
    "section": "Q1a",
    "text": "Q1a\n\nDescribe how the distribution of rating varies across week 1, week 2, and week 3 using the faceted histogram."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q1b",
    "href": "danl-cw/danl-m1-cw-5.html#q1b",
    "title": "Classwork 5",
    "section": "Q1b",
    "text": "Q1b\n\nWhich artist(s) have the most number of tracks in billboard DataFrame?"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q1c",
    "href": "danl-cw/danl-m1-cw-5.html#q1c",
    "title": "Classwork 5",
    "section": "Q1c",
    "text": "Q1c\n\nMake ny_pincp longer."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q1d",
    "href": "danl-cw/danl-m1-cw-5.html#q1d",
    "title": "Classwork 5",
    "section": "Q1d",
    "text": "Q1d\n\nMake a wide-form DataFrame of covid whose variable names are from countriesAndTerritories and values are from cases."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q1e",
    "href": "danl-cw/danl-m1-cw-5.html#q1e",
    "title": "Classwork 5",
    "section": "Q1e",
    "text": "Q1e\n\nUse the wide-form DataFrame of covid to find the top 10 countries for which their cases are highly correlated with USA’s cases using DataFrame.corr()"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#load-dataframe-for-q2a-and-q2b",
    "href": "danl-cw/danl-m1-cw-5.html#load-dataframe-for-q2a-and-q2b",
    "title": "Classwork 5",
    "section": "Load DataFrame for Q2a and Q2b",
    "text": "Load DataFrame for Q2a and Q2b\n\npaidsearch = pd.read_csv('https://bcdanl.github.io/data/paidsearch.csv')\n\n\nVariable description\n\ndma: an identification number of a designated market (DM) area i (e.g., Boston, Los Angeles)\ntreatment_period: 0 if date is before May 22, 2012 and 1 after.\nsearch_stays_on: 1 if the paid-search goes off in dma i, 0 otherwise.\nrevenue: eBay’s sales revenue for dma i and date t"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q2a",
    "href": "danl-cw/danl-m1-cw-5.html#q2a",
    "title": "Classwork 5",
    "section": "Q2a",
    "text": "Q2a\nSummarize the mean vale of revenue for each group of search_stays_on and for each date."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q2b",
    "href": "danl-cw/danl-m1-cw-5.html#q2b",
    "title": "Classwork 5",
    "section": "Q2b",
    "text": "Q2b\nCalculate the log difference between mean revenues in each group of search_stays_on. (This is the log of the average revenue in group of search_stays_on == 1 minus the log of the average revenue in group of search_stays_on == 0.)\n\nFor example, consider the following two observations:\nThe log difference of daily mean revenues between the two group of search_stays_on for date 1-Apr-12 is log(120277.57) - log(93650.68)."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#load-dataframe-for-q2c-q2d-and-q2e",
    "href": "danl-cw/danl-m1-cw-5.html#load-dataframe-for-q2c-q2d-and-q2e",
    "title": "Classwork 5",
    "section": "Load DataFrame for Q2c, Q2d, and Q2e",
    "text": "Load DataFrame for Q2c, Q2d, and Q2e\n\npaid_search = pd.read_csv('https://bcdanl.github.io/data/paid_search.csv')"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q2c",
    "href": "danl-cw/danl-m1-cw-5.html#q2c",
    "title": "Classwork 5",
    "section": "Q2C",
    "text": "Q2C\nSort paid_search by DM and May22_2012 in ascending order."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q2d",
    "href": "danl-cw/danl-m1-cw-5.html#q2d",
    "title": "Classwork 5",
    "section": "Q2d",
    "text": "Q2d\nFor each DM, calculate the difference between log_revenue before and after May22_2012."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q2e",
    "href": "danl-cw/danl-m1-cw-5.html#q2e",
    "title": "Classwork 5",
    "section": "Q2e",
    "text": "Q2e\n\nConsider the DataFrame from Q2d.\nFor each group of no_paid_search, calculate the mean value of the difference between log_revenue before and after May22_2012 .\nWhat is the difference in the mean values?\nAfter the paid-search went off, sales revenue decreased by 0.66%\n\nWas eBay’s paid search worth it?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-1",
    "title": "Lecture 7",
    "section": "Missing Data",
    "text": "Missing Data\nRarely will we be given a data set without any missing values.\nPandas usually displays missing values as NaN. - NaN is the actual representation of “Not a Number” values. - I would prefer calling it “Not Available” (NA), for which some other programming languages as well as Pandas use to represent missing values."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-2",
    "title": "Lecture 7",
    "section": "Missing Data",
    "text": "Missing Data\n\nThe NaN value in Pandas comes from Numpy.\n\nfrom numpy import NaN\nNaN == True\nNaN == 0\nNaN == \"\nNaN == NaN"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-3",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-3",
    "title": "Lecture 7",
    "section": "Missing Data",
    "text": "Missing Data\n\nPandas has functions to test for missing values, isnull().\nPandas also has functions for testing non-missing values, notnull().\n\nimport pandas as pd\npd.isnull(NaN)\npd.notnull(NaN)\npd.notnull(210)\npd.notnull('missing')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from",
    "title": "Lecture 7",
    "section": "Where Can Missing Values Come From?",
    "text": "Where Can Missing Values Come From?\nLoad Data\n\nread_csv()na_valueskeep_default_na\n\n\n\nWhen we load the data, Pandas automatically finds the missing data cell and give us a DataFrame with the NaN value in the appropriate cell.\nIn the read_csv() function, three parameters are related to reading missing values: na_values and keep_default_na.\n\n\n\nThe na_values parameter allows us to specify additional missing or NaN values.\n\nWe can pass in either a Python str (i.e., string) or a list-like object to be automatically coded as missing values when the file is read.\n\npath = 'https://bcdanl.github.io/data/survey_visited.csv'\nsurvey_visited_0 = pd.read_csv(path)\nsurvey_visited_1 = pd.read_csv(path, na_values = [\"MSK-4\"])\n\n\nThe keep_default_na parameter is a bool (i.e., True or False boolean) that allows us to specify whether any additional values need to be considered as missing.\n\nkeep_default_na = False will only use the missing values specified in na_values.\n\nsurvey_visited_2 = pd.read_csv(path, keep_default_na = False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-1",
    "title": "Lecture 7",
    "section": "Where Can Missing Values Come From?",
    "text": "Where Can Missing Values Come From?\nMerged Data\n\nLet’s merge survey_visited_0 with survey.\n\npath = 'https://bcdanl.github.io/data/survey_survey.csv'\nsurvey = pd.read_csv(path)\nsurvey\n\nvs = survey_visited_0.merge(survey, left_on='ident', right_on='taken')\nvs"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-2",
    "title": "Lecture 7",
    "section": "Where Can Missing Values Come From?",
    "text": "Where Can Missing Values Come From?\nReindexing\n\n(1)(2)\n\n\nAnother way to introduce missing values into our data is to reindex our dataframe.\n\nThis is useful when we want to add new indices to your dataframe, but still want to retain its original values.\nA common usage is when the index represents some time interval, and we want to add more dates.\n\ngapminder = pd.read_csv('https://bcdanl.github.io/data/gapminder.tsv', sep='\\t')\nlife_exp = gapminder.groupby(['year'])['lifeExp'].mean()\n\n\n\nWe can reindex the dataframe by using the .reindex() method.\n\n## subset\ny2000 = life_exp[life_exp.index &gt; 2000]\n\n## reindexing\ny2000.reindex(range(2000, 2010))"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data",
    "title": "Lecture 7",
    "section": "Working With Missing Data",
    "text": "Working With Missing Data\nFind and Count Missing Data\n\n.count()np.count_nonzero().value_counts(dropna=False).sum()\n\n\nOne way to look at the number of missing values is to count() them.\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv')\n## count the number of non-missing values\nebola.count()\nQ. Count the number of non-missing values for each variable in ebola.\n\n\nIf we want to count the total number of missing values in our DataFrame, or count the number of missing values for a particular column, we can use the np.count_nonzero() function from numpy in conjunction with the .isnull() method.\nnp.count_nonzero(ebola.isnull())\nnp.count_nonzero(ebola['Cases_Guinea'].isnull())\n\n\nAnother way to get missing data counts is to use the .value_counts() method, giving a frequency table of values in a Series. - If we use the dropna = False, we can also get a missing value count.\ncnts = ebola['Cases_Guinea'].value_counts(dropna=False)\ncnts\n\nThe results are sorted so we can subset the count vector to just look at the missing values.\n\ncnts.loc[pd.isnull(cnts.index)]\n\n\n\nIn Python, True values equate to the integer value 1, and False values equate to the integer value 0.\n\nWe can use this behavior to get the number of missing values by summing up a boolean vector with the .sum() method.\n\n\nebola.Cases_Guinea.isnull().sum()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-1",
    "title": "Lecture 7",
    "section": "Working With Missing Data",
    "text": "Working With Missing Data\nClean Missing Data\n\nCleaning NaN.fillna().fillna(method=‘ffill’).fillna(method=‘bfill’).interpolate().dropna()Calculationskipna\n\n\nThere are many different ways we can deal with missing data. 1. We can replace the missing value with another value, 2. We can fill in the cells with the missing value using existing value, 3. We can drop the observations with missing values from our DataFrame.\n\n\n\nWe can use the .fillna() method to recode the missing values to another value.\n\n## fill the missing values to 0\nebola0 = ebola.fillna(0)\n\n\n\nWe can use built-in methods to fill forward (method = ffill).\n\nWhen we fill data forward, the last known value (from top to bottom) is used for the next missing value.\n\n\nebola_f = ebola.fillna(method='ffill')\n\n\n\nWe can also use built-in methods to fill backward (method = bfill).\n\nWhen we fill data backward, the newest value (from top to bottom) is used to replace the missing data.\n\n\nebola_b = ebola.fillna(method='bfill')\n\n\n\nInterpolation uses existing values to fill in missing values.\n\nBy default, .interpolate() treats the missing values as if they should be equally spaced apart.\n\nebola_linear = ebola.interpolate()\nThe .interpolate() method behaves kind of in a forward fill fashion.\n\n\n\nIf we want to keep the observations with only non-missing values, we can use .dropna()\nebola_dropna = ebola.dropna()\nWe are left with just one row of data!\n\n\nSuppose we wanted to look at the case counts for multiple regions.\n\n\nebola[\"Cases_multiple\"] = (\n  ebola[\"Cases_Guinea\"]\n  + ebola[\"Cases_Liberia\"]\n  + ebola[\"Cases_SierraLeone\"]\n)\n\nebola_subset = ebola.loc[:,\n    [\"Cases_Guinea\", \n     \"Cases_Liberia\", \n     \"Cases_SierraLeone\",\n     \"Cases_multiple\"] ]\n\n\n\n\n.mean() and .sum() can ignore missing values. - These functions will typically have a skipna parameter that will still calculate a value by skipping over the missing values.\nebola.Cases_Guinea.sum(skipna = True) ## default\nebola.Cases_Guinea.sum(skipna = False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-2",
    "title": "Lecture 7",
    "section": "Working With Missing Data",
    "text": "Working With Missing Data\nPandas Built-In NA Missing\nPandas 1.0 introduced a built-in &lt;NA&gt; value (pd.NA).\n\neconomists = pd.DataFrame(\n  {\n    \"Name\": [\"John Forbes Nash\", \"William Nordhaus\"],\n    \"Occupation\": [\"Mathematician\", \"Climate Economist\"],\n    \"Born\": [\"1928-06-13\", \"1941-05-31\"],\n    \"Died\": [\"2015-05-23\", \"\"],\n    \"Age\": [86, 81]\n  }\n)\n\neconomists.loc[1, \"Age\"] = pd.NA"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nPython’s datetime Object\nOne of the bigger reasons for using Pandas is its ability to work with timeseries data.\n\nWe can use datetime to get the current date and time.\n\nfrom datetime import datetime\nnow = datetime.now()\n\nWe can also create our own datetime manually.\n\n\nt1 = datetime.now()\nt2 = datetime(2000,1,1)\n\ndiff = t1 - t2\ntype(diff)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-1",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nConverting to datetime\n\ndata.to_datatime()formatparse_dates\n\n\n\nLet’s load up our Ebola data set and convert the Date column into a proper datetime object.\n\nimport pandas as pd\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv')\nebola.info()\n\nThe Date column is encoded as a generic string object.\n\n\n\n\nWe can use .to_datatime() to create a new column, date_dt, that converts the Date column into a datetime.\n\nebola['date_dt'] = pd.to_datetime(ebola['Date'])\n\n\n\nThe to_datetime() method has a parameter called format that allows us to manually specify the format of the date.\n\n\n\nThe read_csv() function has several parameters about datetime. - We can parse the Date column directly by specifying the column we want in the parse_dates parameter.\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv', parse_dates=[\"Date\"])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-2",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nExtracting Date Components\nNow that we have a datetime object, we can extract various parts of the date, such as year, month, or day.\nLet’s consider the example datetime object.\nd = pd.to_datetime('2021-12-14')\ntype(d)\nd.year\nd.month\nd.day\nd.quarter"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-3",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-3",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nExtracting Date Components in DataFrame\n\nWe can extract various parts of the datetime column in DataFrame by accessing datetime methods using the .dt accessor.\n\nebola['date_dt'] = pd.to_datetime(ebola['Date'])\n\nebola = ebola.assign(\n    year = ebola[\"date_dt\"].dt.year,\n    month = ebola[\"date_dt\"].dt.month,\n    day = ebola[\"date_dt\"].dt.day\n)\n\nebola.info() ## what are the data types of year, month, and day?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-4",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-4",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nDate Ranges\n\n(1)(2)(3)\n\n\n\nIn our Ebola data set, we do not have an observation for every day in the date range.\n\nThis is quite common.\n\n\nebola = pd.read_csv(\n'https://bcdanl.github.io/data/country_timeseries.csv', parse_dates=[\"Date\"]\n)\n\nHere, 2015-01-01 is missing.\n\n\n\n\nIt’s common practice to create a date range to .reindex() a data set.\n\nWe can use the date_range().\n\n\nhead_range = pd.date_range(start='2014-12-31', end='2015-01-05')\nebola_5 = ebola.head()\nebola_5.index = ebola_5['Date']\nebola_5 = ebola_5.reindex(head_range)\n\n\nebola = pd.read_csv(\n  \"https://bcdanl.github.io/data/country_timeseries.csv\",\n  index_col=\"Date\",\n  parse_dates=[\"Date\"],\n)\n\nnew_idx = pd.date_range(ebola.index.min(), ebola.index.max())\nnew_idx = reversed(new_idx) ## to reverse new_idx\nebola = ebola.reindex(new_idx)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-1",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\n\n\nIn a tidy data.frame,\n\nA variable is in a column.\nAn observation is in a row.\nA value are in a cell.\n\n\nTidy data is a framework to structure data sets so they can be easily analyzed and visualized."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-2",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Values, Not Variables\nData can have columns that contain values instead of variables.\n\nLet’s consider data on income and religion in the United States from the Pew Research Center\n\nimport pandas as pd\npew = pd.read_csv('https://bcdanl.github.io/data/pew.csv')\n\nNot every column here is a variable.\n\nThe values that relate to income are spread across multiple columns.\n\n\nFor data analysis, the pew can be reshaped so that we have religion, income, and count variables."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-3",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Values, Not Variables\n## Show only the first few columns\npew.iloc[:,0:5]\nThe form of the data like pew is known as “wide” data. - To turn it into the “long” tidy data format, we will have to melt our DataFrame."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-4",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-4",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nPandas DataFrames have a method called .melt() that will reshape the DataFrame into a tidy format and it takes a few parameters: - id_vars is a container (list, tuple, ndarray) that represents the variables that will remain as is. - value_vars identifies the columns you want to melt down (or unpivot). - By default, it will melt all the columns not specified in the id_vars parameter. - var_name is a string for the new column name when the value_vars is melted down. - By default, it will be called variable. - value_name is a string for the new column name that represents the values for the var_name. - By default, it will be called value."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-5",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-5",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Values, Not Variables\n\n(1)(2)\n\n\n## we do not need to specify a value_vars since we want to pivot\n## all the columns except for the 'religion' column\npew_long = pew.melt(id_vars='religion')\npew_long\n\n## The .melt() method also exists as a pandas function, pd.melt()\n## The below line of code is the equivalent one:\n## melt function\npew_long = pd.melt(pew, id_vars='religion')\n\n\n\nWe can change the defaults so that the melted/unpivoted columns are named.\n\npew_long = pew.melt(\n  id_vars =\"religion\", var_name=\"income\", value_name =\"count\"\n)\n\npew_long"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-6",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-6",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nKeep Multiple Columns Fixed\n\n(1)(2)\n\n\nNot every data set will have one column to hold still while we unpivot the rest of the columns. - Let’s consider the Billboard data set.\nbillboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')\nEach week has its own column. - What if we want to create a faceted plot of the weekly ratings?\n\n\n## use a list to reference more than 1 variable\nbillboard_long = billboard.melt(\n  id_vars = [\"year\", \"artist\", \"track\", \"time\", \"date.entered\"],\n  var_name = \"week\",\n  value_name = \"rating\",\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-7",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-7",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Multiple Variables\n\n(1)(2)(3)(4)(5)(6)(2)(3)(4)(5)\n\n\nSometimes columns in a data set may represent multiple variables. - Let’s look at the Ebola data set.\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv')\nebola.columns\nebola.iloc[ :5, [0, 1, 2, 10] ]\n\n\nThe column names Cases_Guinea and Deaths_Guinea actually contain two variables. - The individual status (cases and deaths, respectively) as well as the country name, Guinea. - The data is also arranged in a wide format that needs to be reshaped (with the .melt() method). - First, let’s fix the problem by melting the data into long format.\nebola_long = ebola.melt(id_vars=['Date', 'Day'])\n\n\n\nIn this case, we can split the column of interest based on the underscore, _.\n\nWe can use the .split() method that takes a string and “splits” it up based on a given delimiter.\nTo get access to the string methods, we need to use the .str. attribute.\n\n\n## split the column based on a delimiter\nvariable_split = ebola_long.variable.str.split('_')\nvariable_split\ntype(variable_split); type(variable_split[0])\n\n\n\nNow that the column has been split into various pieces, the next step is to assign those pieces to a new column.\n\nTo do so, we can use the .get() string method to “get” the index we want for each row.\n\n\nstatus_values = variable_split.str.get(0)\ncountry_values = variable_split.str.get(1)\n\n\n\nNow that we have the vectors we want, we can add them to our DataFrame.\n\nebola_long['status'] = status_values\nebola_long['country'] = country_values\nebola_long\n\n\n\nIn the .split() method, there is a parameter expand that defaults to False.\n\nWhen we set it to True, it will return a DataFrame where each result of the split is in separate columns. ```{.python} ## reset our ebola_long data ebola_long = ebola.melt(id_vars =[‘Date’, ‘Day’]) ## split the column by _ into a dataframe using expand variable_split = ebola_long.variable.str.split(’_’, expand=True)\n\n\nebola_long[[‘status’, ‘country’]] = variable_split\n\n\n\n::: \n\n\n\n## Tidy Data\n### Variables in Both Rows and Columns \n\n::: {.panel-tabset}\n## (1)\n\nWhat happens if a column of data actually holds two variables instead of one variable?\n  -  In this case, we will have to `pivot` the variable into separate columns, i.e., go from \"long\" data to \"wide\" data.\n\n```{.python}\nweather = pd.read_csv('https://bcdanl.github.io/data/weather.csv')\n\n\nThe weather data include minimum (tmin) and maximum (tmax) temperatures recorded for each day (d1, d2, … , d31) of the month (month). - The element column contains variables that may need to be pivoted wider to become new columns, - The day variables may need to be melted into row values.\nLet’s first fix the day values.\nweather_melt = weather.melt(\n  id_vars=[\"id\", \"year\", \"month\", \"element\"],\n  var_name=\"day\",\n  value_name=\"temp\",\n)\n\n\nNext, we need to pivot up the variables stored in the element column.\nweather_tidy = weather_melt.pivot_table(\n    index = ['id', 'year', 'month', 'day'],\n    columns = 'element',\n    values = 'temp'\n)\n\n\nWe can also flatten the hierarchical columns.\nweather_tidy_flat = weather_tidy.reset_index()\n\n\nFor day variable, we can replace ‘d’ with ““. - Then, convert day from string to integer. - Then, sort weather_tidy_flat by ['year', 'month', 'day'].\nweather_tidy_flat['day'] = weather_tidy_flat.day.str.replace('d', \"\")\nweather_tidy_flat['day'] = weather_tidy_flat['day'].astype(int)\nweather_tidy_flat = weather_tidy_flat.sort_values(by = ['year', 'month', 'day'])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-1",
    "title": "Lecture 6",
    "section": "Data Types",
    "text": "Data Types\nLet’s consider the built-in tips DataFrame from the seaborn library.\nTo get a list of the data types stored in each column of our DataFrame, we call the dtypes attribute.\nimport seaborn as sns\ntips = sns.load_dataset(\"tips\")\ntips.dtypes\n\nThe category data type represents categorical variables.\n\nIt differs from the generic object data type that stores arbitrary Python objects (usually strings)."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-2",
    "title": "Lecture 6",
    "section": "Data Types",
    "text": "Data Types\nConverting Types\n## convert the category sex column into a string dtype\ntips['sex_str'] = tips['sex'].astype(str)\n\n## convert total_bill into a string\ntips['total_bill'] = tips['total_bill'].astype(str)\n\n## convert it back to a float\ntips['total_bill'] = tips['total_bill'].astype(float)\n\n## convert sex into a string\ntips['sex'] = tips['sex'].astype(str)\n\n## convert it back to a category\ntips['sex'] = tips['sex'].astype(category)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-3",
    "title": "Lecture 6",
    "section": "Data Types",
    "text": "Data Types\nto_numeric() method\n\n(1)(2)(3)(4)\n\n\nWhen converting variables into numeric values (e.g., int, float), we can also use the Pandas to_numeric() function. - It handles non-numeric values better.\n\n## subset the tips data\ntips_sub_miss = tips.head(10).copy()\n## assign some 'missing' values\ntips_sub_miss.loc[[1, 3, 5, 7], 'total_bill'] = 'missing'\n\n\n## this will cause an error\ntips_sub_miss['total_bill'].astype(float)\n\n## this will cause an error\npd.to_numeric(tips_sub_miss['total_bill'])\n\n\nThe to_numeric() function has a parameter called errors that governs what happens when the function encounters a value that it is unable to convert to a numeric value. - 'raise': Default. - 'coerce': Invalid parsing will be set as NaN - 'ignore': Invalid parsing will return the input as is.\n\n\ntips_sub_miss[\"total_bill\"] = pd.to_numeric(\n    tips_sub_miss[\"total_bill\"], errors=\"ignore\"\n)\n\ntips_sub_miss[\"total_bill\"]=pd.to_numeric(\n    tips_sub_miss[\"total_bill\"], errors=\"coerce\"\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-assembly-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-assembly-1",
    "title": "Lecture 6",
    "section": "Data Assembly",
    "text": "Data Assembly\n\nSometimes, we better combine various DataFrames together to analyze a set of data.\n\nThe data may have been split up into separate DataFrames to reduce the amount of redundant information.\ne.g., DataFrame for county-level data and DataFrame for geographic information, such as longitude and latitude"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\n\nConcatenation can be thought of as appending a row or column to our data. - This approach is possible if our data was split into parts or if we performed a calculation that we want to append to our existing data set. - Let’s consider the following example DataFrames:\ndf1 = pd.read_csv('https://bcdanl.github.io/data/concat_1.csv')\ndf2 = pd.read_csv('https://bcdanl.github.io/data/concat_2.csv')\ndf3 = pd.read_csv('https://bcdanl.github.io/data/concat_3.csv')\n\nWe will be working with .index and .columns in this Section.\n\ndf1.index\ndf1.columns\ndf1.values"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-1",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\nConcatenating the DataFrames on top of each other uses the concat() function. - All of the DataFrames to be concatenated are passed in a list.\nrow_concat = pd.concat([df1, df2, df3])\nrow_concat\n\nThe row names (i.e., the row indices) are simply a stacked version of the original row indices.\n\nrow_concat.iloc[3, :]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-2",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\n\nLet’s consider a new Series and concatenate it with df1:\n\n## create a new row of data\nnew_row_series = pd.Series(['n1', 'n2', 'n3', 'n4'])\nnew_row_series\n\n\n## attempt to add the new row to a dataframe\ndf = pd.concat([df1, new_row_series])\ndf\n\nNot only did our code not append the values as a row, but it also created a new column completely misaligned with everything else.\nWhy?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-3",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\nTo fix the problem, we need turn our series into a DataFrame. - This data frame contains one row of data, and the column names are the ones the data will bind to.\nnew_row_df = pd.DataFrame(\n  ## note the double brackets to create a \"row\" of data\n  data =[[\"n1\", \"n2\", \"n3\", \"n4\"]],\n  columns =[\"A\", \"B\", \"C\", \"D\"],\n)\n\ndf = pd.concat([df1, new_row_df])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-4",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-4",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nIgnore the Index\n\nWe can use the ignore_index parameter to reset the row index after the concatenation if we simply want to concatenate or append data together.\n\nrow_concat_i = pd.concat([df1, df2, df3], ignore_index=True)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-5",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-5",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Columns\nConcatenating columns is very similar to concatenating rows. - The main difference is the axis parameter in the concat function. - The default value of axis is 0 (or axis = \"index\"), so it will concatenate data in a row-wise fashion. - If we pass axis = 1 (or axis = \"columns\") to the function, it will concatenate data in a column-wise manner.\ncol_concat = pd.concat([df1, df2, df3], axis = \"columns\")"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-6",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-6",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Columns\n\nAdding a single column to a dataframe can be done directly without using any specific Pandas function.\n\ncol_concat['new_col_list'] = ['n1', 'n2', 'n3', 'n4']\n\nWe can reset the column indices so we do not have duplicated column names.\n\npd.concat([df1, df2, df3], axis=\"columns\", ignore_index=True)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-7",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-7",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\nWhat would happen when the row and column indices are not aligned?\n\nLet’s modify our DataFrames for the next few examples.\n\n## rename the columns of our dataframes\ndf1.columns = ['A', 'B', 'C', 'D']\ndf2.columns = ['E', 'F', 'G', 'H']\ndf3.columns = ['A', 'C', 'F', 'H']\n\nIf we try to concatenate these DataFrames as we did, the DataFrames now do much more than simply stack one on top of the other.\n\nrow_concat = pd.concat([df1, df2, df3])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-8",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-8",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\n\nWe can set join = 'inner' to keep only the columns that are shared among the data sets.\n\npd.concat([df1, df2, df3], join ='inner')\n\nIf we use the DataFrames that have columns in common, only the columns that all of them share will be returned.\n\npd.concat([df1, df3], join ='inner',  ignore_index =False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-9",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-9",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\n\nLet’s modify our DataFrames further.\n\n## re-indexing the rows of our dataframes\ndf1.index = [0, 1, 2, 3]\ndf2.index = [4, 5, 6, 7]\ndf3.index = [0, 2, 5, 7]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-10",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-10",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\n\nWhen we concatenate along axis=\"columns\" (axis=1), the new DataFrames will be added in a column-wise fashion and matched against their respective row indices.\n\ncol_concat = pd.concat([df1, df2, df3], axis=\"columns\")\n\nJust as we did when we concatenated in a row-wise manner, we can choose to keep the results only when there are matching indices by using join=\"inner\".\n\npd.concat([df1, df3], axis =\"columns\", join='inner')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data",
    "title": "Lecture 6",
    "section": "Relational data",
    "text": "Relational data\nWhy is one data set sometimes scattered across multiple files? - The size of the files can be huge. - The data collection process can be scattered across time and space.\nSometimes we may have two or more DataFrames (tables) that we want to combine based on common data values. - This task is known in the database world as performing a “join.” - We can do this with the .merge() method in Pandas."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-1",
    "title": "Lecture 6",
    "section": "Relational data",
    "text": "Relational data\n\nThe variables that are used to connect each pair of tables are called keys."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-2",
    "title": "Lecture 6",
    "section": "Relational data",
    "text": "Relational data\nMerges\n\n\n\nx = pd.DataFrame({\n    'key': [1, 2, 3],\n    'val_x': ['x1', 'x2', 'x3']\n})\n\ny = pd.DataFrame({\n    'key': [1, 2, 4],\n    'val_y': ['y1', 'y2', 'y3']\n})\n\n\n\nThe colored column represents the “key” variable.\nThe grey column represents the “value” column."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-1",
    "title": "Lecture 6",
    "section": "Merges",
    "text": "Merges\n\ninnerleftrightouter full\n\n\n\nAn inner join matches pairs of observations whenever their keys are equal:\n\n\n\n\n\n\n\n\n\n\n## the default value for 'how' is 'inner'\n## so it doesn't actually need to be specified\nmerge_inner = pd.merge(x, y, on='key', how='inner')\nmerge_inner_x = x.merge(y, on='key', how='inner')\nmerge_inner_x_how = x.merge(y, on='key')\n\n\n\nA left join keeps all observations in x.\n\n\n\n\n\n\n\n\n\n\nmerge_left = pd.merge(x, y, on='key', how='left')\nmerge_left_x = x.merge(y, on='key', how='left')\n\nThe most commonly used join is the left join.\n\n\n\n\nA right join keeps all observations in y.\n\n\n\n\n\n\n\n\n\n\nmerge_right = pd.merge(x, y, on='key', how='right')\nmerge_right_x = x.merge(y, on='key', how='right')\n\n\n\nA full join keeps all observations in x and y.\n\n\n\n\n\n\n\n\n\n\nmerge_outer = pd.merge(x, y, on='key', how='outer')\nmerge_outer_x = x.merge(y, on='key', how='outer')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-2",
    "title": "Lecture 6",
    "section": "Merges",
    "text": "Merges\nDuplicate keys\n\none-to-manymany-to-many\n\n\n\nOne data frame has duplicate keys (a one-to-many relationship).\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = pd.DataFrame({\n    'key': [1, 2, 3],\n    'val_x': ['x1', 'x2', 'x3'] })\n\n\n\ny = pd.DataFrame({\n    'key': [1, 2, 4],\n    'val_y': ['y1', 'y2', 'y3'] })\none_to_many = x.merge(y, on='key', how='left')\n\n\n\n\n\n\nBoth data frames have duplicate keys (many-to-many relationship).\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = pd.DataFrame({\n  'key': [1, 2, 2, 3],\n  'val_x': ['x1', 'x2', 'x3', 'x4'] })\n\n\n\ny = pd.DataFrame({\n  'key': [1, 2, 2, 3],\n  'val_y': ['y1', 'y2', 'y3', 'y4'] })\nmany_to_many = x.merge(y, on='key', how='left')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-3",
    "title": "Lecture 6",
    "section": "Merges",
    "text": "Merges\nDefining the key columns\n\nIf the left and right columns do not have the same name for the key columns, we can use the left_on and right_on parameters instead.\n\n\n\n\nx = pd.DataFrame({\n  'key_x': [1, 2, 3],\n  'val_x': ['x1', 'x2', 'x3']\n})\n\n\n\ny = pd.DataFrame({\n  'key_y': [1, 2],\n  'val_y': ['y1', 'y2'] })\n\nkeys_xy = x.merge(y, \n                  left_on='key_x', \n                  right_on = 'key_y', \n                  how='left')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-libraries",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-libraries",
    "title": "Lecture 2",
    "section": "pandas Libraries",
    "text": "pandas Libraries\n\nPython is a general-purpose programming language and is not specialized for numerical or statistical computation.\nA library is a collection of packages that includes a bunch of related Python codes.\n\n\n\npandas is a Python library that provides high-performance data structures and data analysis tools:\n\nData manipulation and analysis\nDataFrame and Series objects\nExport and import data from files and web\nHandling of missing data"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#importing-pandas-libraries",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#importing-pandas-libraries",
    "title": "Lecture 2",
    "section": "Importing pandas Libraries",
    "text": "Importing pandas Libraries\n\nWe refer to code of libraries by using the Python import statement.\n\nThis makes the code and variables in the imported module available to our programming codes.\nWe can use the as keyword when importing the libraries using their canonical names.\n\n\nimport pandas as pd"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas",
    "title": "Lecture 2",
    "section": "Pandas",
    "text": "Pandas\nLoad Data Sets\n\nWhen given a data set, we first load it and begin looking at its structure and contents.\nThe simplest way of looking at a data set is to look at and subset specific rows and columns.\nWe can see what type of information is stored in each column, and can start looking for patterns by aggregating descriptive statistics."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-1",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-1",
    "title": "Lecture 2",
    "section": "Pandas",
    "text": "Pandas\nLoad Data Sets\n\nWith the library loaded we can use the read_csv() function to load a CSV data file.\nIn order to access the read_csv() function from pandas, we use something called “dot” notation.\n\nWe write pd.read_csv() to say: within the pandas library we just loaded, look inside for the read_csv() function.\n\n\n# *.tsv is a file of tab-separated values.\n# we can use the sep parameter and indicate a tab with \\t\ndf  = pd.read_csv('https://bcdanl.github.io/data/gapminder.tsv', sep='\\t')\ndf"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas---load-data-sets",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas---load-data-sets",
    "title": "Lecture 2",
    "section": "Pandas - Load Data Sets",
    "text": "Pandas - Load Data Sets\n\ntype().shape.columns.dtypes\n\n\n\nWe can check to see if we are working with a Pandas Dataframe by using the built-in type() function (i.e., it comes directly from Python, not a separate library such as Pandas).\n\ntype(df)\n\nThe type() function is handy when we begin working with many different types of Python objects and need to know what object we are currently working on.\n\n\n\n\nEvery DataFrame object has a .shape attribute that will give us the number of rows and columns of the DataFrame.\n\ndf.shape\n#  It’s written as df.shape and not df.shape()\n\nSince .shape is an attribute of the DataFrame object, and not a function or method of the DataFrame object, it does not have round parentheses after the period.\n\n\n\n\nTo get a gist of what information the data set contains, we look at the column names.\nThe column names are given using the .column attribute of the DataFrame object.\n\ndf.columns\n\nQ. What is the type of the column names?\n\n\n\n\nEach column (i.e., Series) has to be the same type, whereas each row can contain mixed types.\nWe can use the .dtypes attribute or the .info() method to see the type of each column in DataFrame.\n\ndf.dtypes\ndf.info()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSelect and Subset Columns by Name\n\nhead()[ ][ [] ]\n\n\n\nWe can use the .head() method of a DataFrame to look at the first 5 rows of our data.\n\nThis is useful to see if our data loaded properly, and to get a better sense of the columns and contents.\n\n\ndf.head()\n\n\n\nIf we want only a specific column from our data, we can access the data using square brackets, [ ].\n\n# just get the country column and save it to its own variable\ncountry_df = df['country']\n# show the first 5 observations\ncountry_df.head()\n\n\n\nIn order to specify multiple columns by the column name, we need to pass in a Python list between the square brackets.\n\n# Looking at country, continent, and year\nsubset = df[['country', 'continent', 'year']]\nsubset"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-1",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-1",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSingle Value Returns DataFrame or Series\n\nSeriesDataFrame\n\n\n\nWhen we first select a single column, a Series object comes out.\n\ncountry_df = df['country']\ntype(country_df)\ncountry_df\n\n\n\nCompare df['country'] with df[ ['country'] ]\n\ncountry_df_list = df[ ['country'] ]\ntype(country_df_list)\ncountry_df_list\n\nIf we use a list to subset, we will always get a DataFrame object back."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-2",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-2",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nUsing Dot Notation to Pull a Column of Values\n\nThere is a shorthand notation where we can pull the column by treating it as a DataFrame attribute, or using dot (.).\n\ndf['country']\ndf.country\n\nWe do have to be mindful of what our columns are named if we want to use the dot notation.\n\nIf there is a column named shape, the df.shape will return the number of rows and columns\nIf our column name has spaces or special characters, we will not be able to use the dot notation to select that column of values."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-3",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-3",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubset Rows\n\nRows can be subset in multiple ways, by row name or row index."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-4",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-4",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubset Rows by index Label: .loc[]\n\nIndex.loc[]last row.tail()multiple rows\n\n\n\nLet’s take a look at our gapminder data:\n\ndf\n\nWe can see on the left side of the printed DataFrame, what appear to be row numbers.\n\nThis column-less row of values is the “index” label of the DataFrame.\n\nBy default, Pandas fills in the index labels with the row numbers starting from 0.\n\n\n\n\nWe can use the .loc[] attribute on the DataFrame to subset rows based on the index label.\n\n:::: {.columns} ::: {.column width=“50%”}\n# get the first row\ndf.loc[0]\n\n# get the 100th row\ndf.loc[99]\n\n# get the last row\ndf.loc[ -1]\n\nDoes df.loc[ -1] work?\n\n\n\n\n\nHere, passing -1 to the .loc[] causes an error.\n\nIt is actually looking for the row index label (i.e., row number -1), which does not exist in our example DataFrame.\n\n\n# use the first value given from shape to get the number of rows\nnumber_of_rows = df.shape[0]\n# subtract 1 from the value since we want the last index value\nlast_row_index = number_of_rows - 1\n\n# finally do the subset using the index of the last row\ndf.loc[last_row_index]\n\n\n\nWe can use the .tail() method to return the last n = 1 row, instead of the default 5.\n\ndf.tail(n = 1)\n\nUsing .tail() and .loc[] prints out the results differently.\n\ntype( df.tail(n = 1) )\ntype( df.loc[last_row_index] )\n\n\n\nWe can filter multiple rows.\n\ndf.loc[ [0, 99, 999] ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-5",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-5",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubset Rows by Row Number: .iloc[]\n\n.iloc[] does the same thing as .loc[], but is used to subset by the row index number.\nIn our current example, gapminder DataFrame, .iloc[] and .loc[] behave exactly the same way because the index labels are the same as the row numbers.\n\n\n\n# get the 2nd row\ndf.iloc[1]\n\n# get the 100th row\ndf.iloc[99]\n\n# get the last row\ndf.iloc[ -1]. # does it work?\n\n# get the first, 100th, and 1000th row\ndf.iloc[ [0, 99, 999] ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-6",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-6",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nMix It Up\n\nWe can use .loc[] and .iloc[] to obtain rows, columns, or both.\nThe general syntax for .loc[] and .iloc[] uses square brackets with a comma.\ndf.loc[ [rows], [columns] ]\ndf.iloc[ [rows], [columns] ]\nTo just subset columns, we can use the slicing methods.\n\nIf we are subsetting columns, we are getting all the rows for the specified column.\nSo, we need a method to capture all the rows.\nIf we have just a colon (:) when using slicing methods, it “slices” all the values in that axis."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-7",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-7",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSelecting Columns\n\nWe can write df.loc[:, [columns]] or df.iloc[:, [columns]] to subset the column(s).\n\n# subset columns with loc\nsubset_1 = df.loc[:, ['year', 'pop']]\nsubset_1\n\n# subset columns with iloc\n# iloc will allow us to use integers -1 to select the last column\nsubset_2 = df.iloc[:, [2, 4, -1]]\nsubset_2\n\n# do the followings work?\nsubset = df.loc[:, [2, 4, -1]] \nsubset = df.iloc[:, ['year', 'pop']]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-8",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-8",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with range()\n\nWe can use the built-in range() function to create a range of integer values.\n\nBy default, range(start, stop) creates all integer values between the beginning and the end (inclusive left, exclusive right).\nFYI, we can also pass in a 3rd parameter into range(start, stop, step), step, that allows us to change how to increment between the start and stop values (defaults to step=1).\n\n\n# create a range of integers from 0 to 4 inclusive\nsmall_range = list( range(5) )   # equivalent to range(0, 5)\nsmall_range\n# subset the dataframe with the range\nsubset = df.iloc[:, small_range]\nsubset"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-9",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-9",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with range()\n\nLet’s consider one more example with range():\n\n#  create a range from 3 to 5 inclusive\nsmall_range = list( range(3, 6) )\n\nsubset = df.iloc[:, small_range]\nsubset\n\nQ. What happens when you specify a range() that’s beyond the number of columns you have?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-10",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-10",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with Slicing :\n\nUnlike the range() function, the slicing method separates the values with the colon within the square bracket, [start : end : step ].\nLet’s see the columns of the DataFrame:\n\ndf.columns\n\nSee how range() and : are used to slice the first 3 columns.\n\nsmall_range = list( range(3) )\nsubset_1 = df.iloc[ : , small_range ]\n\nsubset_2 = df.iloc[ : , :3 ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-11",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-11",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with Slicing :\nQ. Let’s slice the columns 3 to 5 inclusive using (1) the range() function and (2) the slicing method.\nQ. What happens if we use the slicing method with 2 colons, but leave a value out? For example:\n df.iloc[: , 0 : 6 :   ]\n df.iloc[: , 0 :   : 2 ]\n df.iloc[: ,   : 6 : 2 ]\n df.iloc[: ,   :   : 2 ]\n df.iloc[: ,   :   :   ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes",
    "title": "Lecture 4",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nModifying Columns with .assign()\n\n(0)(1)(2)\n\n\n\nLet’s create a new set of columns that contain the datetime representations of the object (string) dates.\n\n## format the 'Born' column as a datetime\nborn_datetime = pd.to_datetime( scientists['Born'] )\ndied_datetime = pd.to_datetime( scientists['Died'] )\n\nscientists['born_dt'], scientists['died_dt'] = (\n  born_datetime,\n  died_datetime\n)\n\nscientists['age_days'] =  scientists['died_dt'] - scientists['born_dt']\n\n\n\nLet’s create the age_year_assign column using .assign().\n\nscientists = scientists.assign(\n  ## new columns on the left of the equal sign\n  ## how to calculate values on the right of the equal sign\n  ## separate new columns with a comma\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = scientists['age_days'].astype('timedelta64[Y]')\n)\n\n\n\nIn the previous panel, we had to use age_days to create age_year_assign.\n\nTo be able to use age_days_assign when calculating age_year_assign in the previous panel, we need to know about lambda functions.\n\n\nscientists = scientists.drop( ['age_days'], axis = 1)  ## to drop age_days\nscientists = scientists.assign(\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = lambda some_df: \n    some_df['age_days_assign'].astype('timedelta64[Y]') \n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-1",
    "title": "Lecture 4",
    "section": "Pandas Data Structures Basics",
    "text": "Pandas Data Structures Basics\nLambda functions\n\n(1)(2)(3)(4)(5)\n\n\n\nPython has support for so-called anonymous or lambda functions.\n\nLambda functions are a way of writing functions consisting of a single statement, the result of which is the return value.\nA syntax for a lambda function is lambda ARGUMENTS : EXPRESSION\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\nshort_function(2)\nequiv_anon(2)\n\n\n\nA lambda function can have multiple arguments:\n\nfn_two = lambda a, b : a * b\nfn_two(1, 2)\n\nfn_three = lambda a, b, c : a + b + c\nfn_two(1, 2, 3)\n\n\n\nThe power of lambda is better shown when we use them as an anonymous function inside another function.\n\nSay we have a function definition that takes one argument, and that argument will be multiplied with an unknown number:\n\n\ndef myfunc(n):\n  return lambda a : a * n\n\n\n\n\n\nUse that function definition to make a function that always doubles the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_doubler = myfunc(2)\nmy_doubler(10)\n\n\nUse the same function definition to make a function that always triples the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_tripler = myfunc(3)\nmy_tripler(10)\n\n\n\n\n\nHere is the example of applying a lambda function to single variable in DataFrame using .assign()\n\nvalues= [['Rohan',182],['Elvish',100],['Deepak',198],\n         ['Soni',160],['Radhika',140],['Vansh',180]]\ndf = pd.DataFrame(values, columns = ['name','tot_marks'])\n \n## Applying lambda function to find percentage of 'tot_marks' column\n## using df.assign()\ndf = df.assign( percentage = \n  lambda some_name: (some_name['tot_marks'] /200 * 100) )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe",
    "title": "Lecture 4",
    "section": "The DataFrame",
    "text": "The DataFrame\nSubsetting Multiple Rows and Columns"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-1",
    "title": "Lecture 4",
    "section": "The DataFrame",
    "text": "The DataFrame\nBoolean Subsetting\n\nSeriesDataFrameQ.\n\n\n\nWe can not only subset values using labels and indices, but also supply a vector of boolean values.\nBoolean subsetting of numeric Series works as follows:\n\nSeries[ Series &gt; VALUE  ]\nSeries[ Series == VALUE  ]\nSeries[ Series &lt; VALUE  ]\n\n\n\n\n\nBoolean subsetting of DataFrames works like boolean subsetting a Series.\n\nDataFrame[ DataFrame['VARIABLE_NAME'] &gt; VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] == VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] &lt; VALUE  ]\n\n\nimport pandas as pd\nscientists = read_csv('data/scientists.csv')  \n\n## What if we want to subset our ages by identifying those above the mean?\nscientists.loc[ scientists['Age'] &gt; scientists['Age'].mean() ]\n\n\n\nConsider the two Series, area and pop:\n\narea = pd.Series({'California': 423967, 'Texas': 695662,\n                  'New York': 141297, 'Florida': 170312,\n                  'Illinois': 149995})\npop = pd.Series({'California': 38332521, 'Texas': 26448193,\n                 'New York': 19651127, 'Florida': 19552860,\n                 'Illinois': 12882135})\n\nUse boolean subsetting to find states whose population density is greater than 100 or less than 50."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-2",
    "title": "Lecture 4",
    "section": "The DataFrame",
    "text": "The DataFrame\nSubsetting Rows with .query()\n\nWe can also subset rows based on a condition using DataFrame.query():\n\n\nData.query() (1)(2)(3)(4)\n\n\ndf = pd.DataFrame(\n    ## array with numbers from 0 to 35, 6 rows and 6 columns\n    data = np.reshape( range(36), (6, 6) ),\n    index = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"],\n    columns = [\"col\" + str(i) for i in range(6)],\n    dtype = float,\n)\ndf[\"col6\"] = [\"apple\", \"orange\", \"pineapple\", \"mango\", \"kiwi\", \"lemon\"]\ndf\n\n\n\nSelect rows if a value of col6 is “kiwi” or “pineapple”:\n\ndf.query(\"col6 == 'kiwi' or col6 == 'pineapple'\")\n\nSelect rows if a value of col0 is greater than 6:\n\ndf.query(\"col0 &gt; 6\")\n\n\n\nSelect flights that departed on January 1:\n\n## download NY_flights.csv from the Files section in Canvas\nflights = pd.read_csv(\"data/NY_flights.csv\")\nflights.head()\n\nflights.query(\"month == 1 and day == 1\")\n\n\n\nSelect penguins whose bill_length_mm is smaller than 1.8 * bill_depth_mm.\n\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\npenguins.query('bill_length_mm &lt; bill_depth_mm*1.8')\n\n\n\nObjects not in DataFrame can be referenced with an @ character like@a + b.\n\noutside_var = 21\npenguins.query('bill_depth_mm &gt; @outside_var')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe",
    "title": "Lecture 4",
    "section": "The Series and the DataFrame",
    "text": "The Series and the DataFrame\nSorting and Ranking\n\nLet’s consider .sort_index() and .sort_values() methods:\n\n\nSeries\n\n\n\nSeries.sort_index(ascending=False) sorts Series by index in descending order.\nSeries.sort_values(ascending=False) sorts Series by value in descending order.\n\nrev_ages = ages.sort_index(ascending =False) \nsorted_ages = ages.sort_values(ascending =False) \n] ## DataFrame - DataFrame.sort_index(ascending=False) sorts DataFrame by index in descending order. - DataFrame.sort_values(by = \"VAR\", ascending=False) sorts DataFrame by value of VAR in descending order.\nrev_ages = scientists.sort_index(ascending =False) \nsorted_df = scientists.sort_values(by = 'Age', \n                                   ascending =False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe-1",
    "title": "Lecture 4",
    "section": "The Series and the DataFrame",
    "text": "The Series and the DataFrame\nSorting and Ranking\n\nIn Pandas, there are a variety of ranking functions with .rank().\n\n\nexamplemethod (1)method (2)na_optionpct\n\n\n\nConsider the following DataFrame.\n\nimport numpy as np\ndf = pd.DataFrame(data={'Animal': ['fox', 'Kangaroo', 'deer',\n                                   'spider', 'snake'],\n                        'Number_legs': [4, 2, 4, 8, np.nan]})\ndf\n\n\n\n.rank(method = \"min\", ascending=False) does give the largest values the smallest ranks.\n.rank(method = \"dense\", ascending=False) does give the largest values the smallest ranks without any gaps between ranks when breaking ties.\n.rank(method = \"average\", ascending=False) calculates the average rank for each unique value.\n\n\n\ndf['default_rank'] = df['Number_legs'].rank(ascending = False)  ## method = 'average'\ndf['min_rank'] = df['Number_legs'].rank(method='min', ascending = False)\ndf['dense_rank'] = df['Number_legs'].rank(method='dense', ascending = False)\ndf\n\n\n\nna_option = keep assigns NaN rank to NaN values (default).\nna_option = top assign smallest rank to NaN values if ascending\nna_option = bottom assign highest rank to NaN values if ascending\n\ndf['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')\ndf['NA_top'] = df['Number_legs'].rank(na_option='top')\ndf\n\n\n\npct = True displays the returned rankings in percentile form.\n\n\ndf['pct_rank'] = df['Number_legs'].rank(pct=True)\ndf"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-1",
    "title": "Lecture 4",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDropping Values\n\nColumnsRows\n\n\n\nTo drop a column, we can select columns to drop with the .drop() method with axis = 1 or axis = \"columns\" on our dataframe.\n\n## all the current columns in our data\nscientists.columns\n\n## drop the shuffled age column\n## we provide the axis=1 argument to drop column-wise\nscientists_dropped = scientists.drop( ['Age'], axis =\"columns\")\nscientists_dropped.columns\n\n\n\nTo drop rows, we can select rows by index to drop with the .drop() method with axis = 0, which is default.\n\n## all the current columns in our data\nscientists.columns\n\n## drop rows by their indices\nscientists_rows_dropped = scientists.drop( [2, 4, 6] )\nscientists_rows_dropped"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-2",
    "title": "Lecture 4",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nModifying Columns with .assign()\n\n(0)(1)(2)\n\n\n\nLet’s create a new set of columns that contain the datetime representations of the object (string) dates.\n\n## format the 'Born' column as a datetime\nborn_datetime = pd.to_datetime( scientists['Born'] )\ndied_datetime = pd.to_datetime( scientists['Died'] )\n\nscientists['born_dt'], scientists['died_dt'] = (\n  born_datetime,\n  died_datetime\n)\n\nscientists['age_days'] =  scientists['died_dt'] - scientists['born_dt']\n\n\n\nLet’s create the age_year_assign column using .assign().\n\nscientists = scientists.assign(\n  ## new columns on the left of the equal sign\n  ## how to calculate values on the right of the equal sign\n  ## separate new columns with a comma\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = scientists['age_days'].astype('timedelta64[Y]')\n)\n\n\n\nIn the previous panel, we had to use age_days to create age_year_assign.\n\nTo be able to use age_days_assign when calculating age_year_assign in the previous panel, we need to know about lambda functions.\n\n\nscientists = scientists.drop( ['age_days'], axis = 1)  ## to drop age_days\nscientists = scientists.assign(\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = lambda some_df: \n    some_df['age_days_assign'].astype('timedelta64[Y]') \n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-2",
    "title": "Lecture 4",
    "section": "Pandas Data Structures Basics",
    "text": "Pandas Data Structures Basics\nLambda functions\n\n(1)(2)(3)(4)(5)\n\n\n\nPython has support for so-called anonymous or lambda functions.\n\nLambda functions are a way of writing functions consisting of a single statement, the result of which is the return value.\nA syntax for a lambda function is lambda ARGUMENTS : EXPRESSION\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\nshort_function(2)\nequiv_anon(2)\n\n\n\nA lambda function can have multiple arguments:\n\nfn_two = lambda a, b : a * b\nfn_two(1, 2)\n\nfn_three = lambda a, b, c : a + b + c\nfn_two(1, 2, 3)\n\n\n\nThe power of lambda is better shown when we use them as an anonymous function inside another function.\n\nSay we have a function definition that takes one argument, and that argument will be multiplied with an unknown number:\n\n\ndef myfunc(n):\n  return lambda a : a * n\n\n\n\n\n\nUse that function definition to make a function that always doubles the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_doubler = myfunc(2)\nmy_doubler(10)\n\n\nUse the same function definition to make a function that always triples the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_tripler = myfunc(3)\nmy_tripler(10)\n\n\n\n\n\nHere is the example of applying a lambda function to single variable in DataFrame using .assign()\n\nvalues= [['Rohan',182],['Elvish',100],['Deepak',198],\n         ['Soni',160],['Radhika',140],['Vansh',180]]\ndf = pd.DataFrame(values, columns = ['name','tot_marks'])\n \n## Applying lambda function to find percentage of 'tot_marks' column\n## using df.assign()\ndf = df.assign( percentage = \n  lambda some_name: (some_name['tot_marks'] /200 * 100) )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-1",
    "title": "Lecture 4",
    "section": "Exporting and Importing Data",
    "text": "Exporting and Importing Data\nComma-Separated Values (CSV)\n\nComma-separated values (CSV) are the most flexible data storage type.\n\nFor each row, the column information is separated with a comma.\n\nTo export Series or DataFrame as a csv file, we use the to_csv() method.\n\n## index =False  does not write the row names in the CSV output\nscientists.to_csv('output/scientists_df_no_index.csv', \n                   index =False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-2",
    "title": "Lecture 4",
    "section": "Exporting and Importing Data",
    "text": "Exporting and Importing Data\nExcel\n\nThe more of your work you can do in Python and/or R, the easier it will be to scale up to larger projects, catch and fix mistakes, and collaborate.\nHowever, Excel’s popularity and market share are unrivaled.\nTo export Series or DataFrame as an .xlsx file, we use the to_excel() method.\n\n## saving a DataFrame into Excel format\nscientists.to_excel(\n  \"output/scientists_df.xlsx\",\n  sheet_name = \"scientists\",\n  index = False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nHere we discuss how to use summary statistics and visualization to explore your data in a systematic way, a task that statisticians call exploratory data analysis (EDA).\nEDA is an iterative cycle. We:\n\nGenerate questions about your data.\nSearch for answers by visualizing, transforming, and modelling our data.\nUse what we learn to refine our questions and/or generate new questions.\n\nHere, we focus on the visualization part."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-1",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nTidy data.frame\n\n\nIn a tidy DataFrame,\n\nA variable is in a column.\nAn observation is in a row.\nA value are in a cell."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-2",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nTidy data.frame\n\nA variable is a quantity, quality, or property that we can measure/count.\nAn observation is a set of measurements made under similar conditions (e.g, similar unit of entity, time, and/or geography).\n\nWe usually make all of the measurements in an observation at the same time and on the same object.\nAn observation will contain several values, each associated with a different variable.\nWe sometimes refer to an observation as a data point.\n\nThe value of a variable may change from measurement to measurement."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-3",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-3",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nCategorical/Discrete vs. Continuous Variables\n\nA discrete/categorical variable is a variable whose value is obtained by counting and is whole numbers.\n\nNumber of red marbles in a jar\nNumber of heads when flipping three coins\nStudents’ letter grade\nUS state/county\n\nA continuous variable is a variable whose value is obtained by measuring and can have a decimal or fractional value.\n\nHeight/weight of students\nTime it takes to get to school\nFuel efficiency of a vehicle (e.g., miles per gallon)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-4",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-4",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nMaking Discoveries from a Data Set\n\nDistributionRelationshipStepsTypesSummary Stat.\n\n\n\nWhat type of variation occurs within a variable?\nVariation is the tendency of the values of a variable to change from measurement to measurement.\n\nWe can see variation easily in real life; if we measure any continuous variable twice, we will be likely to get two different values.\nWhich values are the most common? Why?\nWhich values are rare? Why? Does that match your expectations?\nCan we see any unusual patterns? What might explain them?\n\n\n\n\n\nCo-variation is the tendency for the values of two or more variables to vary together in a related way.\nWhat type of co-variation occurs between variables?\n\n\n\n\nFigure out whether variables of interests are categorical or continuous.\nThink which geometric objects, aesthetic mappings, and faceting are appropriate to visualize distributions and relationships.\nIf needed, transform a given DataFrame (e.g., subset of observations, new variables, summarized data) and try new visualizations.\n\n\n\n\nA distribution of a categorical variable (e.g., bar charts and more)\nA distribution of a continuous variable (e.g., histograms and more)\nA relationship between two categorical variables (e.g., bar charts and more)\nA relationship between two continuous variables (e.g., scatter plots and more)\nA relationship between a categorical variable and a continuous variable (e.g., boxplots and more)\nA time trend of a categorical variable (e.g., bar plots and more)\nA time trend of a continuous variable (e.g., line plots and more)\n\n\n\n\nUse skim(DataFrame) or .describe() to know:\n\nMean (Average, Expected Value);\nStandard Deviation (SD)\nMinimum, First Quartile (Q1), Median (Q2), Third Quartile (Q3), and Maximum."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization",
    "title": "Lecture 4",
    "section": "Data Visualization",
    "text": "Data Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraphs and charts let us explore and learn about the structure of the information we have in DataFrame.\nGood data visualizations make it easier to communicate our ideas and findings to other people."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#seaborn",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#seaborn",
    "title": "Lecture 4",
    "section": "seaborn",
    "text": "seaborn\n\nseaborn is a Python data visualization library based on matplotlib.\n\nIt allows us to easily create beautiful but complex graphics using a simple interface.\nIt also provides a general improvement in the default appearance of matplotlib-produced plots, and so I recommend using it by default.\n\n\nimport seaborn as sns\nsns.set_theme(rc={'figure.dpi': 600, \n                  'figure.figsize': (5, 3.75)})   ## better quality"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-1",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nGetting started with seaborn\n\nLet’s get the names of DataFrames provided by the seaborn library:\n\nimport seaborn as sns\nprint( sns.get_dataset_names() )\n\nLet’s use the titanic and tips DataFrames:\n\ntitanic = sns.load_dataset('titanic')\ntitanic.head()\ntips = sns.load_dataset('tips')\ntips.head()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-2",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nBar Chart\n\nA bar chart is used to plot the frequency of the different categories.\n\nIt is useful to visualize how values of a categorical variable are distributed.\nA variable is categorical if it can only take one of a small set of values.\n\nWe use sns.countplot() function to plot a bar chart:\n\n\n\nsns.countplot(data = titanic,\n              x =  'sex')\n\n\nMapping\n\ndata: DataFrame.\nx: Name of a categorical variable (column) in DataFrame"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-3",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-3",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nBar Chart\n\nWe can further break up the bars in the bar chart based on another categorical variable.\n\nThis is useful to visualize the relationship between the two categorical variables.\n\n\n\n\nsns.countplot(data = titanic,\n              x = 'sex'\n              hue = 'survived')\n\n\nMapping\n\nhue: Name of a categorical variable"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-4",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-4",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nHistogram\n\nA histogram is a continuous version of a bar chart.\n\nIt is used to plot the frequency of the different values.\nIt is useful to visualize how values of a continuous variable are distributed.\n\nWe use sns.histplot() function to plot a histogram: :::: {.columns} ::: {.column width=“50%”}\n\nsns.histplot(data = titanic,\n             x =  'age', \n             bins = 5)\n:::\n\n\nMapping\n\nbins: Number of bins\n\n\n\n::::"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-5",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-5",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nHistogram\n\nA boxplot computes a summary of the distribution and then display a specially formatted box.\n\nIt is useful to visualize how values of a continuous variable are distributed across different values of another (categorical) variable.\n\nWe use sns.histplot() function to plot a histogram: :::: {.columns} ::: {.column width=“50%”}\n\nsns.boxplot(data = tips,\n             y='total_bill')\n:::\n\nsns.boxplot(data = tips,\n            x='time', y='total_bill')\n\n::::"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-6",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-6",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nScatter plot\n\nA scatter plot is used to display the relationship between two continuous variables.\n\nWe can see co-variation as a pattern in the scattered points.\n\nWe use sns.scatterplot() function to plot a scatter plot:\n\n\n\nsns.scatterplot(data = tips,\n                x = 'total_bill', \n                y = 'tip')\n\n\nMapping\n\nx: Name of a continuous variable on the horizontal axis\ny: Name of a continuous variable on the vertical axis"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-7",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-7",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nScatter plot\n\nTo the scatter plot, we can add a hue-VARIABLE mapping to display how the relationship between two continuous variables varies by VARIABLE.\nSuppose we are interested in the following question:\n\nQ. Does a smoker and a non-smoker have a difference in tipping behavior?\n\n\nsns.scatterplot(data = tips,\n                x = 'total_bill', \n                y = 'tip',\n                hue = 'smoker')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-8",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-8",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nFitted line\n\nFrom the scatter plot, it is often difficult to clearly see the relationship between two continuous variables.\n\nsns.lmplot() adds a line that fits well into the scattered points.\nOn average, the fitted line describes the relationship between two continuous variables.\n\n\nsns.lmplot(data = tips,\n           x = 'total_bill', \n           y = 'tip')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-9",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-9",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nTransparency with alpha\n\nIn a scatter plot, adding transparency with alpha helps address many data points on the same location.\n\nWe can map alpha to number between 0 and 1.\n\n\n\n\nsns.scatterplot(x = 'total_bill', \n                y = 'tip',\n                hue = 'smoker',\n                alpha = .25)\n\nsns.lmplot(data = tips,\n           x = 'total_bill', \n           y = 'tip',\n           scatter_kws = {'alpha' : 0.2})"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-10",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-10",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nScatter plot\n\nTo the scatter plot, we can add a hue-VARIABLE mapping to display how the relationship between two continuous variables varies by VARIABLE.\nUsing the fitted lines, let’s answer the following question:\n\nQ. Does a smoker and a non-smoker have a difference in tipping behavior?\n\n\nsns.scatterplot(data = tips,\n                x = 'total_bill', \n                y = 'tip',\n                hue = 'smoker')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-11",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-11",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nLine cahrt\n\nA line chart is used to display the trend in a continuous variable or the change in a continuous variable over other variable.\n\nIt draws a line by connecting the scattered points in order of the variable on the x-axis, so that it highlights exactly when changes occur.\n\nWe use sns.lineplot() function to plot a line plot:\n\n\n\npath_csv = 'https://bcdanl.github.io/data/dji.csv'\ndow = pd.read_csv(path_csv, index_col=0, parse_dates=True)\nsns.lineplot(data = dow,\n             x =  'Date', \n             y =  'Close')\n\n\nMapping\n\nx: Name of a continuous variable (often time variable) on the horizontal axis\ny: Name of a continuous variable on the vertical axis"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-12",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-12",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nLine cahrt\n\nFor line charts, we often need to group or connect observations to visualize the number of distinct lines.\n\nhealthexp = ( sns.load_dataset(\"healthexp\")\n             .sort_values([\"Country\", \"Year\"])\n             .query(\"Year &lt;= 2020\") )\nhealthexp.head()\n\nsns.lineplot(data = dow,\n             x =  'Year', \n             y =  'Life_Expectancy',\n             color = 'Country')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-13",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-13",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nFaceting\n\nFaceting allows us to plot subsets (facets) of our data across subplots.\n\n\nStep 1. FacetGrid()Step2. FacetGrid().map()\n\n\n\nFirst, we create a FacetGrid() object with the data we will be using and define how it will be subset with the row and col arguments:\n\ng = sns.FacetGrid(\n      data = titanic,\n      row='class',\n      col='sex')\n\n\n\nSecodn, we use the FacetGrid().map() method to run a plotting function on each of the subsets, passing along any necessary arguments.\n\ng.map(sns.histplot, 'age', kde=True)  ## kde: kernel density (probability density function)"
  },
  {
    "objectID": "why-colab-github.html",
    "href": "why-colab-github.html",
    "title": "Why and How Do We Use Google Coalb and GitHub?",
    "section": "",
    "text": "Google Colab, short for Google Colaboratory, is an external tool that we integrate into our course to provide students with a dynamic and collaborative coding environment. It is a cloud-based platform developed by Google that offers a free, web-based Python programming environment. The purpose of using Google Colab is to enable learners to write, run, and collaborate on Python code without the need for complex local installations. It provides several advantages:\n\nAccessibility: Google Colab is accessible from any device with an internet connection without setting up Python environment for data analysis on a local computer, making it convenient for learners to work on coding assignments from anywhere.\nReal-time Collaboration: Multiple users can work on the same Colab notebook simultaneously, allowing for real-time collaboration on coding projects and assignments.\nPowerful Resources: Google Colab provides access to powerful computing resources, including GPUs and TPUs, which can be useful for computationally intensive tasks like machine learning.\nVersion Control: It supports version control through integration with GitHub, making it easy to manage and track changes in code."
  },
  {
    "objectID": "why-colab-github.html#setting-up-github-access-from-colab",
    "href": "why-colab-github.html#setting-up-github-access-from-colab",
    "title": "Why and How Do We Use Google Coalb and GitHub?",
    "section": "Setting up GitHub access from Colab",
    "text": "Setting up GitHub access from Colab\nIn order to access Git repositories, you need to allow Colab to access the repositories. To set this up, make sure you are logged in to your Github account. Then, follow the steps below.\n\nGo to the main Google Colab page. You will get a splash page that looks like this.\n\n\n\n\n\n\n\n\n\n\nClick on the “GitHub” tab of the orange bar.\n\nClick the checkbox for including private repos if you also want to include repositories you have set to be private.\nAn authorization window from GitHub will pop up. (Make sure you are not blocking pop-up windows.) You should authorize Colab to access your repositories.\nUnder where it says “Enter a GitHub URL or search by organization or user,” search for the desired repository name. Underneath the search bar, you will have a pulldown menu of available notebooks. Select the one you want to work on."
  },
  {
    "objectID": "why-colab-github.html#pushing-your-notebook-to-the-repository",
    "href": "why-colab-github.html#pushing-your-notebook-to-the-repository",
    "title": "Why and How Do We Use Google Coalb and GitHub?",
    "section": "Pushing your notebook to the repository",
    "text": "Pushing your notebook to the repository\nAfter you have made changes to your notebook, you can commit and push them to the repository. To do so from within a Colab notebook, click File → Save a copy in GitHub in GitHub. You will be prompted to add a commit message, and after you click OK, the notebook will be pushed to your repository."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#pandas-data-structures-basics-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#pandas-data-structures-basics-1",
    "title": "Lecture 3",
    "section": "Pandas Data Structures Basics",
    "text": "Pandas Data Structures Basics\nCreate Your Own Data\n\nKnowing how to create Series or DataFrames without loading data from a file is a useful skill.\nSeries is a one-dimensional container.\nA Series is very similar to a Python list, except that each element must be the same dtype (object, int64, float64, or datetime64).\n\nThis is the same behavior as the NumPy array (ndarray).\nIf a column contains the number 1 and the sequence of letters “pizza”, the entire dtype of the column will be a string (which is object).\n\nA DataFrame can be thought of as a dictionary of Series objects.\n\nEach key is the column name and the value is the Series."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-1",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nNumPy Array\n\nPython library NumPy introduces an N-dimensional array object, or ndarray.\n\nPandas implicitly uses ndarray, so here let’s see what ndarray is.\nThe easiest way to create an array is to use the .array() method.\n\n\n\n\nimport pandas as np\ndata1 = [6, 7.5, 8, 0, 1]\narr1 = np.array(data1) \narr1\narr2.ndim\narr2.shape\n\ndata2 = [ [1, 2, 3, 4], \n          [5, 6, 7, '8'] ]\narr2 = np.array(data2)\narr2\narr2.ndim\narr2.shape"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-2",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a Series\n\nA Series is a data structure in pandas.\n\nContaining a sequence of values and a corresponding labels, called the index,\nA Series displays the index on the left and the values on the right,\nThe default index consists of the integers 0 through N-1.\n\npd.Series() creates a one-dimensional container including values and an index.\n\nimport pandas as pd\ns = pd.Series( ['banana', 42] )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-3",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a Series\n\nThe “row number” is shown on the left of the Series.\nThis is actually the index for the Series.\nIt is similar to the row name and row index for DataFrame.\n\n## manually assign index values to a series\n## by passing a Python list\ns = pd.Series(\n  data =[\"Wes McKinney\", \"Creator of Pandas\"],\n  index =[\"Person\", \"Who\"],\n)\n\ns"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-4",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a Series\n\nPandas Series can also be created from ndarrys, tuples, and dictionaries.\nQ. Use dictdata to create a Pandas Series.\n\ndictdata = {\n    \"Name\": [\"William Nordhaus\", \"Ronald Coase\"],\n    \"Occupation\": [\"Economist\", \"Economist\"],\n    \"Born\": [\"1941-05-31\", \"1910-12-29\"],\n    \"Died\": [\"\", \"2013-09-02\"],\n    \"Age\": [81, 102],\n  }"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-5",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-5",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a DataFrame\n\nA DataFrame can be thought of as a dictionary of Series objects.\n\nDictionaries are one of the most common ways of creating a DataFrame.\nThe key represents the column name, and the values are the contents of the column.\n\n\neconomists = pd.DataFrame(\n {  \"Name\": [\"William Nordhaus\", \"Ronald Coase\"],\n    \"Occupation\": [\"Economist\", \"Economist\"],\n    \"Born\": [\"1941-05-31\", \"1910-12-29\"],\n    \"Died\": [\"\", \"2013-09-02\"],\n    \"Age\": [81, 102]  } )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\n\nLet’s re-create our example DataFrame.\n\n## create our example dataframe with a row index label\neconomists = pd.DataFrame(\n  data = {\n    \"Occupation\": [\"Economist\", \"Economist\"],\n    \"Born\": [\"1941-05-31\", \"1910-12-29\"],\n    \"Died\": [\"\", \"2013-09-02\"],\n    \"Age\": [81, 102] },\n  index =  [\"William Nordhaus\", \"Ronald Coase\"],\n  columns =[\"Occupation\", \"Born\", \"Died\", \"Age\"] \n)\n\nQ. Select an economist from economists by the row index label to get a Series."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-1",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nSeries Attributes\n\nWhen a series is printed (i.e., the string representation), the index is printed as the first “column”, and the values are printed as the second “column”.\nThere are many attributes and methods associated with a Series object.\n\nfirst_row = economists.loc['William Nordhaus']\ntype(first_row)\nfirst_row.index\nfirst_row.values\ntype(first_row.values)\nfirst_row.shape\nfirst_row.size\nfirst_row.dtypes"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-2",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nAttributes vs. Methods\n\nAttributes can be thought of as features of an object (in this example, our object is a Series).\nMethods can be thought of as some calculation or operation that is performed on an object.\n\nMethods or functions have round parentheses (()), while attributes do not.\n\nThe subsetting syntax .loc[] and .iloc[] consists of all attributes."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-3",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nSeries Methods\n\nLet’s first get a series of the Age column from our economists DataFrame.\n\nages = economists['Age']\n\nWhen we have a vector of numbers, there are common calculations we can perform.\n\nages.mean()\nages.min()\nages.max()\nages.std()\n\n.mean(), .min(), .max(), and .std() are also methods in np.ndarray."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#workflow",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#workflow",
    "title": "Lecture 3",
    "section": "Workflow",
    "text": "Workflow\nWorking Directory\n\nIn Spyder, we can set the working directory.\n\nWindows: Tools &gt; Preferences &gt; Working directory &gt; The following directory\nMac: python &gt; Preferences &gt; Working directory &gt; The following directory\n\nDownload the CSV file, scientists.csv from the Files section in our Canvas.\n\nCreate the data folder in your working directory.\nThen move the scientists.csv file to the data folder in your working directory."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#descriptive-statistics",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#descriptive-statistics",
    "title": "Lecture 3",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n.describe() Methods\n\nLet’s load the CSV file, scientists.csv.\n\n## If we set the working directory, we do not need to use the absolute path\nscientists = pd.read_csv('data/scientists.csv')  \nages = scientists['Age']\n\nThe .describe() method calculates multiple descriptive statistics for numeric variables.\n\nscientists.describe()\nages.describe()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-4",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nBoolean Subsetting on Series\n\nWe can not only subset values using labels and indices, but also supply a vector of boolean values.\nBoolean subsetting of numeric Series works as follows:\n\nSeries[ Series &gt; VALUE  ]\nSeries[ Series == VALUE  ]\nSeries[ Series &lt; VALUE  ]\n\nQ. What if we wanted to subset our ages by identifying those above the mean?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-5",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-5",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nBoolean Subsetting on Series\n\nLet’s look at what ages &gt; ages.mean() returns.\n\nages &gt; ages.mean()\n\ncond = ages &gt; ages.mean()\nages[cond]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-6",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-6",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nMany of the methods that work on Series (and also DataFrames) are “vectorized”, meaning that they work on the entire vector simultaneously.\nages &gt; ages.mean() returns a vector without any for loops.\n\n\nsame lengthw/ scalarsw/ different lengthw/ different length (e.g.,)\n\n\n\nIf we perform an operation between two vectors of the same length, the resulting vector will be an element-by-element calculation of the vectors.\n\nages + ages\nages * ages\n\n\n\nWhen we perform an operation on a vector using a scalar, the scalar will be recycled across all the elements in the vector.\n\nages + 100\nages * 2\n\n\n\nWhen we are working with vectors of different lengths, the behavior will depend on the type() of the vectors.\n\nWith a Series, the vectors will perform an operation matched by the index.\nWith other types(), the shapes must match.\n\n\n\n\nages + pd.Series( [1, 100] )\nages + np.array( [1, 100] )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-7",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-7",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nWhat’s convenient in Pandas is how data alignment is almost always automatic. - If possible, things will always align themselves with the index label when actions are performed.\nLet’s consider .sort_index() method:\n\nSeries.sort_index(ascending=False) sorts Series by index in descending order.\n\nrev_ages = ages.sort_index(ascending =False) \nrev_ages\nQ. What is ages + rev_ages?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nParts of a DataFrame\n\nThe DataFrame is the most common Pandas object.\n\nIt can be thought of as Python’s way of storing spreadsheet-like data.\nMany of the features of the Series data structure carry over into the DataFrame.\n\nThere are 3 main parts of a Pandas DataFrame object:\n\n\n.index, (2) .columns, and (3) .values\n\n\n\nscientists.index\nscientists.columns\nscientists.values"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-1",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nBoolean Subsetting on DataFrames\n\nBoolean subsetting of DataFrames works like boolean subsetting a Series.\n\nDataFrame[ DataFrame['VARIABLE_NAME'] &gt; VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] == VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] &lt; VALUE  ]\n\n\n## boolean vectors will subset rows\nscientists.loc[ scientists['Age'] &gt; scientists['Age'].mean() ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-2",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nSubsetting Multiple Rows and Columns"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-3",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nPandas supports broadcasting because the Series and DataFrame objects are built on top of the numpy library.\n\nBroadcasting describes what happens when performing operations between array-like objects.\nThese behaviors depend on the type of object, its length, and any labels associated with the object."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-4",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nw/ scalarsw/ Series\n\n\n\nWhen we perform an action on a dataframe with a scalar, it will try to apply the operation on each cell of the dataframe.\n\nscientists * 2\n\n\n\nBy default, arithmetic operations between DataFrames and Series match the index of the Series on the DataFrame’s columns,\nThe operations will be broadcasted along the rows.\n\npd.Series([10]) + scientists[['Age']]\npd.Series([10], index = ['Age']) + scientists[['Age']]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nAdd Additional Columns\n\nThe type of the Born and Died columns is object, meaning they are strings or a sequence of characters.\n\nscientists.dtypes\n\nWe can convert the strings to a proper datetime type so we can perform common date and time operations.\n\n## format the 'Born' column as a datetime\nborn_datetime = pd.to_datetime( scientists['Born'] )\n\ndied_datetime = pd.to_datetime( scientists['Died'] )\n\nMore examples with datetimes would be discussed later in March or April."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-1",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nAdd Additional Columns\n\nWe can create a new set of columns that contain the datetime representations of the object (string) dates.\n\nscientists['born_dt'], scientists['died_dt'] = (\n  born_datetime,\n  died_datetime\n)\n\nscientists.head()\nscientists.shape\nscientists.dtypes"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-2",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDirectly Change a Column\n\n(1)(2)(3)(4)(5)\n\n\n\nLet’s look at the original Age values.\n\nscientists['Age']\n\n\n\nLet’s shuffle the values using .sample().\n\n## the frac=1 tells pandas to randomly select 100% of the values\n## the random_state makes the randomization the same each time\nscientists['Age'].sample(frac=1, random_state = 210)\n\n\n\nLet’s assign scientists['Age'] to the shuffled one.\n\nscientists['Age'] = (\n  scientists['Age']\n  .sample(frac=1, random_state = 210)\n)\nscientists['Age']\n\nHow is scientists['Age']?\n\n\n\n\nWe tried to randomly shuffle the values, but when we assigned the values back into the dataframe, it reverted back to the original order.\n\nThat’s because Pandas will try to automatically join on the .index values on many operations, for this example to get around this problem we need to remove that .index information.\nOne way of doing that, is to assign just the .values of the shuffled values that does not have any .index value associated with it.\n\n\n\n\nscientists['Age'] = (\n  scientists['Age']\n  .sample(frac=1, random_state = 210)\n  .values    ## remove the index so it doesn't auto align the values\n)\n\nscientists['Age']\n\nHow is scientists['Age'] now?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-3",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDirectly Change a Column\n\nHere is how we re-calculate the real ages.\n\n## subtracting dates will give us number of days\nscientists['age_days'] =  scientists['died_dt'] - scientists['born_dt']\n\n## we can convert the value to just the year\n## using the astype method\nscientists['age_years'] = (\n  scientists['age_days']\n  .astype('timedelta64[Y]')\n)\n\nscientists"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-4",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nModifying Columns with .assign()\n\n(1)(2)lambda (1)lambda (2)\n\n\n\nLet’s redo the age_years column creation, but this time using .assign().\n\nscientists = scientists.assign(\n  ## new columns on the left of the equal sign\n  ## how to calculate values on the right of the equal sign\n  ## separate new columns with a comma\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = scientists['age_days'].astype('timedelta64[Y]')\n)\n\n\n\nWhen calculating age_year_assign, we did not use age_days_assign.\n\nTo be able to use age_days_assign when calculating age_year_assign in the previous panel, we need to know about lambda functions.\n\n\nscientists = scientists.assign(\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = lambda some_df: some_df['age_days_assign'].astype('timedelta64[Y]')\n)\n\nWe will cover lambda functions in detail later.\n\n\n\n\nPython has support for so-called anonymous or lambda functions.\n\nLambda functions are a way of writing functions consisting of a single statement, the result of which is the return value.\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\n\nshort_function(2)\nequiv_anon(2)\n\n\n\nThe power of lambda is better shown when we use them as an anonymous function inside another function.\n\nSay we have a function definition that takes one argument, and that argument will be multiplied with an unknown number:\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\n\nshort_function(2)\nequiv_anon(2)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-5",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-5",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDropping Values\n\nColumnsRows\n\n\n\nTo drop a column, we can select columns to drop with the .drop() method with axis = 1 or axis = \"columns\" on our dataframe.\n\n## all the current columns in our data\nscientists.columns\n\n## drop the shuffled age column\n## we provide the axis=1 argument to drop column-wise\nscientists_dropped = scientists.drop( ['Age'], axis =\"columns\")\nscientists_dropped.columns\n\n\n\nTo drop rows, we can select rows by index to drop with the .drop() method with axis = 0, which is default.\n\n## all the current columns in our data\nscientists.columns\n\n## drop rows by their indices\nscientists_rows_dropped = scientists.drop( [2, 4, 6] )\nscientists_rows_dropped"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-1",
    "title": "Lecture 1",
    "section": "Instructor",
    "text": "Instructor\nCurrent Appointment & Education\n\nName: Byeong-Hak Choe.\nAssistant Professor of Data Analytics and Economics, School of Business at SUNY Geneseo.\nPh.D. in Economics from University of Wyoming.\nM.S. in Economics from Arizona State University.\nM.A. in Economics from SUNY Stony Brook.\nB.A. in Economics & B.S. in Applied Mathematics from Hanyang University at Ansan, South Korea.\n\nMinor in Business Administration.\nConcentration in Finance."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-2",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-2",
    "title": "Lecture 1",
    "section": "Instructor",
    "text": "Instructor\nEconomics, Data Science, and Climate Change\n\nI consider myself an applied economist specializing in environmental economics, with a specific emphasis on climate change.\nMy methodological approach involves leveraging causal inference, econometrics, machine learning methods, and various data science tools for conducting empirical analyses.\nChoe, B.H., 2021. “Social Media Campaigns, Lobbying and Legislation: Evidence from #climatechange/#globalwarming and Energy Lobbies.”\nChoe, B.H. and Ore-Monago, T., 2024. “Governance and Climate Finance in the Developing World”"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-1",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nEmail, Class & Office Hours\n\nEmail: bchoe@geneseo.edu\nClass Homepage:\n\nhttps://brightspace.geneseo.edu/\nhttp://bcdanl.github.io/module-1/\n\nOffice: South Hall 301\nOffice Hours:\n\nBy appointment via email"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-2",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-2",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Description\n\nThis course aims to provide overview of how one can process, clean, and crunch datasets with practical case studies.\nKey topics include:\n\nloading, slicing, filtering, transforming, reshaping, and merging data\nsummarizing and visualizing data,\nexploratory data analysis.\n\nWe will cover these topics to solve real-world data analysis problems with thorough, detailed examples."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-3",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-3",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nReference Materials\n\nPython for Data Analysis (3rd Edition) by Wes McKinney\nGuide for Quarto\nPython Programming for Data Science by Tomas Beuzen\nCoding for Economists by Arthur Turrell\nPython for Econometrics in Economics by Fabian H. C. Raters\nQuantEcon DataScience - Python Fundamentals by Chase Coleman, Spencer Lyon, and Jesse Perla\nQuantEcon DataScience - pandas by Chase Coleman, Spencer Lyon, and Jesse Perla"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-4",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-4",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Requirements\n\nLaptop: You should bring your own laptop (Mac or Windows) to the classroom.\n\nIt is recommended to have 2+ core CPU, 4+ GB RAM, and 500+ GB disk storage in your laptop for this course.\n\nHomework: There will be six homework assignments.\nExam: There will be one take-home exam.\nDiscussions: You are encouraged to participate in GitHub-based online discussions for each lecture, classwork, and homework.\n\nCheckout the netiquette policy in the syllabus."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-5",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-5",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Schedule and Contents\n\nThere will be tentatively 7 class sessions."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-6",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-6",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nAssessments\n\nEach of the six homework assignments accounts for 10% of the total percentage grade.\nThe exam account for 30% of the total percentage grade.\nParticipation in discussions accounts for 10% of the total percentage grade.\n\n\\[\n\\begin{align}\n(\\text{Total Percentage Grade}) =\\quad\\, &0.60\\times(\\text{Total Homework Score})\\notag\\\\\n\\,+\\, &0.30\\times(\\text{Take-Home Exam Score})\\notag\\\\\n\\,+\\, &0.10\\times(\\text{Total Discussion Score})\\notag\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-data-analytics",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-data-analytics",
    "title": "Lecture 1",
    "section": "Why Data Analytics?",
    "text": "Why Data Analytics?\n\n\nFill in the gaps left by traditional business and economics classes.\n\nPractical skills that will benefit your future career.\nNeglected skills like how to actually find datasets in the wild and clean them.\n\nData analytics skills are largely distinct from (and complementary to) the core quantitative works familiar to business undergrads.\n\nData visualization, cleaning and wrangling; databases; machine learning; etc.\n\nIn short, we will cover things that I wish someone had taught me when I was undergraduate."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#you-at-the-end-of-this-course",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#you-at-the-end-of-this-course",
    "title": "Lecture 1",
    "section": "You, at the end of this course",
    "text": "You, at the end of this course"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-data-analytics-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-data-analytics-1",
    "title": "Lecture 1",
    "section": "Why Data Analytics?",
    "text": "Why Data Analytics?\n\nData analysts use analytical tools and techniques to extract meaningful insights from data.\n\nSkills in data analytics are also useful for business analysts or market analysts.\n\nBreau of Labor Statistics forecasts that the projected growth rate of the employment in the industry related to data analytics from 2021 to 2031 is 36%.\n\nThe average growth rate for all occupations is 5%."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-personal-website",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-personal-website",
    "title": "Lecture 1",
    "section": "Why Personal Website?",
    "text": "Why Personal Website?\nBenefits of a Personal Website in Data Analytics\n\n\nHere are the example websites:\n\nByeong-Hak’s Website\nDANL Website Template\n\n\n\n\n\nProfessional Showcase: Display skills and projects\nVisibility and Networking: Increase online presence\nControlled Narrative: Manage your professional brand\nContent Sharing and Engagement: Publish articles, insights\nJob Opportunities: Attract potential employers and clients\nLong-term Asset: A growing repository of your career journey\nReproducible Research: Showcase data-driven reports"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-python-r-and-databases",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-python-r-and-databases",
    "title": "Lecture 1",
    "section": "Why Python, R, and Databases?",
    "text": "Why Python, R, and Databases?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-python-r-and-databases-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-python-r-and-databases-1",
    "title": "Lecture 1",
    "section": "Why Python, R, and Databases?",
    "text": "Why Python, R, and Databases?\n\nStack Overflow is the most popular Q & A website specifically for programmers and software developers in the world.\nSee how programming languages have trended over time based on use of their tags in Stack Overflow from 2008 to 2022.\n\n\n\nMost Popular Languagues\n\n\nData Science and Big Data"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#the-state-of-the-art",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#the-state-of-the-art",
    "title": "Lecture 1",
    "section": "The State of the Art",
    "text": "The State of the Art\nGenerative AI and ChatGPT\n\n\nData Science and Big Data Trend\nFrom 2008 to 2023\n\n\nProgrammers in 2024"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#the-state-of-the-art-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#the-state-of-the-art-1",
    "title": "Lecture 1",
    "section": "The State of the Art",
    "text": "The State of the Art\nGenerative AI and ChatGPT\n\nGenerative AI refers to a category of artificial intelligence (AI) that is capable of generating new content, ranging from text, images, and videos to music and code.\n\n\n\nIn the early 2020s, advances in transformer-based deep neural networks enabled a number of generative AI systems notable for accepting natural language prompts as input.\n\nThese include large language model (LLM) chatbots such as ChatGPT, Copilot, Bard, and LLaMA.\n\nChatGPT (Chat Generative Pre-trained Transformer) is a chatbot developed by OpenAI and launched on November 30, 2022.\n\nBy January 2023, it had become what was then the fastest-growing consumer software application in history."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#the-state-of-the-art-2",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#the-state-of-the-art-2",
    "title": "Lecture 1",
    "section": "The State of the Art",
    "text": "The State of the Art\nGenerative AI and ChatGPT\n\nUsers around the world have explored how to best utilize GPT for writing essays and programming codes.\n\n\n\n\nIs AI a threat to data analytics?\n\nFundamental understanding of the subject matter is still crucial for effectively utilizing AI’s capabilities.\n\n\n\n\n\nIf you use Generative AI such as ChatGPT, please try to understand what ChatGPT gives you.\n\nCopying and pasting it without any understanding harms your learning opportunity."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-git",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-git",
    "title": "Lecture 1",
    "section": "What is Git?",
    "text": "What is Git?\n\n\n\n\n\\(\\quad\\)\n\nGit is the most popular version control tool for any software development.\n\nIt tracks changes in a series of snapshots of the project, allowing developers to revert to previous versions, compare changes, and merge different versions.\nIt is the industry standard and ubiquitous for coding collaboration."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-git-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-git-1",
    "title": "Lecture 1",
    "section": "What is Git?",
    "text": "What is Git?\n\n\n\n\ngit add .\ngit commit -m \"any message is here\"\ngit push -u origin main\n\\(\\quad\\)\n\n\nGit operates primarily through command-line tools (e.g., Terminal) and is local to a user’s computer.\n\nIt has a steep learning curve.\n\nWe will not do git collaboration but use only the 3-step git commands on Terminal to update a website."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-github",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-github",
    "title": "Lecture 1",
    "section": "What is GitHub?",
    "text": "What is GitHub?\n\nGitHub is a web-based hosting platform for Git repositories to store, manage, and share code.\n\n\n\nYour personal website will be hosted on a GitHub repository.\nCourse contents will be posted not only in Brightspace but also in our GitHub repositories (“repos”) and websites.\nGithub is useful for many reasons, but the main reason is how user friendly it makes uploading and sharing code."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-python",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-python",
    "title": "Lecture 1",
    "section": "What is Python?",
    "text": "What is Python?\n\n\nPython is an interpreted, object-oriented, high-level programming language with dynamic semantics.\n\nIt supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\nIts extensive standard library and the vast ecosystem of third-party packages make it suitable for a wide range of applications, from web development and data analysis to artificial intelligence and scientific computing."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-google-colab",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-google-colab",
    "title": "Lecture 1",
    "section": "What is Google Colab?",
    "text": "What is Google Colab?\n\nhttps://colab.research.google.com/ is analogous to Google Drive, but specifically for writing and executing Python code in your browser.\n\nThe base Colab link listed above leads to a Python notebook introducing Colab and how to use it.\n\nThis video also helps get started with Colab if you are unfamiliar with the format!\n\nhttps://www.youtube.com/watch?v=inN8seMm7UI"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-use-colab",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-use-colab",
    "title": "Lecture 1",
    "section": "Why use Colab?",
    "text": "Why use Colab?\n\n\nA key benefit of Colab is that it is entirely free to use and has many of the standard Python modules pre installed.\n\nIt allows for CPU or GPU usage, even for free users, and stores the files in Google’s servers so you can access your files from anywhere you can connect to the Internet.\n\nUsing Colab also means you can entirely avoid the process of installing Python and any dependencies onto your computer.\nColab notebooks don’t just contain Python code. They can contain text, images, and HTML!\nUltimately, they’re intuitive to use and let you jump right into the code and data analysis without needing to worry about the more cumbersome details needed to run Python notebooks on a personal computer."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-r",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-r",
    "title": "Lecture 1",
    "section": "What is R?",
    "text": "What is R?\n\nR is a programming language and software environment designed for statistical computing and graphics.\nR has become a major tool in data analysis, statistical modeling, and visualization.\n\nIt is widely used among statisticians and data scientists for developing statistical software and performing data analysis.\nR is open source and freely available."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-rstudio",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-rstudio",
    "title": "Lecture 1",
    "section": "What is RStudio?",
    "text": "What is RStudio?\n\n\nRStudio is an integrated development environment (IDE) mainly for R.\nRStudio is a user-friendly interface that makes using R easier and more interactive.\n\nIt provides a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging, and workspace management.\nIt integrates well with Git."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-vs.-r",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-vs.-r",
    "title": "Lecture 1",
    "section": "Python vs. R",
    "text": "Python vs. R\n\n\n\nPython\n\nPython can be used for a wide range of applications, from web and game development to machine learning, making it a highly versatile language.\nPython has the largest community in the programming world, providing a wealth of resources, libraries, and frameworks.\n\n\nR\n\nR is particularly strong in statistical analysis and visualization, with a vast number of packages for statistical methods, including machine learning.\nThe community around R, particularly in academia and research, is very active.\n\n\n\n\nBoth Python and R hold significant value in industry and government sectors.\n\nHowever, Python is often more favored for roles in the industry, whereas R tends to be preferred for positions in the public sector."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-1",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nObjectives\n\nIn this Section, we will discuss the basics of objects, types, operations, and imports.\n\nThese are the basic building blocks of almost all programming languages and will serve you well for your coding journey.\n\nEverything is an object, and every object has a type."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-2",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-2",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nVariables Are Names, Not Places\n\nThe most basic built-in data types that you’ll need to know about are: integers 10, floats 1.23, strings like this, booleans True, and nothing None.\nPython also has a built-in type called a list [10, 15, 20] that can contain anything, even different types\n\nlist_example = [10, 1.23, \"like this\", True, None]\nprint(list_example)\ntype(list_example)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-3",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-3",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nTypes\n\n\n\n\n\n\n\nPython’s basic data types\n\n\n\n\n\n\nThe second column (Type) contains the Python name of that type.\nThe third column (Mutable?) indicates whether the value can be changed after creation."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-4",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-4",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBrackets\n\nThere are several kinds of brackets in Python, including [], {}, and ().\n\n\n[]{}()\n\n\n\n[] is used to denote a list or to signify accessing a position using an index.\n\nvector = ['a', 'b']\nvector[0]\n\n\n\n{} is used to denote a set or a dictionary (with key-value pairs).\n\n{'a', 'b'}\n{'first_letter': 'a', 'second_letter': 'b'}\n\n\n\n() is used to denote a tuple, or the arguments to a function, e.g., function(x) where x is the input passed to the function, or to indicate the order operations are carried out.\n\nnum_tup = (1, 2, 3)\nsum(num_tup)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-5",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-5",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\n\nLists are a really useful way to work with lots of data at once.\n\nWe can also construct them by appending entries:\n\n\nlist_example = [10, 1.23, \"like this\", True, None]\nlist_example.append(\"one more entry\")\nprint(list_example)\n\nWe can access earlier entries using an index, which begins at 0 and ends at one less than the length of the list.\n\nprint(list_example[0])\nprint(list_example[-1])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-6",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-6",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\n\nSlicing can be even more elaborate than that because we can jump entries using a second colon.\n\n# range() produces a list of integers from the value to one less than the last\nlist_of_numbers = list(range(1, 11))\nstart = 1\nstop = -1\nstep = 2\nprint(list_of_numbers[ start : stop : step ])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-7",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-7",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\n\nAll of the basic operators you see in mathematics are available to use: + for addition, - for subtraction, * for multiplication, ** for powers, / for division, // for integer division, and % for modulo.\n\nThese work as you’d expect on numbers.\nThese operators are sometimes defined for other built-in data types too.\n\n\nWe can ‘sum’ strings (which really concatenates them):\n\n\nstring_one = \"This is an example \"\nstring_two = \"of string concatenation\"\nstring_full = string_one + string_two\nprint(string_full)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-8",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-8",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\n\n\n\nIt works for lists too:\n\nlist_one = [\"apples\", \"oranges\"]\nlist_two = [\"pears\", \"satsumas\"]\nlist_full = list_one + list_two\nprint(list_full)\n\n\nWe can multiply strings!\n\nstring = \"apples, \"\nprint(string * 3)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-9",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-9",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\n\nWe can combine the arithmetic operators with assignment by putting the operator before the =.\n\n\n\n\nx += 1 is equivalent to x = x + 1.\n\nx = 3\nx += 1\n\n\na -= 2 is equivalent to a = a - 2.\n\na = 2\na -= 2"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-10",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-10",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\nQ. Using Python operations only, what is \\[\\frac{2^5}{7 \\cdot (4 - 2^3)} \\quad\\text{?}\\]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-11",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-11",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nStrings\n\nFrom strings, we can access the individual characters via slicing and indexing.\n\nstring = \"cheesecake\"\nprint( string[-4:] )\n\nBoth lists and strings will allow us to use the len() command to get their length:\n\n\n\n\nx += 1 is equivalent to x = x + 1.\n\nstring = \"cheesecake\"\nprint( \"String has length:\" )\nprint( len(string) )\n\n\na -= 2 is equivalent to a = a - 2.\n\nlist_of_numbers = range(1, 20)\nprint( \"List of numbers has length:\" )\nprint( len(list_of_numbers) )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-12",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-12",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nBoolean data have either True or False value.\nThere are two types of operation that are associated with booleans: boolean operations.\n\nExisting booleans are combined, and condition operations, which create a boolean when executed."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-13",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-13",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nConditions are expressions that evaluate as booleans."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-14",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-14",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nThe == is an operator that compares the objects on either side and returns True if they have the same values\n\nboolean_condition1 = 10 == 20\nprint(boolean_condition1)\n\nboolean_condition2 = 10 == '10'\nprint(boolean_condition2)\nQ. What does not (not True) evaluate to?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-15",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-15",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nThe real power of conditions comes when we start to use them in more complex examples, such as if statements.\n\nname = \"Geneseo\"\nscore = 99\n\nif name == \"Geneseo\" and score &gt; 90:\n    print(\"Geneseo, you achieved a high score.\")\n\nif name == \"Geneseo\" or score &gt; 90:\n    print(\"You could be called Geneseo or have a high score\")\n\nif name != \"Geneseo\" and score &gt; 90:\n    print(\"You are not called Geneseo and you have a high score\")"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-16",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-16",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nGiven that == and != test for equality and not equal, respectively, you may be wondering what the keyword is is for.\n\nRemember that everything in Python is an object, and that values can be assigned to objects.\n== and != compare values, while is compare objects.\n\n\n\n\nname_list = [\"Ada\", \"Adam\"]\nname_list_two = [\"Ada\", \"Adam\"]\n\n# Compare values\nprint(name_list == name_list_two)\n\n# Compare objects\nprint(name_list is name_list_two)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-17",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-17",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nOne of the most useful conditional keywords is in.\n\nThis one must pop up ten times a day in most coders’ lives because it can pick out a variable or make sure something is where it’s supposed to be.\n\n\nname_list = [\"Lovelace\", \"Smith\", \"Hopper\", \"Babbage\"]\n\nprint(\"Lovelace\" in name_list)\n\nprint(\"Bob\" in name_list)\nQ. Check if “a” is in the string “Sun Devil Arena” using in. Is “a” in “Anyone”?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-18",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-18",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nOne conditional construct we’re bound to use at some point, is the if-else chain:\n\nscore = 98\n\nif score == 100:\n    print(\"Top marks!\")\nelif score &gt; 90 and score &lt; 100:\n    print(\"High score!\")\nelif score &gt; 10 and score &lt;= 90:\n    pass\nelse:\n    print(\"Better luck next time.\")"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-19",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-19",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nQ. Create a new if-else chain that prints “well done” if a score is over 90, “good” if between 40 and 90, and “bad luck” otherwise."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-20",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-20",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\nWe can make multiple assignment or multiple boolean comparisons in a single line.\n\na, b = 3, 6\n\n1 &lt; a &lt; b &lt; 20"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-1",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-1",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nBuilt-in Aggregation Methods\n\n(1)(2)(3)(4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know we can calculate multiple summary statistics simultaneously with .describe().\n## group by continent and describe each group\ncontinent_describe = df.groupby('continent')[\"lifeExp\"].describe()\ncontinent_describe"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-2",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-2",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nAggregation Functions\nFunctions From Other Libraries\nWe can also use other libraries’ functions that are not listed in the previous tables. (e.g., numpy, scipy)\n## calculate the average life expectancy by continent\n## but use the np.mean function\ncont_le_agg = df.groupby('continent')[\"lifeExp\"].agg(np.mean)\nQ. Add a new variable, the log of lifeExp, using np.log to DataFrame df."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-3",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-3",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nAggregation Functions\n\n(1)(2)(3)(4)(5)\n\n\nSometimes we may want to perform a calculation that is not provided by Pandas or another library.\n\nLet’s create our own mean function with def.\n\ndef my_mean(values):\n  n = len(values)    ## get the total number of numbers for the denominator\n  sum = 0   ## start the sum at 0\n  for value in values:\n      sum += value   ## add each value to the running sum\n  return sum / n   ## return the summed values divided by the number of values\n\n\nWe can pass our custom function straight into the .agg() method with my_mean.\n## use our custom function into agg\nagg_my_mean = df.groupby('year')[\"lifeExp\"].agg(my_mean)\n\nagg_my_mean\n\n\nWe can write functions that take multiple parameters.\ndef my_mean_diff(values, diff_value):\n    \"\"\"Difference between the mean and diff_value\n    \"\"\"\n    n = len(values)\n    sum = 0\n    for value in values:\n        sum += value\n    mean = sum / n\n    return(mean - diff_value)\n\n\nUsing my_mean_diff, we will calculate the global average life expectancy, diff_value, and subtract it from each grouped value.\n## custom aggregation function with multiple parameters\nagg_mean_diff = (\n  df\n  .groupby(\"year\")[\"lifeExp\"]\n  .agg(my_mean_diff, diff_value = global_mean)\n)\n\n\nWhen we want to calculate multiple functions, we can pass the multiple functions into .agg()\n\n## calculate the count, mean, std of the lifeExp by continent\ngdf = (\n  df\n  .groupby(\"year\")\n  [\"lifeExp\"]\n  .agg([np.count_nonzero, np.mean, np.std])\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-4",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-4",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nUse a dict in .agg() on a Series\nWe can also pass .agg() a Python dictionary ({key : value}). - The results will differ depending on whether we are aggregating directly on a DataFrame or on a Series object. - When specifying a dict on a grouped DataFrame, the keys are the columns of the DataFrame, and the values are the functions used in the aggregated calculation.\ngdf_dict = df.groupby(\"year\").agg(\n  {\"lifeExp\": \"mean\",\n    \"pop\": \"median\",\n    \"gdpPercap\": \"median\"})"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-5",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-5",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nUse a dict in .agg() on a DataFrame\nTo have user-defined column names in the output of a grouped series calculation, we need to rename those columns.\n\n\ngdf = (\n  df\n  .groupby(\"year\")\n  [\"lifeExp\"]\n  .agg(\n    [\n      np.count_nonzero,\n      np.mean,\n      np.std,\n    ]\n  )\n\n  .rename(\n    columns={\n      \"count_nonzero\": \"count\",\n      \"mean\": \"avg\",\n      \"std\": \"std_dev\",\n    }\n  )\n  .reset_index() ## return a flat dataframe\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-6",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-6",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform\nWhen we transform data, we pass values from our dataframe into a function.\n\nThe function then “transforms” the data.\nUnlike .agg(), which can take multiple values and return a single (aggregated) value, .transform() takes multiple values and returns a one-to-one transformation of the values.\nThat is, it does not reduce the amount of data."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-7",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-7",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\n\nLet’s calculate the z-score of our life expectancy data by year. \\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\n\\(x\\) is a value in a variable\n\\(\\mu\\) is the average of a variable\n\\(\\sigma\\) is the standard deviation of a variable\n\n\\[\n\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n}(x - \\mu)^{2}}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-8",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-8",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\nThe following is a function, my_zscore(x), that calculates a z-score using Pandas methods.\ndef my_zscore(x):\n  '''Calculates the z-score of provided data\n  'x' is a vector or series of values\n  '''\n  return((x - x.mean()) / x.std())"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-9",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-9",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\nNow we can use this function to .transform() our data by group.\ntransform_z = df.groupby('year')[\"lifeExp\"].transform(my_zscore)\n\nNote that both df and transform_z have the same number of rows and data.\n\ndf.shape\ntransform_z.shape"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-10",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-10",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\nThe scipy library has its own zscore() function.\nLet’s use its zscore() function in a .groupby() .transform() and compare it to what happens when we do not use .groupby().\nfrom scipy.stats import zscore\n## calculate a grouped zscore\nsp_z_grouped = df.groupby('year')[\"lifeExp\"].transform(zscore)\n\n## calculate a nongrouped zscore\nsp_z_nogroup = zscore(df[\"lifeExp\"])\n\ntransform_z.head()\nsp_z_grouped.head()\nsp_z_nogroup[:5]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-11",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-11",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nagg, transform, and apply: when to use each with a groupby\nWith all of the different options available, it can be confusing to know when to use the different functions available for performing groupby operations, namely: .agg, .transform, and .apply.\nHere are the key points to remember: - Use .agg when using a groupby, but you want your groups to become the new index. - Use .transform when using a groupby, but you want to retain your original index. - Use .apply when using a groupby, but you want to perform operations that will leave neither the original index nor an index of groups."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-12",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-12",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\n.filter()\n.filter() allows us to split our data by keys, and then perform some kind of boolean subsetting on the data.\n\nLet’s work with the tips data set from seaborn:\n\nimport seaborn as sns\ntips = sns.load_dataset('tips')\n\n## note the number of rows in the original data\ntips.shape\n\n\n## look at the frequency counts for the table size\ntips['size'].value_counts()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-13",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-13",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\n.filter() (cont.)\nSuppose we want each group to consist of 30 or more observations. - To accomplish this goal, we can use the .filter() method on a grouped operation.\n## filter the data such that each group has more than 30 observations\ntips_filtered = (\n  tips\n  .groupby(\"size\")\n  .filter( lambda x : x[\"size\"].count() &gt;= 30 )\n)\n\ntips_filtered.shape\ntips_filtered['size'].value_counts()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-14",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-14",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nThe pandas.core.groupby.DataFrameGroupBy object\nThe .agg(), .transform(), and .filter() methods are commonly used after the .groupby().\ntips_10 = sns.load_dataset('tips').sample(10, random_state=42)\nprint(tips_10)\n\nWe can choose to save just the groupby object without running any other .agg(), .transform(), or .filter() method on it.\n\n\n\n## save just the grouped object\ngrouped = tips_10.groupby('sex')\ngrouped\n\n## see the actual groups of the groupby\n## it returns only the index\ngrouped.groups  ## row numbers?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-15",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-15",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nGroup Calculations Involving Multiple Variables\n\nWe have been usually performing .groupby() calculations on a single column.\n\nIf we specify the calculation we want right after the .groupby(), however, Python will perform the calculation on all the columns it can and silently drop the rest.\n\n\n## calculate the mean on relevant columns\navgs = grouped.mean()\navgs\n\n## list all the columns\ntips_10.columns  ## not all the columns reported a mean."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-16",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-16",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nSelecting a Group\nIf we want to extract a particular group, we can use the .get_group() method, and pass in the group that we want.\n\nFor example, if we wanted the Female values:\n\n## get the 'Female' group\nfemale = grouped.get_group('Female')\nfemale"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-17",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-17",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nIterating Through Groups\n\nAnother benefit of saving just the groupby object is that we can then iterate through the groups individually.\nWe can iterate through our grouped values just like any other container (e.g., list, dictionary) in Python using a for-loop.\n\nfor sex_group in grouped:\n    print(sex_group)\n    \n## we can't really get the 0 element from the grouped object\nprint(grouped[0])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-18",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-18",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nIterating Through Groups (cont.)\n\nLet’s modify the for loop to just show the first element, along with some of the things we get when we loop over the grouped object.\n\n\n\nfor sex_group in grouped:\n    type(sex_group) ## tuple\n    len(sex_group) ## length \n\n    ## get the first element\n    first_element = sex_group[0]\n    first_element\n    type(sex_group[0])\n\n    ## get the second element\n    second_element = sex_group[1]\n    second_element\n    type(second_element)\n\n    ## print what we have\n    print(f'what we have:')\n    print(sex_group)\n\n    ## stop after first iteration\n    break"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-19",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-19",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nIterating Through Groups (cont.)\n\nThe option of iterating through groups with for-loop is available for us if we need to iterate through the groups one at a time."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-20",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-20",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nMultiple Groups\n\nWe can add multiple variables to the .groupby()\n\n## mean by sex and time\nbill_sex_time = tips_10.groupby(['sex', 'time'])\n\ngroup_avg = bill_sex_time.mean()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-21",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-21",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nFlattening the Results (.reset_index())\n\nLet’s look at the type of the group_avg we just calculated.\n\ntype(group_avg)\n\nIf we look at the columns and the index, we get what we expect.\n\ngroup_avg.columns\ngroup_avg.index"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-22",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-22",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nFlattening the Results (.reset_index())\n\nIf we want to get a regular flat DataFrame back, we can call the .reset_index() method on the results.\n\ngroup_method = tips_10.groupby(['sex', 'time']).mean().reset_index()\ngroup_method\n\nAlternatively, we can use the as_index = False parameter in the .groupby() method (it is True by default).\n\ngroup_param = tips_10.groupby(['sex', 'time'], as_index=False).mean()\ngroup_param"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-23",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-23",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nSometimes, we may want to chain calculations after a .groupby() method. - We can always “flatten” the results and then execute another .groupby() statement, but that may not always be the most efficient way of performing the calculation.\nDownload epi_sim.zip from the Files section in Canvas.\nLet’s begin with this epidemiological simulation data on influenza cases in Chicago.\n## notice that we can even read a compressed zip file of a csv\nintv_df = pd.read_csv('data/epi_sim.zip')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-24",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-24",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nThe data set includes six columns: - ig_type: edge type (type of relationship between two nodes in the network, such as “school” and “work”) - intervened: time in the simulation at which an intervention occurred for a given person (pid) - pid: simulated person’s ID number - rep: replication run (each set of simulation parameters was run multiple times) - sid: simulation ID - tr: transmissibility value of the influenza virus"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-25",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-25",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nLet’s count the number of interventions for each replicate, intervention time, and treatment value. - Here, we are counting the ig_type arbitrarily. - We just need a value to get a count of observations for the groups.\ncount_only = (\n  intv_df\n  .groupby([\"rep\", \"intervened\", \"tr\"])\n  [\"ig_type\"]\n  .count()\n)\n\ncount_only"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-26",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-26",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nNow that we’ve done a .groupby() .count(), we can perform an additional .groupby() that calculates the average value. - However, our initial .groupby() method does not return a regular flat DataFrame.\ntype(count_only)\nThe results take the form of a multi-index Series."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-27",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-27",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nIf we want to do another .groupby() operation, we have to pass in the levels parameter to refer to the multi-index levels. - Here we pass in [0, 1, 2] for the first, second, and third index levels, respectively.\ncount_mean = count_only.groupby(level=[0, 1, 2]).mean()\ncount_mean.head()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-28",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-28",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nWe can combine all of these operations in a single command.\ncount_mean = (\n    intv_df\n    .groupby([\"rep\", \"intervened\", \"tr\"])[\"ig_type\"]\n    .count()\n    .groupby(level=[0, 1, 2])\n    .mean()\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-29",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-29",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nSee how the relationship between intervened and ig_type varies by rep and tr.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfig = sns.lmplot(\n   data = count_mean.reset_index(),\n   x = \"intervened\",\n   y = \"ig_type\",\n   hue = \"rep\",\n   col = \"tr\",\n   fit_reg = False,\n   palette = \"viridis\" )\n   \nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html",
    "href": "danl-cw/danl-m1-cw-4.html",
    "title": "Classwork 4",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom skimpy import skim\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#load-libraries",
    "href": "danl-cw/danl-m1-cw-4.html#load-libraries",
    "title": "Classwork 4",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom skimpy import skim\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#load-dataframe",
    "href": "danl-cw/danl-m1-cw-4.html#load-dataframe",
    "title": "Classwork 4",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbeer_mkt = pd.read_csv('https://bcdanl.github.io/data/beer_markets.csv')\nbeer_mkt.head(10)\n\n\nVariable Description\n\nhh: An identifier of the purchasing household;\n_purchase_desc: Details on the purchased item;\nquantity: Number of items purchased;\nbrand: BUD LIGHT, BUSCH LIGHT, COORS LIGHT, MILLER LITE, or NATURAL LIGHT;\nspent: Total dollar value of purchase;\nbeer_floz: Total volume of beer, in fluid ounces;\nprice_per_floz: Price per fl.oz. (i.e., spent/beer_floz);\ncontainer: Type of container;\npromo: Whether the item was promoted (coupon or something else);\nmarket: Scan-track market (or state if rural);\nvarious demographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher.\n\nSummarize DataFrame beer_mkt."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#q1a",
    "href": "danl-cw/danl-m1-cw-4.html#q1a",
    "title": "Classwork 4",
    "section": "Q1a",
    "text": "Q1a\n\nSort the DataFrame beer_mkt by hh in ascending order."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#q1b",
    "href": "danl-cw/danl-m1-cw-4.html#q1b",
    "title": "Classwork 4",
    "section": "Q1b",
    "text": "Q1b\n\nFind the top 5 beer markets in terms of the number of households that purchased beer."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#q1c",
    "href": "danl-cw/danl-m1-cw-4.html#q1c",
    "title": "Classwork 4",
    "section": "Q1c",
    "text": "Q1c\n\nFind the top 5 beer markets in terms of the amount of total beer consumption."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#q1d",
    "href": "danl-cw/danl-m1-cw-4.html#q1d",
    "title": "Classwork 4",
    "section": "Q1d",
    "text": "Q1d\n\nProvide (1) seaborn code and (2) a simple comment to describe how the distribution of price_per_floz varies by brand."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#q1e",
    "href": "danl-cw/danl-m1-cw-4.html#q1e",
    "title": "Classwork 4",
    "section": "Q1e",
    "text": "Q1e\n\nProvide (1) seaborn code and (2) a simple comment to describe how the relationship between price_per_floz and beer_floz varies by brand."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html",
    "href": "danl-cw/danl-m1-cw-7.html",
    "title": "Classwork 7",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#load-libraries",
    "href": "danl-cw/danl-m1-cw-7.html#load-libraries",
    "title": "Classwork 7",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q1a",
    "href": "danl-cw/danl-m1-cw-7.html#q1a",
    "title": "Classwork 7",
    "section": "Q1a",
    "text": "Q1a\nAdd the new variables, closing_quarter and closing_year, to the DataFrame banks. - closing_quarter: the quarter in which the bank closed (1, 2, 3, or 4) - closing_year: the year in which the bank closed"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q1b",
    "href": "danl-cw/danl-m1-cw-7.html#q1b",
    "title": "Classwork 7",
    "section": "Q1b",
    "text": "Q1b\nCount the number of banks that were closed for each pair of year-quarter."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q1c",
    "href": "danl-cw/danl-m1-cw-7.html#q1c",
    "title": "Classwork 7",
    "section": "Q1c",
    "text": "Q1c\nProvide both seaborn code and a simple comment to describe the quarterly trend of bank failure."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q2a",
    "href": "danl-cw/danl-m1-cw-7.html#q2a",
    "title": "Classwork 7",
    "section": "Q2a",
    "text": "Q2a\nAdd a variable, date_dt, which is a datetime type of Date variable, to the stock DataFrame."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q2b",
    "href": "danl-cw/danl-m1-cw-7.html#q2b",
    "title": "Classwork 7",
    "section": "Q2b",
    "text": "Q2b\n\nFor each year, find the two dates, for which\n\nTSLA’s Close was the highest of the year.\nTSLA’s Close was the lowest of the year."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q2c",
    "href": "danl-cw/danl-m1-cw-7.html#q2c",
    "title": "Classwork 7",
    "section": "Q2c",
    "text": "Q2c\n\nCalculate the gap between the two adjacent dates with the highest Close of the year.\nCalculate the gap between the two adjacent dates with the lowest Close of the year."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html",
    "href": "danl-cw/danl-m1-cw-2.html",
    "title": "Classwork 2",
    "section": "",
    "text": "The following is the Python libraries we need for this homework.\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1a",
    "href": "danl-cw/danl-m1-cw-2.html#q1a",
    "title": "Classwork 2",
    "section": "Q1a",
    "text": "Q1a\n\nSort the DataFrame beer_markets by hh in ascending order.\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1b",
    "href": "danl-cw/danl-m1-cw-2.html#q1b",
    "title": "Classwork 2",
    "section": "Q1b",
    "text": "Q1b\nCount the number of households for each market. \nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1c",
    "href": "danl-cw/danl-m1-cw-2.html#q1c",
    "title": "Classwork 2",
    "section": "Q1c",
    "text": "Q1c\nFind the top 5 beer markets in terms of the number of households that purchased beer.\n\nAnswer\n\n\n\n\n\nmarket\nn\n\n\n\n\nTAMPA\n280\n\n\nDETROIT\n214\n\n\nCOLUMBUS\n184\n\n\nMIAMI\n178\n\n\nPHOENIX\n168"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1d",
    "href": "danl-cw/danl-m1-cw-2.html#q1d",
    "title": "Classwork 2",
    "section": "Q1d",
    "text": "Q1d\nSum of beer_floz for each market. \nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1e",
    "href": "danl-cw/danl-m1-cw-2.html#q1e",
    "title": "Classwork 2",
    "section": "Q1e",
    "text": "Q1e\nFind the top 5 beer markets in terms of the amount of total beer consumption. \nAnswer\n\n\n\n\n\nmarket\nbeer_floz\n\n\n\n\nTAMPA\n462904\n\n\nPHOENIX\n388824\n\n\nDETROIT\n309588\n\n\nMIAMI\n271870\n\n\nCOLUMBUS\n266896"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1f",
    "href": "danl-cw/danl-m1-cw-2.html#q1f",
    "title": "Classwork 2",
    "section": "Q1f",
    "text": "Q1f\n\nVariable price_per_floz is continuous.\nVariable brand is categorical.\n\nDescribe the distribution of price_per_floz for each brand using seaborn.\nMake a simple comment on comparison for the distribution of price_per_floz across brands. \nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBUD LIGHT, COORS LIGHT, and MILLER LITE have the similar distribution of price_per_floz with each other.\nBUSCH LIGHT and NATURAL LIGHT have the similar distribution of price_per_floz with each other.\nOverall, BUD LIGHT, COORS LIGHT, and MILLER LITE are more expensive than BUSCH LIGHT and NATURAL LIGHT."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1g",
    "href": "danl-cw/danl-m1-cw-2.html#q1g",
    "title": "Classwork 2",
    "section": "Q1g",
    "text": "Q1g\n\nBoth variables price_per_floz and beer_floz are continuous.\nDescribe the relationship between price_per_floz and beer_floz by brand using seaborn.\nMake a simple comment on the visualization result regarding how the relationship between price_per_floz and beer_floz varies by brand.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe law of demand holds in the beer market:\n\nFrom the fitted straight line, we see the downward-sloping demand curve for each beer brand.\n\nThe demand curves for BUD LIGHT, COORS LIGHT, and MILLER LITE are steeper than those for BUSCH LIGHT and NATURAL LIGHT.\nAccording to the demand curves, BUD LIGHT, COORS LIGHT, and MILLER LITE have larger demands than BUSCH LIGHT and NATURAL LIGHT given the same level of price_per_floz \\(\\geq\\) 0.0375."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html",
    "href": "danl-cw/danl-m1-cw-1.html",
    "title": "Classwork 1",
    "section": "",
    "text": "Question 1\n\nlist_variable = [100, 144, 169, 1000, 8]\n\nWrite a Python code that uses print() and max() functions to print out the largest value in the list, list_variable, as follows:\n\nThe largest value in the list is: 1000\n\n\nThe smallest value in the list is: 8\n\n\n\n\nQuestion 2\n\nfare = \"$10.00\"\ntip = \"2.00$\"\ntax = \"$ 0.80\"\n\nWrite a Python code that uses slicing and the print() function to print out the following message:\n\nThe total trip cost is: $12.80\n\n\n\n\nQuestion 3\n\nx = 4.0\ny = .5\n\nFor each expression below, what is the value of the expression? Explain thoroughly.\n\n20 == '20'\nbool(0) != bool('')\nx &lt; y or 3*y &lt; x\nnot (100 == '100' and 25 &lt; 36)\n\n\nThe statement 20 == '20' is False.\n\n\nThe left-hand side 20 is integer and the right-hand side '20' is string.\n\n\n\nThe statement bool(0) != bool('') is False.\n\n\nThe left-hand side bool(0) is False.\nThe right-hand side bool('') is False.\nThe statement False != False is False.\n\n\n\nThe statement x &lt; y or 3*y &lt; x is True.\n\n\nLet x &lt; y be the statement A. The statement A is False.\nLet 3*y &lt; x be the statement B. The statement B is True.\nThe statement A or B is True, because either A or B is True.\n\n\n\nThe statement not (100 == '100' and 25 &lt; 36) is True.\n\n\nLet 100 == '100' be the statement C.\nThe statement C is False, because the left-hand side 100 is integer and the right-hand side ‘100’ is string.\nLet 25 &lt; 36 be the statement D. The statement D is True.\nThe statement C and D is False, because either C or D is False.\nThe statement not (C and D) is True, because not (False) is not False, which is True.\n\n\n\n\nQuestion 4\nThis Python code creates a tuple with seven different ages:\n\nrespondent_ages = (65, 29, 25, 35, 58, 23, 19)\n\nWrite a Python code that uses a for-loop statement and a if-else statement to assign the list, under40_list, to [29, 25, 35, 23, 19] and the list, over40_list, to [65, 58].\n\n\n\nQuestion 5\n\ntuple_var = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\ntotal = 0\n\nWrite a Python code that uses the following two lines (1) a for-loop statement to process each element of tuple_var and (2) the following line.\n\nprint(\"The total of the values in the tuple is: \", total)\n\n\nWhen we have a list of things to loop through, we can construct a for loop.\n\nThe above for-loop is equivalent to the following Python code without for-loop.\n\n\n\n\n\nQuestion 6\nWrite a Python code that uses a while-loop and the print() function to print out the following message 5 times:\n\nProgramming for Data Analytics is so fun!\n\n\nStep 1. We first assigned the value 0 to i.\nStep 2. The while loop compared the value of i to 5 and continued if i was less than 5.\nStep 3. Inside the while loop, we printed Programming for Data Analytics is so fun! and then incremented i by 1 with the statement i += 1.\nStep 4. Python goes back to the top of the loop, and again compares i with 5.\nStep 5. The value of i is now 1, so the contents of the while loop are again executed, and i is incremented to 2.\nStep 6. This continues until i is incremented from 4 to 5 at the bottom of the loop.\nStep 7. On the next trip to the top, i &lt; 5 is now False, and the while loop ends.\n\n\n\n\n\n Back to topReuse© 2024 Byeong-Hak Choe | This post is licensed under &lt;a href='http://creativecommons.org/licenses/by-nc-sa/4.0/' target='_blank'&gt;CC BY-NC-SA 4.0&lt;/a&gt;.CitationFor attribution, please cite this work as:\nChoe, Byeong-Hak. 2024. “Classwork 1.” February 6, 2024."
  },
  {
    "objectID": "listing-danl-m1-lec.html",
    "href": "listing-danl-m1-lec.html",
    "title": "DANL 210 - Lecture",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\nLecture 7\n\n\nMarch 19, 2024\n\n\n\n\nLecture 6\n\n\nMarch 12, 2024\n\n\n\n\nLecture 5\n\n\nMarch 5, 2024\n\n\n\n\nLecture 4\n\n\nFebruary 27, 2024\n\n\n\n\nLecture 3\n\n\nFebruary 20, 2024\n\n\n\n\nLecture 2\n\n\nFebruary 13, 2024\n\n\n\n\nLecture 1\n\n\nFebruary 6, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-07.html",
    "href": "danl-qa/danl-m1-qa-07.html",
    "title": "Lecture 7 - Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-04.html",
    "href": "danl-qa/danl-m1-qa-04.html",
    "title": "Lecture 4 - Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-01.html",
    "href": "danl-qa/danl-m1-qa-01.html",
    "title": "Lecture 1 - Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-03.html",
    "href": "danl-qa/danl-m1-qa-03.html",
    "title": "Lecture 3 - Q & A",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html",
    "href": "danl-hw/danl-m1-hw-4.html",
    "title": "Homework Assignment 4",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1a",
    "href": "danl-hw/danl-m1-hw-4.html#q1a",
    "title": "Homework Assignment 4",
    "section": "Q1a",
    "text": "Q1a\nWhat are the minimum, first quartile, median, thrid quartile, maximum, mean, and standard deviation of Close and Volume for each company?"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1b",
    "href": "danl-hw/danl-m1-hw-4.html#q1b",
    "title": "Homework Assignment 4",
    "section": "Q1b",
    "text": "Q1b\nFind the 10 largest values for Volume. What are the companies and dates associated with those 10 largest values for Volume?"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1c",
    "href": "danl-hw/danl-m1-hw-4.html#q1c",
    "title": "Homework Assignment 4",
    "section": "Q1c",
    "text": "Q1c\nCalculate the Z-scores of Open and Close for each company using apply()."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1d",
    "href": "danl-hw/danl-m1-hw-4.html#q1d",
    "title": "Homework Assignment 4",
    "section": "Q1d",
    "text": "Q1d\nUse the transform() method on the stock data to represent all the values of Open, High, Low, Close, Adj Close, and Volume in terms of the first date in the data.\nTo do so, divide all values for each company by the values of the first date in the data for that company."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1e",
    "href": "danl-hw/danl-m1-hw-4.html#q1e",
    "title": "Homework Assignment 4",
    "section": "Q1e",
    "text": "Q1e\nProvide both seaborn code and a simple comment to describe the daily trend of normalized values of Close for each company in one plot. The normalized values of Close are the one calculated from Q1d."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1f",
    "href": "danl-hw/danl-m1-hw-4.html#q1f",
    "title": "Homework Assignment 4",
    "section": "Q1f",
    "text": "Q1f\nCreate a box plot of Close for each company in one plot. Make a simple comment on the plot."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q2a",
    "href": "danl-hw/danl-m1-hw-4.html#q2a",
    "title": "Homework Assignment 4",
    "section": "Q2a",
    "text": "Q2a\nHow many parties have provided or disbursed positive funding contributions to other countries or regions for their adaptation projects for every single year from 2011 to 2018?"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q2b",
    "href": "danl-hw/danl-m1-hw-4.html#q2b",
    "title": "Homework Assignment 4",
    "section": "Q2b",
    "text": "Q2b\nFor each party, calculate the total funding contributions that were disbursed or provided for mitigation projects for each year."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q2c",
    "href": "danl-hw/danl-m1-hw-4.html#q2c",
    "title": "Homework Assignment 4",
    "section": "Q2c",
    "text": "Q2c\nFor each party, calculate the ratio between adaptation contribution and mitigation contribution for each type of Status for each year."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q2d",
    "href": "danl-hw/danl-m1-hw-4.html#q2d",
    "title": "Homework Assignment 4",
    "section": "Q2d",
    "text": "Q2d\nProvide both seaborn code and a simple comment to describe the distribution of the ratio between adaptation contribution and mitigation contribution, which is calculated in Q2c."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q2e",
    "href": "danl-hw/danl-m1-hw-4.html#q2e",
    "title": "Homework Assignment 4",
    "section": "Q2e",
    "text": "Q2e\nProvide both seaborn code and a simple comment to describe how the distribution of Contribution varies by Type of support and Status."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-6.html",
    "href": "danl-hw/danl-m1-hw-6.html",
    "title": "Homework Assignment 6",
    "section": "",
    "text": "Direction\n\nWrite a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here.\n\n\nCiti Bike NYC is New York City’s official bike-sharing program. Residents and tourists can pick up and drop off bicycles at hundreds of locations around the city. Ride data is publicly available and released monthly by the city at https://www.citibikenyc.com/system-data. citibike.csv is a collection of ~1.9 million rides that cyclists took in June 2020. For simplicity’s sake, the data set has been modified from its original version and includes only two columns: each ride’s start time and end time. Let’s import the data set and assign it to a citi_bike variable:\n\nciti_bike = pd.read_csv(\"citibike.csv\")\n\n\n\nQuestion 1\nConvert the start_time and stop_time columns to store datetime (Timestamp) values instead of strings.\n\n\n\nQuestion 2\nCount the rides that occurred on each day of the week (Monday, Tuesday, and so on). Which weekday is the most popular for a bike ride? Use the start_time column as your starting point.\n\n\n\nQuestion 3\nCount the rides per week for each week within the month. To do so, round each date in the start_time column to its previous or current Monday. Assume that each week starts on a Monday and ends on a Sunday. Thus, the first week of June would be Monday, June 1 through Sunday, June 7.\n\n\n\nQuestion 4\nCalculate the duration of each ride, and save the results to a new duration column.\n\n\n\nQuestion 5\nFind the average duration of a bike ride.\n\n\n\nQuestion 6\nExtract the five longest bike rides by duration from the data set.\n\n\n\n\n\n\n\n Back to topReuse© 2024 Byeong-Hak Choe | This post is licensed under &lt;a href='http://creativecommons.org/licenses/by-nc-sa/4.0/' target='_blank'&gt;CC BY-NC-SA 4.0&lt;/a&gt;.CitationFor attribution, please cite this work as:\nChoe, Byeong-Hak. 2024. “Homework Assignment 6.” March 12,\n2024."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html",
    "href": "danl-hw/danl-m1-hw-3.html",
    "title": "Homework Assignment 3",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q1a",
    "href": "danl-hw/danl-m1-hw-3.html#q1a",
    "title": "Homework Assignment 3",
    "section": "Q1a",
    "text": "Q1a\n\nCalculate the simple difference between the probability of survival when passengers are first-class and the probability of survival when they are not."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q1b",
    "href": "danl-hw/danl-m1-hw-3.html#q1b",
    "title": "Homework Assignment 3",
    "section": "Q1b",
    "text": "Q1b\n\nHow much does the probability of survival increase for first-class passengers relative to those who are not first-class passengers?\nSDP tells us what would happen to the probability of survival if non-first-class passengers were first-class.\n\nIn other words, SDP means the effect of being the first-class on the probability of survival from the Titanic Disaster."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q1c",
    "href": "danl-hw/danl-m1-hw-3.html#q1c",
    "title": "Homework Assignment 3",
    "section": "Q1c",
    "text": "Q1c\n\nConsider the probability of survival in titanic_2.csv.\nAfter stratifying on gender and age, what happens to the difference in the probabilities of survival between first-class passengers and non-first-class passengers.\nExplain in your own words what stratifying on gender and age did for this difference in probabilities of survival between first-class passengers and non-first-class passengers.\nThe probability of survival for the first-class passengers can be different across gender and age groups.\n\nIn other words, the effect of being the first-class on the probability of survival from the Titanic Disaster can be different across genders and age groups.\n\nWSDP takes into account the difference in the effect of being first-class across gender and age groups by weighting.\nThe probability of survival for first-class after taking into account gender and age (WSDP) is less than that (SDP) which does presumably assume that characteristics of passengers such as gender and ages is related with the probability of survival."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2a",
    "href": "danl-hw/danl-m1-hw-3.html#q2a",
    "title": "Homework Assignment 3",
    "section": "Q2a",
    "text": "Q2a\n\nHow many players have been recorded?"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2b.",
    "href": "danl-hw/danl-m1-hw-3.html#q2b.",
    "title": "Homework Assignment 3",
    "section": "Q2b.",
    "text": "Q2b.\n\nA column points (“P”) is missing in the data. The number of points of a player is defined as the sum of his goals (“G”) and assists (“A”).\nAdd the point column “P” to your DataFrame."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2c.",
    "href": "danl-hw/danl-m1-hw-3.html#q2c.",
    "title": "Homework Assignment 3",
    "section": "Q2c.",
    "text": "Q2c.\n\nWho is the top scorer in terms of points?"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2d.",
    "href": "danl-hw/danl-m1-hw-3.html#q2d.",
    "title": "Homework Assignment 3",
    "section": "Q2d.",
    "text": "Q2d.\n\nHow many Russian (non-goalie) players had some ice time in there 2016/2017 regular season?\nHint: Nationality of a player can be found in “Nat”. Russians are indicated by “RUS”."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2e.",
    "href": "danl-hw/danl-m1-hw-3.html#q2e.",
    "title": "Homework Assignment 3",
    "section": "Q2e.",
    "text": "Q2e.\n\nWhat are their names?"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2f.",
    "href": "danl-hw/danl-m1-hw-3.html#q2f.",
    "title": "Homework Assignment 3",
    "section": "Q2f.",
    "text": "Q2f.\n\nWho performed best among the Russian players in terms of points (“P”)?"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2g.",
    "href": "danl-hw/danl-m1-hw-3.html#q2g.",
    "title": "Homework Assignment 3",
    "section": "Q2g.",
    "text": "Q2g.\n\nHow many points (“P”) did he have?\nThe above code creates a new DataFrame called q2g, which:\n\nFilters q2b for players whose nationality is ‘RUS’\nSorts the filtered DataFrame by the ‘P’ column in descending order\nSelects the top row of the sorted DataFrame using the .head(1) method\nFilters the columns ‘Last_Name’, ‘First_Name’, and ‘P’ from the selected row"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2h.",
    "href": "danl-hw/danl-m1-hw-3.html#q2h.",
    "title": "Homework Assignment 3",
    "section": "Q2h.",
    "text": "Q2h.\n\nHow well did he perform in the entire league? Put differently, what was his rank in terms of points?"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2i.",
    "href": "danl-hw/danl-m1-hw-3.html#q2i.",
    "title": "Homework Assignment 3",
    "section": "Q2i.",
    "text": "Q2i.\n\nFind the top ten scorers (in terms of points) and print them including their number of point and their respective team."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2j.",
    "href": "danl-hw/danl-m1-hw-3.html#q2j.",
    "title": "Homework Assignment 3",
    "section": "Q2j.",
    "text": "Q2j.\n\nWhat are the three countries with the most players originating from?"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q3a.",
    "href": "danl-hw/danl-m1-hw-3.html#q3a.",
    "title": "Homework Assignment 3",
    "section": "Q3a.",
    "text": "Q3a.\n\nFor each type of mine, calculate the total coal production for each pair of state-year."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q3b.",
    "href": "danl-hw/danl-m1-hw-3.html#q3b.",
    "title": "Homework Assignment 3",
    "section": "Q3b.",
    "text": "Q3b.\n\nFind the top 5 coal-producing states for each year."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q3c.",
    "href": "danl-hw/danl-m1-hw-3.html#q3c.",
    "title": "Homework Assignment 3",
    "section": "Q3c.",
    "text": "Q3c.\n\nVisualize the yearly trend of the total coal production from each type of mine."
  },
  {
    "objectID": "listing-danl-m1-hw.html",
    "href": "listing-danl-m1-hw.html",
    "title": "DANL 210 - Homework",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nHomework Assignment 6\n\n\nMerging, Joining, and Concaternating DataFrames\n\n\nMarch 12, 2024\n\n\n\n\nHomework Assignment 5\n\n\nReshaping and Pivoting DataFrames\n\n\nMarch 5, 2024\n\n\n\n\nHomework Assignment 4\n\n\nGroup Operations with GroupBy Objects\n\n\nFebruary 27, 2024\n\n\n\n\nHomework Assignment 3\n\n\nFiltering, Sorting, Ranking, and Visualizing DataFrames\n\n\nFebruary 20, 2024\n\n\n\n\nHomework Assignment 2\n\n\nPandas Basics\n\n\nFebruary 13, 2024\n\n\n\n\nHomework Assignment 1\n\n\nPython Basics\n\n\nFebruary 6, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]