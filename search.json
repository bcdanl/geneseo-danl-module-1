[
  {
    "objectID": "listing-danl-m1-qa.html",
    "href": "listing-danl-m1-qa.html",
    "title": "DANL Module 1 - Lecture Discussion",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\nWeek 1 - Q & A\n\n\nFebruary 6, 2024\n\n\n\n\nWeek 2 - Q & A\n\n\nFebruary 13, 2024\n\n\n\n\nWeek 3 - Q & A\n\n\nFebruary 20, 2024\n\n\n\n\nWeek 4 - Q & A\n\n\nFebruary 27, 2024\n\n\n\n\nWeek 5 - Q & A\n\n\nMarch 5, 2024\n\n\n\n\nWeek 6 - Q & A\n\n\nMarch 12, 2024\n\n\n\n\nWeek 7 - Q & A\n\n\nMarch 19, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-1.html",
    "href": "danl-hw/danl-m1-hw-1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Direction\n\nWrite a Python Notebook (*.ipynb) to answer each question.\nMake at least some simple comment (# ...) in each question.\nMake one text cell to explain things in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files.\n\n\n\n\nQuestion 1\nHow do you assign the value 5 to a variable named x?\nAnswer\n\n\n\nQuestion 2\nGiven lst = [1, 2, 3, 4, 5], what does lst[1:4] return?\nAnswer\n\n\n\nQuestion 3\n\nHow do you create a dictionary with keys \"apple\", \"banana\", and \"cherry\", each with the corresponding values 5, 3, and 7?\nHow do you access the value associated with the key \"banana\" in the dictionary created above?\n\nAnswer\n\n\n\nQuestion 4\nFor the expressions below, what is the value of the expression? Explain thoroughly.\n\nTrue and False\n\n\nTrue or False\n\n\nnot (100 == '100' and 25 &lt; 36)\n\nAnswer\n\n\n\nQuestion 5\n\nCreate a new if-elif-else chain that prints \"well done\" if a score is over 90, “good” if between 40 and 90, and \"bad luck\" otherwise.\n\nAnswer\n\n\n\nQuestion 6\nWrite a for loop that prints out \"coding for data analysts\" so that each word is printed in a successive iteration.\nAnswer\n\n\n\nQuestion 7\n\nHow do you import the math module in a Python script?\nAfter importing math, how do you use its sqrt function to find the square root of 16?\n\nAnswer\n\n\n\n\n\n\n Back to topReuse© 2024 Byeong-Hak Choe | This post is licensed under &lt;a href='http://creativecommons.org/licenses/by-nc-sa/4.0/' target='_blank'&gt;CC BY-NC-SA 4.0&lt;/a&gt;.CitationFor attribution, please cite this work as:\nChoe, Byeong-Hak. 2024. “Homework 1.” February 6, 2024."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html",
    "href": "danl-hw/danl-m1-hw-2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1a",
    "href": "danl-hw/danl-m1-hw-2.html#q1a",
    "title": "Homework 2",
    "section": "Q1a",
    "text": "Q1a\nWrite a Python code that uses the list, a, to create the following Numpy Array:\n\narray([0.1, 1.2, 2.3, 3.4, 4.5])\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1b",
    "href": "danl-hw/danl-m1-hw-2.html#q1b",
    "title": "Homework 2",
    "section": "Q1b",
    "text": "Q1b\nWrite a Python code that uses the list, a, to create the following pandas Series:\n\n########################\n# Index      0\n\n# 0         0.1\n# 1         1.2\n# 2         2.3\n# 3         3.4\n# 4         4.5\n\n# dtype: float64    \n########################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1c",
    "href": "danl-hw/danl-m1-hw-2.html#q1c",
    "title": "Homework 2",
    "section": "Q1c",
    "text": "Q1c\nWrite a Python code that uses the list, a, to create the following pandas Series:\n\n########################\n# Index      0\n\n# a         0.1\n# b         1.2\n# c         2.3\n# d         3.4\n# e         4.5\n\n# dtype: float64    \n########################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q1d",
    "href": "danl-hw/danl-m1-hw-2.html#q1d",
    "title": "Homework 2",
    "section": "Q1d",
    "text": "Q1d\nWrite a Python code that uses (1) the Series created in Q1c and (2) Boolean indexing to get the following Series:\n\n########################\n# Index      0\n\n# c         2.3\n# d         3.4\n# e         4.5\n\n# dtype: float64    \n########################  \n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q2a",
    "href": "danl-hw/danl-m1-hw-2.html#q2a",
    "title": "Homework 2",
    "section": "Q2a",
    "text": "Q2a\nWrite a Python code that uses the list, hh_income, to assign the object, hh_income_array, to the following Numpy array:\n\n############################\n\n# array([[    10,  14629],\n#        [    20,  25600],\n#        [    30,  37002],\n#        [    40,  50000],\n#        [    50,  63179],\n#        [    60,  79542],\n#        [    70, 100162],\n#        [    80, 130000],\n#        [    90, 184292]])\n\n############################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q2b",
    "href": "danl-hw/danl-m1-hw-2.html#q2b",
    "title": "Homework 2",
    "section": "Q2b",
    "text": "Q2b\nWrite a Python code that uses the print() function to report the dimensions of the ndarray and the number of elements in hh_income_array as follows:\n\nDimensions of the NumPy array, hh_income_array, is: (9, 2)\n\n\nNumber of elements in the NumPy array, hh_income_array, is: 18\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q3a.",
    "href": "danl-hw/danl-m1-hw-2.html#q3a.",
    "title": "Homework 2",
    "section": "Q3a.",
    "text": "Q3a.\nWrite a Python code that uses the NumPy array, c, to create the following DataFrame:\n\n############################\n\n# index     0    1\n# 0         1.0  2.0\n# 1         3.0  4.0\n\n############################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q3b.",
    "href": "danl-hw/danl-m1-hw-2.html#q3b.",
    "title": "Homework 2",
    "section": "Q3b.",
    "text": "Q3b.\nWrite a Python code that uses the NumPy array, c, to create the following DataFrame:\n\n############################\n\n# index     dogs    cats\n# 0         1.0     2.0\n# 1         3.0     4.0\n\n############################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q3c.",
    "href": "danl-hw/danl-m1-hw-2.html#q3c.",
    "title": "Homework 2",
    "section": "Q3c.",
    "text": "Q3c.\nWrite a Python code that uses the NumPy array, c, to create the following DataFrame:\n\n############################\n\n# index             dogs    cats\n# byeong-hak        1.0     2.0\n# your_first_name   3.0     4.0\n\n############################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4a",
    "href": "danl-hw/danl-m1-hw-2.html#q4a",
    "title": "Homework 2",
    "section": "Q4a",
    "text": "Q4a\nRead the data file, US_state_GDP.csv, as the object name, state_gdp, using (1) path_csv and (2) pd.read_csv() function.\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4b",
    "href": "danl-hw/danl-m1-hw-2.html#q4b",
    "title": "Homework 2",
    "section": "Q4b",
    "text": "Q4b\nWrite a Python code that uses the DataFrame, state_gdp, to create the DataFrame, whose first five rows are as follows:\n\n############################################\n\n# index    state_code           state\n\n# 0          AK                Alaska\n# 1          AL               Alabama\n# 2          AR              Arkansas\n# 3          AZ               Arizona\n# 4          CA            California\n\n############################################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4c",
    "href": "danl-hw/danl-m1-hw-2.html#q4c",
    "title": "Homework 2",
    "section": "Q4c",
    "text": "Q4c\nWrite a Python code that uses (1) the DataFrame, state_gdp, and (2) state_gdp.columns to create the DataFrame, whose first five rows are as follows:\n\n############################################\n\n#                    state  gdp_2009\n\n# 0                 Alaska     44215\n# 1                Alabama    149843\n# 2               Arkansas     89776\n# 3                Arizona    221405\n# 4             California   1667152\n\n############################################\n\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4d",
    "href": "danl-hw/danl-m1-hw-2.html#q4d",
    "title": "Homework 2",
    "section": "Q4d",
    "text": "Q4d\nWrite a Python code to get the first three rows of the DataFrame, state_gdp:\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4e",
    "href": "danl-hw/danl-m1-hw-2.html#q4e",
    "title": "Homework 2",
    "section": "Q4e",
    "text": "Q4e\nWrite a Python code to get all the rows of the DataFrame, state_gdp, for which the value of gdp_growth_2010 is less than 0\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4f",
    "href": "danl-hw/danl-m1-hw-2.html#q4f",
    "title": "Homework 2",
    "section": "Q4f",
    "text": "Q4f\nWrite a Python code that uses state_gdp.loc[] to get the following DataFrame:\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-2.html#q4g",
    "href": "danl-hw/danl-m1-hw-2.html#q4g",
    "title": "Homework 2",
    "section": "Q4g",
    "text": "Q4g\nWrite a Python code that uses state_gdp.iloc[] to get the following DataFrame:\n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-exam.html",
    "href": "danl-hw/danl-m1-exam.html",
    "title": "Take-home Exam",
    "section": "",
    "text": "Direction\n\nWrite a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here.\n\n\n\n\n\n\n Back to topReuse© 2024 Byeong-Hak Choe | This post is licensed under &lt;a href='http://creativecommons.org/licenses/by-nc-sa/4.0/' target='_blank'&gt;CC BY-NC-SA 4.0&lt;/a&gt;.CitationFor attribution, please cite this work as:\nChoe, Byeong-Hak. 2024. “Take-Home Exam.” March 19, 2024."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html",
    "href": "danl-hw/danl-m1-hw-4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1a",
    "href": "danl-hw/danl-m1-hw-4.html#q1a",
    "title": "Homework 4",
    "section": "Q1a",
    "text": "Q1a\nWhat are the minimum, first quartile, median, thrid quartile, maximum, mean, and standard deviation of Close and Volume for each company? \nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1b",
    "href": "danl-hw/danl-m1-hw-4.html#q1b",
    "title": "Homework 4",
    "section": "Q1b",
    "text": "Q1b\nFind the 10 largest values for Volume. What are the companies and dates associated with those 10 largest values for Volume? \nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1c",
    "href": "danl-hw/danl-m1-hw-4.html#q1c",
    "title": "Homework 4",
    "section": "Q1c",
    "text": "Q1c\nCalculate the Z-scores of Open and Close for each company using apply(). \nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1d",
    "href": "danl-hw/danl-m1-hw-4.html#q1d",
    "title": "Homework 4",
    "section": "Q1d",
    "text": "Q1d\nUse the transform() method on the stock data to represent all the values of Open, High, Low, Close, Adj Close, and Volume in terms of the first date in the data.\nTo do so, divide all values for each company by the values of the first date in the data for that company. \nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1e",
    "href": "danl-hw/danl-m1-hw-4.html#q1e",
    "title": "Homework 4",
    "section": "Q1e",
    "text": "Q1e\nProvide both seaborn code and a simple comment to describe the daily trend of normalized values of Close for each company in one plot. The normalized values of Close are the one calculated from Q1d. \nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q1f",
    "href": "danl-hw/danl-m1-hw-4.html#q1f",
    "title": "Homework 4",
    "section": "Q1f",
    "text": "Q1f\nCreate a box plot of Close for each company in one plot. Make a simple comment on the plot. \nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q2a",
    "href": "danl-hw/danl-m1-hw-4.html#q2a",
    "title": "Homework 4",
    "section": "Q2a",
    "text": "Q2a\nHow many parties have provided or disbursed positive funding contributions to other countries or regions for their adaptation projects for every single year from 2011 to 2018? \nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q2b",
    "href": "danl-hw/danl-m1-hw-4.html#q2b",
    "title": "Homework 4",
    "section": "Q2b",
    "text": "Q2b\nFor each party, calculate the total funding contributions that were disbursed or provided for mitigation projects for each year. \nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q2c",
    "href": "danl-hw/danl-m1-hw-4.html#q2c",
    "title": "Homework 4",
    "section": "Q2c",
    "text": "Q2c\nFor each party, calculate the ratio between adaptation contribution and mitigation contribution for each type of Status for each year. \nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q2d",
    "href": "danl-hw/danl-m1-hw-4.html#q2d",
    "title": "Homework 4",
    "section": "Q2d",
    "text": "Q2d\nProvide both seaborn code and a simple comment to describe the distribution of the ratio between adaptation contribution and mitigation contribution, which is calculated in Q2c. \nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-4.html#q2e",
    "href": "danl-hw/danl-m1-hw-4.html#q2e",
    "title": "Homework 4",
    "section": "Q2e",
    "text": "Q2e\nProvide both seaborn code and a simple comment to describe how the distribution of Contribution varies by Type of support and Status. \nAnswer"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-03.html",
    "href": "danl-qa/danl-m1-qa-03.html",
    "title": "Week 3 - Q & A",
    "section": "",
    "text": "Welcome to our Week 3 Discussion Board! 👋 \nThis space is designed for you to engage with your classmates about the material covered in Week 3, including Lecture 3 slides, Classwork 3, and Homework Assignment 3.\nWhether you are looking to delve deeper into the slides, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Week 3 materials or need clarification on any points, don’t hesitate to ask here.\nLet’s collaborate and learn from each other!\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-01.html",
    "href": "danl-qa/danl-m1-qa-01.html",
    "title": "Week 1 - Q & A",
    "section": "",
    "text": "Welcome to our Week 1 Discussion Board! 👋 \nThis space is designed for you to engage with your classmates about the material covered in Week 1, including Lecture 1 slides, Classwork 1, and Homework Assignment 1.\nWhether you are looking to delve deeper into the slides, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Week 1 materials or need clarification on any points, don’t hesitate to ask here.\nLet’s collaborate and learn from each other!\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-04.html",
    "href": "danl-qa/danl-m1-qa-04.html",
    "title": "Week 4 - Q & A",
    "section": "",
    "text": "Welcome to our Week 4 Discussion Board! 👋 \nThis space is designed for you to engage with your classmates about the material covered in Week 4, including Lecture 4 slides, Classwork 4, and Homework Assignment 1.\nWhether you are looking to delve deeper into the slides, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Week 4 materials or need clarification on any points, don’t hesitate to ask here.\nLet’s collaborate and learn from each other!\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-07.html",
    "href": "danl-qa/danl-m1-qa-07.html",
    "title": "Week 7 - Q & A",
    "section": "",
    "text": "Welcome to our Week 7 Discussion Board! 👋 \nThis space is designed for you to engage with your classmates about the material covered in Week 7, including Lecture 7 slides and Classwork 7.\nWhether you are looking to delve deeper into the slides, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Week 7 materials or need clarification on any points, don’t hesitate to ask here.\nLet’s collaborate and learn from each other!\n\n\n\n Back to top"
  },
  {
    "objectID": "listing-danl-m1-lec.html",
    "href": "listing-danl-m1-lec.html",
    "title": "DANL Module 1 - Lecture",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nLecture 1\n\n\nSyllabus, Course Outline, Python Basics\n\n\nFebruary 6, 2024\n\n\n\n\nLecture 2\n\n\nPandas basics [Lecture 2 slides may be subject to change during the Module.]\n\n\nFebruary 13, 2024\n\n\n\n\nLecture 3\n\n\nPandas Data Structures [Lecture 3 slides may be subject to change during the Module.]\n\n\nFebruary 20, 2024\n\n\n\n\nLecture 4\n\n\nFiltering, Sorting, Ranking, and Visualization [Lecture 4 slides may be subject to change during the Module.]\n\n\nFebruary 27, 2024\n\n\n\n\nLecture 5\n\n\nGroup Opeations [Lecture 5 slides may be subject to change during the Module.]\n\n\nMarch 5, 2024\n\n\n\n\nLecture 6\n\n\nTidy Data [Lecture 6 slides may be subject to change during the Module.]\n\n\nMarch 12, 2024\n\n\n\n\nLecture 7\n\n\nMissing Data and Time-series Data [Lecture 7 slides may be subject to change during the Module.]\n\n\nMarch 19, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html",
    "href": "danl-cw/danl-m1-cw-1.html",
    "title": "Classwork 1",
    "section": "",
    "text": "Using Python operations only, calculate below: \\[\\frac{2^5}{7 \\cdot (4 - 2^3)}\\]\nAnswer\n\n\n\n\nFor each expression below, what is the value of the expression? Explain thoroughly.\n\n20 == '20'\n\n\nx = 4.0\ny = .5\n\nx &lt; y or 3*y &lt; x\n\nAnswer\n\n\n\n\n\nfare = \"$10.00\"\ntip = \"2.00$\"\ntax = \"$ 0.80\"\n\nWrite a Python code that uses slicing and the print() function to print out the following message:\n\nThe total trip cost is: $12.80\n\n\nAnswer\n\n\n\n\n\nlist_variable = [100, 144, 169, 1000, 8]\n\nWrite a Python code that uses print() and max() functions to print out the largest value in the list, list_variable, as follows:\n\nThe largest value in the list is: 1000\n\n\nAnswer\n\n\n\n\nImport the pandas library as pd\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html#question-1",
    "href": "danl-cw/danl-m1-cw-1.html#question-1",
    "title": "Classwork 1",
    "section": "",
    "text": "Using Python operations only, calculate below: \\[\\frac{2^5}{7 \\cdot (4 - 2^3)}\\]\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html#question-2",
    "href": "danl-cw/danl-m1-cw-1.html#question-2",
    "title": "Classwork 1",
    "section": "",
    "text": "For each expression below, what is the value of the expression? Explain thoroughly.\n\n20 == '20'\n\n\nx = 4.0\ny = .5\n\nx &lt; y or 3*y &lt; x\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html#question-3",
    "href": "danl-cw/danl-m1-cw-1.html#question-3",
    "title": "Classwork 1",
    "section": "",
    "text": "fare = \"$10.00\"\ntip = \"2.00$\"\ntax = \"$ 0.80\"\n\nWrite a Python code that uses slicing and the print() function to print out the following message:\n\nThe total trip cost is: $12.80\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html#question-4",
    "href": "danl-cw/danl-m1-cw-1.html#question-4",
    "title": "Classwork 1",
    "section": "",
    "text": "list_variable = [100, 144, 169, 1000, 8]\n\nWrite a Python code that uses print() and max() functions to print out the largest value in the list, list_variable, as follows:\n\nThe largest value in the list is: 1000\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html#question-5",
    "href": "danl-cw/danl-m1-cw-1.html#question-5",
    "title": "Classwork 1",
    "section": "",
    "text": "Import the pandas library as pd\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html#basic-syntax",
    "href": "danl-cw/danl-m1-cw-1.html#basic-syntax",
    "title": "Classwork 1",
    "section": "Basic Syntax",
    "text": "Basic Syntax\n\nHeadings\nHeadings are created by adding one or more # symbols before your heading text. The number of # symbols indicates the level of the heading.\n# Heading 1\n## Heading 2\n### Heading 3\n\n\nEmphasis\nYou can make text bold by wrapping it with two asterisks **, and italic by using one asterisk *.\n*italic* or _italic_\n**bold** or __bold__\n\n\nLists\nUnordered lists are created using *, -, or +, while ordered lists are numbered.\n- Item 1\n- Item 2\n  - Subitem 2.1\n  - Subitem 2.2\n1. First item\n2. Second item\n\n\nLinks and Images\nLinks are created using [Link Text](URL), and images with ![Alt Text](Image URL).\n[DANL 210](https://bcdanl.github.io/210)\n![Geneseo Logo](https://bcdanl.github.io/img/geneseo-logo.gif)"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html#advanced-syntax",
    "href": "danl-cw/danl-m1-cw-1.html#advanced-syntax",
    "title": "Classwork 1",
    "section": "Advanced Syntax",
    "text": "Advanced Syntax\n\nBlockquote\n&gt; Be yourself. Everyone else is already taken. - Oscar Wilde.\n\n\nEmojis\n\nA ton of markdown emojis are available here 😄 ( :smile: )\n\nhttps://github.com/ikatyang/emoji-cheat-sheet\n\n\n\n\nCode Blocks\nCode blocks are created by using triple backticks (```). Optionally, you can specify the language for syntax highlighting.\n```\n\"string\"\n```\n```python\n# Python code block\nimport numpy as np\n```"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html#question-6",
    "href": "danl-cw/danl-m1-cw-1.html#question-6",
    "title": "Classwork 1",
    "section": "Question 6",
    "text": "Question 6\n\nDo the following tasks on this Classwork 1 Discussion Board:\n\nBasic Syntax: Write a comment with your name, a heading, an unordered list, an ordered list, a link, and an image.\nAdvanced Syntax: Write a comment that includes a Python code block, a blockquote, and an emoji."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-1.html#references",
    "href": "danl-cw/danl-m1-cw-1.html#references",
    "title": "Classwork 1",
    "section": "References",
    "text": "References\n\nQuarto Markdown Basics\nStart writing on GitHub"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html",
    "href": "danl-cw/danl-m1-cw-2.html",
    "title": "Classwork 2",
    "section": "",
    "text": "The following is the Python libraries we need for this homework.\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1a",
    "href": "danl-cw/danl-m1-cw-2.html#q1a",
    "title": "Classwork 2",
    "section": "Q1a",
    "text": "Q1a\n\nSort the DataFrame beer_markets by hh in ascending order.\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1b",
    "href": "danl-cw/danl-m1-cw-2.html#q1b",
    "title": "Classwork 2",
    "section": "Q1b",
    "text": "Q1b\nCount the number of households for each market. \nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1c",
    "href": "danl-cw/danl-m1-cw-2.html#q1c",
    "title": "Classwork 2",
    "section": "Q1c",
    "text": "Q1c\nFind the top 5 beer markets in terms of the number of households that purchased beer.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1d",
    "href": "danl-cw/danl-m1-cw-2.html#q1d",
    "title": "Classwork 2",
    "section": "Q1d",
    "text": "Q1d\nSum of beer_floz for each market. \nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1e",
    "href": "danl-cw/danl-m1-cw-2.html#q1e",
    "title": "Classwork 2",
    "section": "Q1e",
    "text": "Q1e\nFind the top 5 beer markets in terms of the amount of total beer consumption. \nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1f",
    "href": "danl-cw/danl-m1-cw-2.html#q1f",
    "title": "Classwork 2",
    "section": "Q1f",
    "text": "Q1f\n\nVariable price_per_floz is continuous.\nVariable brand is categorical.\n\nDescribe the distribution of price_per_floz for each brand using seaborn.\nMake a simple comment on comparison for the distribution of price_per_floz across brands. \nAnswer\n\n\nBUD LIGHT, COORS LIGHT, and MILLER LITE have the similar distribution of price_per_floz with each other.\nBUSCH LIGHT and NATURAL LIGHT have the similar distribution of price_per_floz with each other.\nOverall, BUD LIGHT, COORS LIGHT, and MILLER LITE are more expensive than BUSCH LIGHT and NATURAL LIGHT."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-2.html#q1g",
    "href": "danl-cw/danl-m1-cw-2.html#q1g",
    "title": "Classwork 2",
    "section": "Q1g",
    "text": "Q1g\n\nBoth variables price_per_floz and beer_floz are continuous.\nDescribe the relationship between price_per_floz and beer_floz by brand using seaborn.\nMake a simple comment on the visualization result regarding how the relationship between price_per_floz and beer_floz varies by brand. \n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html",
    "href": "danl-cw/danl-m1-cw-7.html",
    "title": "Classwork 7",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#load-libraries",
    "href": "danl-cw/danl-m1-cw-7.html#load-libraries",
    "title": "Classwork 7",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q1a",
    "href": "danl-cw/danl-m1-cw-7.html#q1a",
    "title": "Classwork 7",
    "section": "Q1a",
    "text": "Q1a\nAdd the new variables, closing_quarter and closing_year, to the DataFrame banks. - closing_quarter: the quarter in which the bank closed (1, 2, 3, or 4) - closing_year: the year in which the bank closed\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q1b",
    "href": "danl-cw/danl-m1-cw-7.html#q1b",
    "title": "Classwork 7",
    "section": "Q1b",
    "text": "Q1b\nCount the number of banks that were closed for each pair of year-quarter.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q1c",
    "href": "danl-cw/danl-m1-cw-7.html#q1c",
    "title": "Classwork 7",
    "section": "Q1c",
    "text": "Q1c\nProvide both seaborn code and a simple comment to describe the quarterly trend of bank failure.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q2a",
    "href": "danl-cw/danl-m1-cw-7.html#q2a",
    "title": "Classwork 7",
    "section": "Q2a",
    "text": "Q2a\nAdd a variable, date_dt, which is a datetime type of Date variable, to the stock DataFrame.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q2b",
    "href": "danl-cw/danl-m1-cw-7.html#q2b",
    "title": "Classwork 7",
    "section": "Q2b",
    "text": "Q2b\n\nFor each year, find the two dates, for which\n\nTSLA’s Close was the highest of the year.\nTSLA’s Close was the lowest of the year.\n\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-7.html#q2c",
    "href": "danl-cw/danl-m1-cw-7.html#q2c",
    "title": "Classwork 7",
    "section": "Q2c",
    "text": "Q2c\n\nCalculate the gap between the two adjacent dates with the highest Close of the year.\nCalculate the gap between the two adjacent dates with the lowest Close of the year.\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html",
    "href": "danl-cw/danl-m1-cw-4.html",
    "title": "Classwork 4",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#load-libraries",
    "href": "danl-cw/danl-m1-cw-4.html#load-libraries",
    "title": "Classwork 4",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#load-dataframe",
    "href": "danl-cw/danl-m1-cw-4.html#load-dataframe",
    "title": "Classwork 4",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbeer_mkt = pd.read_csv('https://bcdanl.github.io/data/beer_markets.csv')\nbeer_mkt.head(10)\n\n\nVariable Description\n\nhh: An identifier of the purchasing household;\n_purchase_desc: Details on the purchased item;\nquantity: Number of items purchased;\nbrand: BUD LIGHT, BUSCH LIGHT, COORS LIGHT, MILLER LITE, or NATURAL LIGHT;\nspent: Total dollar value of purchase;\nbeer_floz: Total volume of beer, in fluid ounces;\nprice_per_floz: Price per fl.oz. (i.e., spent/beer_floz);\ncontainer: Type of container;\npromo: Whether the item was promoted (coupon or something else);\nmarket: Scan-track market (or state if rural);\nvarious demographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher."
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#q1a",
    "href": "danl-cw/danl-m1-cw-4.html#q1a",
    "title": "Classwork 4",
    "section": "Q1a",
    "text": "Q1a\n\nSort the DataFrame beer_mkt by hh in ascending order. \n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#q1b",
    "href": "danl-cw/danl-m1-cw-4.html#q1b",
    "title": "Classwork 4",
    "section": "Q1b",
    "text": "Q1b\n\nFind the top 5 beer markets in terms of the number of households that purchased beer. \n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#q1c",
    "href": "danl-cw/danl-m1-cw-4.html#q1c",
    "title": "Classwork 4",
    "section": "Q1c",
    "text": "Q1c\n\nFind the top 5 beer markets in terms of the amount of total beer consumption. \n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#q1d",
    "href": "danl-cw/danl-m1-cw-4.html#q1d",
    "title": "Classwork 4",
    "section": "Q1d",
    "text": "Q1d\n\nProvide (1) seaborn code and (2) a simple comment to describe how the distribution of price_per_floz varies by brand. \n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-4.html#q1e",
    "href": "danl-cw/danl-m1-cw-4.html#q1e",
    "title": "Classwork 4",
    "section": "Q1e",
    "text": "Q1e\n\nProvide (1) seaborn code and (2) a simple comment to describe how the relationship between price_per_floz and beer_floz varies by brand. \n\nAnswer"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-1",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-1",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nBuilt-in Aggregation Methods\n\n(1)(2)(3)(4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know we can calculate multiple summary statistics simultaneously with .describe().\n## group by continent and describe each group\ncontinent_describe = df.groupby('continent')[\"lifeExp\"].describe()\ncontinent_describe"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-2",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-2",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nAggregation Functions\nFunctions From Other Libraries\nWe can also use other libraries’ functions that are not listed in the previous tables. (e.g., numpy, scipy)\n## calculate the average life expectancy by continent\n## but use the np.mean function\ncont_le_agg = df.groupby('continent')[\"lifeExp\"].agg(np.mean)\nQ. Add a new variable, the log of lifeExp, using np.log to DataFrame df."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-3",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-3",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nAggregation Functions\n\n(1)(2)(3)(4)(5)\n\n\nSometimes we may want to perform a calculation that is not provided by Pandas or another library.\n\nLet’s create our own mean function with def.\n\ndef my_mean(values):\n  n = len(values)    ## get the total number of numbers for the denominator\n  sum = 0   ## start the sum at 0\n  for value in values:\n      sum += value   ## add each value to the running sum\n  return sum / n   ## return the summed values divided by the number of values\n\n\nWe can pass our custom function straight into the .agg() method with my_mean.\n## use our custom function into agg\nagg_my_mean = df.groupby('year')[\"lifeExp\"].agg(my_mean)\n\nagg_my_mean\n\n\nWe can write functions that take multiple parameters.\ndef my_mean_diff(values, diff_value):\n    \"\"\"Difference between the mean and diff_value\n    \"\"\"\n    n = len(values)\n    sum = 0\n    for value in values:\n        sum += value\n    mean = sum / n\n    return(mean - diff_value)\n\n\nUsing my_mean_diff, we will calculate the global average life expectancy, diff_value, and subtract it from each grouped value.\n## custom aggregation function with multiple parameters\nagg_mean_diff = (\n  df\n  .groupby(\"year\")[\"lifeExp\"]\n  .agg(my_mean_diff, diff_value = global_mean)\n)\n\n\nWhen we want to calculate multiple functions, we can pass the multiple functions into .agg()\n\n## calculate the count, mean, std of the lifeExp by continent\ngdf = (\n  df\n  .groupby(\"year\")\n  [\"lifeExp\"]\n  .agg([np.count_nonzero, np.mean, np.std])\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-4",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-4",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nUse a dict in .agg() on a Series\nWe can also pass .agg() a Python dictionary ({key : value}). - The results will differ depending on whether we are aggregating directly on a DataFrame or on a Series object. - When specifying a dict on a grouped DataFrame, the keys are the columns of the DataFrame, and the values are the functions used in the aggregated calculation.\ngdf_dict = df.groupby(\"year\").agg(\n  {\"lifeExp\": \"mean\",\n    \"pop\": \"median\",\n    \"gdpPercap\": \"median\"})"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-5",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-5",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nUse a dict in .agg() on a DataFrame\nTo have user-defined column names in the output of a grouped series calculation, we need to rename those columns.\n\n\ngdf = (\n  df\n  .groupby(\"year\")\n  [\"lifeExp\"]\n  .agg(\n    [\n      np.count_nonzero,\n      np.mean,\n      np.std,\n    ]\n  )\n\n  .rename(\n    columns={\n      \"count_nonzero\": \"count\",\n      \"mean\": \"avg\",\n      \"std\": \"std_dev\",\n    }\n  )\n  .reset_index() ## return a flat dataframe\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-6",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-6",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform\nWhen we transform data, we pass values from our dataframe into a function.\n\nThe function then “transforms” the data.\nUnlike .agg(), which can take multiple values and return a single (aggregated) value, .transform() takes multiple values and returns a one-to-one transformation of the values.\nThat is, it does not reduce the amount of data."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-7",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-7",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\n\nLet’s calculate the z-score of our life expectancy data by year. \\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\n\\(x\\) is a value in a variable\n\\(\\mu\\) is the average of a variable\n\\(\\sigma\\) is the standard deviation of a variable\n\n\\[\n\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n}(x - \\mu)^{2}}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-8",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-8",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\nThe following is a function, my_zscore(x), that calculates a z-score using Pandas methods.\ndef my_zscore(x):\n  '''Calculates the z-score of provided data\n  'x' is a vector or series of values\n  '''\n  return((x - x.mean()) / x.std())"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-9",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-9",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\nNow we can use this function to .transform() our data by group.\ntransform_z = df.groupby('year')[\"lifeExp\"].transform(my_zscore)\n\nNote that both df and transform_z have the same number of rows and data.\n\ndf.shape\ntransform_z.shape"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-10",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-10",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nTransform (cont.)\nThe scipy library has its own zscore() function.\nLet’s use its zscore() function in a .groupby() .transform() and compare it to what happens when we do not use .groupby().\nfrom scipy.stats import zscore\n## calculate a grouped zscore\nsp_z_grouped = df.groupby('year')[\"lifeExp\"].transform(zscore)\n\n## calculate a nongrouped zscore\nsp_z_nogroup = zscore(df[\"lifeExp\"])\n\ntransform_z.head()\nsp_z_grouped.head()\nsp_z_nogroup[:5]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-11",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-11",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nagg, transform, and apply: when to use each with a groupby\nWith all of the different options available, it can be confusing to know when to use the different functions available for performing groupby operations, namely: .agg, .transform, and .apply.\nHere are the key points to remember: - Use .agg when using a groupby, but you want your groups to become the new index. - Use .transform when using a groupby, but you want to retain your original index. - Use .apply when using a groupby, but you want to perform operations that will leave neither the original index nor an index of groups."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-12",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-12",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\n.filter()\n.filter() allows us to split our data by keys, and then perform some kind of boolean subsetting on the data.\n\nLet’s work with the tips data set from seaborn:\n\nimport seaborn as sns\ntips = sns.load_dataset('tips')\n\n## note the number of rows in the original data\ntips.shape\n\n\n## look at the frequency counts for the table size\ntips['size'].value_counts()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-13",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-13",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\n.filter() (cont.)\nSuppose we want each group to consist of 30 or more observations. - To accomplish this goal, we can use the .filter() method on a grouped operation.\n## filter the data such that each group has more than 30 observations\ntips_filtered = (\n  tips\n  .groupby(\"size\")\n  .filter( lambda x : x[\"size\"].count() &gt;= 30 )\n)\n\ntips_filtered.shape\ntips_filtered['size'].value_counts()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-14",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-14",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nThe pandas.core.groupby.DataFrameGroupBy object\nThe .agg(), .transform(), and .filter() methods are commonly used after the .groupby().\ntips_10 = sns.load_dataset('tips').sample(10, random_state=42)\nprint(tips_10)\n\nWe can choose to save just the groupby object without running any other .agg(), .transform(), or .filter() method on it.\n\n\n\n## save just the grouped object\ngrouped = tips_10.groupby('sex')\ngrouped\n\n## see the actual groups of the groupby\n## it returns only the index\ngrouped.groups  ## row numbers?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-15",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-15",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nGroup Calculations Involving Multiple Variables\n\nWe have been usually performing .groupby() calculations on a single column.\n\nIf we specify the calculation we want right after the .groupby(), however, Python will perform the calculation on all the columns it can and silently drop the rest.\n\n\n## calculate the mean on relevant columns\navgs = grouped.mean()\navgs\n\n## list all the columns\ntips_10.columns  ## not all the columns reported a mean."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-16",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-16",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nSelecting a Group\nIf we want to extract a particular group, we can use the .get_group() method, and pass in the group that we want.\n\nFor example, if we wanted the Female values:\n\n## get the 'Female' group\nfemale = grouped.get_group('Female')\nfemale"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-17",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-17",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nIterating Through Groups\n\nAnother benefit of saving just the groupby object is that we can then iterate through the groups individually.\nWe can iterate through our grouped values just like any other container (e.g., list, dictionary) in Python using a for-loop.\n\nfor sex_group in grouped:\n    print(sex_group)\n    \n## we can't really get the 0 element from the grouped object\nprint(grouped[0])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-18",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-18",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nIterating Through Groups (cont.)\n\nLet’s modify the for loop to just show the first element, along with some of the things we get when we loop over the grouped object.\n\n\n\nfor sex_group in grouped:\n    type(sex_group) ## tuple\n    len(sex_group) ## length \n\n    ## get the first element\n    first_element = sex_group[0]\n    first_element\n    type(sex_group[0])\n\n    ## get the second element\n    second_element = sex_group[1]\n    second_element\n    type(second_element)\n\n    ## print what we have\n    print(f'what we have:')\n    print(sex_group)\n\n    ## stop after first iteration\n    break"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-19",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-19",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nIterating Through Groups (cont.)\n\nThe option of iterating through groups with for-loop is available for us if we need to iterate through the groups one at a time."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-20",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-20",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nMultiple Groups\n\nWe can add multiple variables to the .groupby()\n\n## mean by sex and time\nbill_sex_time = tips_10.groupby(['sex', 'time'])\n\ngroup_avg = bill_sex_time.mean()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-21",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-21",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nFlattening the Results (.reset_index())\n\nLet’s look at the type of the group_avg we just calculated.\n\ntype(group_avg)\n\nIf we look at the columns and the index, we get what we expect.\n\ngroup_avg.columns\ngroup_avg.index"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-22",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-22",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nFlattening the Results (.reset_index())\n\nIf we want to get a regular flat DataFrame back, we can call the .reset_index() method on the results.\n\ngroup_method = tips_10.groupby(['sex', 'time']).mean().reset_index()\ngroup_method\n\nAlternatively, we can use the as_index = False parameter in the .groupby() method (it is True by default).\n\ngroup_param = tips_10.groupby(['sex', 'time'], as_index=False).mean()\ngroup_param"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-23",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-23",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nSometimes, we may want to chain calculations after a .groupby() method. - We can always “flatten” the results and then execute another .groupby() statement, but that may not always be the most efficient way of performing the calculation.\nDownload epi_sim.zip from the Files section in Canvas.\nLet’s begin with this epidemiological simulation data on influenza cases in Chicago.\n## notice that we can even read a compressed zip file of a csv\nintv_df = pd.read_csv('data/epi_sim.zip')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-24",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-24",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nThe data set includes six columns: - ig_type: edge type (type of relationship between two nodes in the network, such as “school” and “work”) - intervened: time in the simulation at which an intervention occurred for a given person (pid) - pid: simulated person’s ID number - rep: replication run (each set of simulation parameters was run multiple times) - sid: simulation ID - tr: transmissibility value of the influenza virus"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-25",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-25",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nLet’s count the number of interventions for each replicate, intervention time, and treatment value. - Here, we are counting the ig_type arbitrarily. - We just need a value to get a count of observations for the groups.\ncount_only = (\n  intv_df\n  .groupby([\"rep\", \"intervened\", \"tr\"])\n  [\"ig_type\"]\n  .count()\n)\n\ncount_only"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-26",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-26",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nNow that we’ve done a .groupby() .count(), we can perform an additional .groupby() that calculates the average value. - However, our initial .groupby() method does not return a regular flat DataFrame.\ntype(count_only)\nThe results take the form of a multi-index Series."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-27",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-27",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nIf we want to do another .groupby() operation, we have to pass in the levels parameter to refer to the multi-index levels. - Here we pass in [0, 1, 2] for the first, second, and third index levels, respectively.\ncount_mean = count_only.groupby(level=[0, 1, 2]).mean()\ncount_mean.head()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-28",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-28",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nWe can combine all of these operations in a single command.\ncount_mean = (\n    intv_df\n    .groupby([\"rep\", \"intervened\", \"tr\"])[\"ig_type\"]\n    .count()\n    .groupby(level=[0, 1, 2])\n    .mean()\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-29",
    "href": "danl-lec/danl-m1-lec-05-2024-0305.html#groupby-operations-split-apply-combine-29",
    "title": "Lecture 5",
    "section": "Groupby Operations: Split-Apply-Combine",
    "text": "Groupby Operations: Split-Apply-Combine\nWorking With a MultiIndex\nSee how the relationship between intervened and ig_type varies by rep and tr.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfig = sns.lmplot(\n   data = count_mean.reset_index(),\n   x = \"intervened\",\n   y = \"ig_type\",\n   hue = \"rep\",\n   col = \"tr\",\n   fit_reg = False,\n   palette = \"viridis\" )\n   \nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-1",
    "title": "Lecture 1",
    "section": "Instructor",
    "text": "Instructor\nCurrent Appointment & Education\n\nName: Byeong-Hak Choe.\nAssistant Professor of Data Analytics and Economics, School of Business at SUNY Geneseo.\nPh.D. in Economics from University of Wyoming.\nM.S. in Economics from Arizona State University.\nM.A. in Economics from SUNY Stony Brook.\nB.A. in Economics & B.S. in Applied Mathematics from Hanyang University at Ansan, South Korea.\n\nMinor in Business Administration.\nConcentration in Finance."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-2",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#instructor-2",
    "title": "Lecture 1",
    "section": "Instructor",
    "text": "Instructor\nEconomics, Data Science, and Climate Change\n\nI consider myself an applied economist specializing in environmental economics, with a specific emphasis on climate change.\nMy methodological approach involves leveraging causal inference, econometrics, machine learning methods, and various data science tools for conducting empirical analyses.\nChoe, B.H., 2021. “Social Media Campaigns, Lobbying and Legislation: Evidence from #climatechange/#globalwarming and Energy Lobbies.”\nChoe, B.H. and Ore-Monago, T., 2024. “Governance and Climate Finance in the Developing World”"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-1",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nEmail, Class & Office Hours\n\nEmail: bchoe@geneseo.edu\nClass Homepage:\n\nBrightspace\nGitHub Website\n\nOffice: South Hall 301\nOffice Hours:\n\nTo be determined; By appointment via email."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-2",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-2",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Description\n\nThis course aims to provide overview of how one can process, clean, and crunch datasets with practical case studies.\nKey topics include:\n\nloading, slicing, filtering, transforming, reshaping, and merging data\nsummarizing and visualizing data,\nexploratory data analysis.\n\nWe will cover these topics to solve real-world data analysis problems with thorough, detailed examples."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-3",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-3",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nReference Materials\n\nPython for Data Analysis (3rd Edition) by Wes McKinney\nPython Programming for Data Science by Tomas Beuzen\nCoding for Economists by Arthur Turrell\nPython for Econometrics in Economics by Fabian H. C. Raters\nQuantEcon DataScience - Python Fundamentals by Chase Coleman, Spencer Lyon, and Jesse Perla\nQuantEcon DataScience - pandas by Chase Coleman, Spencer Lyon, and Jesse Perla"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-4",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-4",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Requirements\n\nLaptop or personal computer\n\nOperating System: Mac or Windows.\nSpecification: 2+ core CPU, 4+ GB RAM, and 500+ GB disk storage.\n\nHomework: There will be six homework assignments.\nExam: There will be one take-home exam.\nDiscussions: You are encouraged to participate in GitHub-based online discussions.\n\nCheckout the netiquette policy in the syllabus."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-5",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-5",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Schedule and Contents\nThere will be tentatively 7 class sessions."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-6",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#syllabus-6",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nAssessments\n\\[\n\\begin{align}\n(\\text{Total Percentage Grade}) =\\quad\\, &0.60\\times(\\text{Total Homework Score})\\notag\\\\\n\\,+\\, &0.30\\times(\\text{Take-Home Exam Score})\\notag\\\\\n\\,+\\, &0.10\\times(\\text{Total Discussion Score})\\notag\n\\end{align}\n\\]\n\nEach of the six homework assignments accounts for 10% of the total percentage grade.\nThe exam account for 30% of the total percentage grade.\nParticipation in discussions accounts for 10% of the total percentage grade."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-data-analytics",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-data-analytics",
    "title": "Lecture 1",
    "section": "Why Data Analytics?",
    "text": "Why Data Analytics?\n\n\nFill in the gaps left by traditional business and economics classes.\n\nPractical skills that will benefit your future career.\nNeglected skills like how to actually find datasets in the wild and clean them.\n\nData analytics skills are largely distinct from (and complementary to) the core quantitative works familiar to business undergrads.\n\nData visualization, cleaning and wrangling; databases; machine learning; etc.\n\nIn short, we will cover things that I wish someone had taught me when I was undergraduate to prepare my post-graduate career."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#you-at-the-end-of-this-course",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#you-at-the-end-of-this-course",
    "title": "Lecture 1",
    "section": "You, at the end of this course",
    "text": "You, at the end of this course"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-data-analytics-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-data-analytics-1",
    "title": "Lecture 1",
    "section": "Why Data Analytics?",
    "text": "Why Data Analytics?\n\nData analysts use analytical tools and techniques to extract meaningful insights from data.\n\nSkills in data analytics are also useful for business analysts or market analysts.\n\nBreau of Labor Statistics forecasts that the projected growth rate of the employment in the industry related to data analytics from 2021 to 2031 is 36%.\n\nThe average growth rate for all occupations is 5%."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-python-r-and-databases",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-python-r-and-databases",
    "title": "Lecture 1",
    "section": "Why Python, R, and Databases?",
    "text": "Why Python, R, and Databases?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#why-python-r-and-databases-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#why-python-r-and-databases-1",
    "title": "Lecture 1",
    "section": "Why Python, R, and Databases?",
    "text": "Why Python, R, and Databases?\n\n\nMost Popular Languagues\n\n\nData Science and Big Data\n\n\n\n\nStack Overflow is the most popular Q & A website specifically for programmers and software developers in the world.\nSee how programming languages have trended over time based on use of their tags in Stack Overflow from 2008 to 2022."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#the-state-of-the-art",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#the-state-of-the-art",
    "title": "Lecture 1",
    "section": "The State of the Art",
    "text": "The State of the Art\nGenerative AI and ChatGPT\n\n\nData Science and Big Data Trend\nFrom 2008 to 2023\n\n\nProgrammers in 2024"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#the-state-of-the-art-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#the-state-of-the-art-1",
    "title": "Lecture 1",
    "section": "The State of the Art",
    "text": "The State of the Art\nGenerative AI and ChatGPT\n\nUsers around the world have explored how to best utilize generative pre-trained transformer (GPT) for writing essays and programming codes.\n\n\n\n\nIs AI a threat to data analytics?\n\nFundamental understanding of the subject matter is still crucial for effectively utilizing AI’s capabilities.\n\n\n\n\n\nIf we use Generative AI such as ChatGPT, we should try to understand what Generative AI gives us.\n\nCopying and pasting it without understanding harms our learning opportunity."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-python",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-python",
    "title": "Lecture 1",
    "section": "What is Python?",
    "text": "What is Python?\n\n\nPython is a, simple, easy-to-read, and fast programming language, making it an excellent choice for beginners and experienced developers alike.\n\nVersatility: Python’s extensive library and the vast ecosystem of third-party modules make it suitable for a wide range of applications, from web development and data analysis to machine learning, AI and scientific computing."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-github",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-github",
    "title": "Lecture 1",
    "section": "What is GitHub?",
    "text": "What is GitHub?\n\n\nGitHub is a web-based hosting platform for Git repositories to store, manage, and share code.\nGithub is useful for many reasons, but the main reason is how user friendly it makes uploading and sharing code.\nWe will use a GitHub repository to store Python Notebooks.\nCourse contents will be posted not only in Brightspace but also in our GitHub repositories (“repos”) and websites."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-google-colab",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#what-is-google-colab",
    "title": "Lecture 1",
    "section": "What is Google Colab?",
    "text": "What is Google Colab?\n\n\nGoogle Colab is analogous to Google Drive, but specifically for writing and executing Python code in your web browser.\nA key benefit of Colab is that it is entirely free to use and has many of the standard Python modules pre installed.\n\nIt allows for CPU or GPU usage, even for free users, and stores the files in Google’s servers so you can access your files from anywhere you can connect to the Internet.\n\nUsing Colab also means you can entirely avoid the process of installing Python and any dependencies onto your computer.\nColab notebooks don’t just contain Python code. They can contain text, images, and HTML via Markdown!"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#how-do-we-use-google-colab-with-github",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#how-do-we-use-google-colab-with-github",
    "title": "Lecture 1",
    "section": "How do we use Google Colab with GitHub?",
    "text": "How do we use Google Colab with GitHub?\n\nHow do we use Google Colab with GitHub?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-1",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-1",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nVariables Are Names, Not Places\n\n\n\n\nA value is datum (literal) such as a number or text.\nThere are different types of values:\n\n352.3 is known as a float or double;\n22 is an integer;\n“Hello World!” is a string."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-2",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-2",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nValues, Variables, and Types\na = 10\nprint(a)\n\n\n\n\n\n\n\nA variable is a name that refers to a value.\n\nWe can think of a variable as a box that has a value, or multiple values, packed inside it.\n\nA variable is just a name!"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-3",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-3",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nValues, Variables, and Types\n\n\nSometimes you will hear variables referred to as objects.\nEverything that is not a literal value, such as 10, is an object."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-4",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-4",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nAssignment ( = )\n# Here we assign the integer value 5 to the variable x.\nx = 5   \n\n# Now we can use the variable x in the next line.\ny = x + 12  \ny\n\nIn Python, we use = to assign a value to a variable.\nIn math, = means equality of both sides.\nIn programs, = means assignment: assign the value on the right side to the variable on the left side."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-5",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-5",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nCode and comment style\n\nThe two main principles for coding and managing data are:\n\nMake things easier for your future self.\nDon’t trust your future self.\n\nThe # mark is Google Colab’s comment character.\n\nThe # character has many names: hash, sharp, pound, or octothorpe.\n# indicates that the rest of the line is to be ignored.\nWrite comments before the line that you want the comment to apply to.\n\nConsider adding more comments on code cells and their results using text cells."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-6",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-6",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nAssignment\n\nIn programming code, everything on the right side needs to have a value.\n\nThe right side can be a literal value, or a variable that has already been assigned a value, or a combination.\n\nWhen Python reads y = x + 12, it does the following:\n\nSees the = in the middle.\nKnows that this is an assignment.\nCalculates the right side (gets the value of the object referred to by x and adds it to 12).\nAssigns the result to the left-side variable, y."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-7",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-7",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nVariables Are Names, Not Places\nlist_example = [10, 1.23, \"like this\", True, None]\nprint(list_example)\ntype(list_example)\n\nThe most basic built-in data types that we’ll need to know about are:\n\nintegers 10\nfloats 1.23\nstrings \"like this\"\nbooleans True\nnothing None\n\nPython also has a built-in type of data container called a list (e.g., [10, 15, 20]) that can contain anything, even different types"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-8",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-8",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nTypes\n\n\n\n\nThe second column (Type) contains the Python name of that type.\nThe third column (Mutable?) indicates whether the value can be changed after creation."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-9",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-9",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBrackets\n\n\nThere are several kinds of brackets in Python, including [], {}, and ().\n\n\n[]{}()\n\n\nvector = ['a', 'b']\nvector[0]\n\n[] is used to denote a list or to signify accessing a position using an index.\n\n\n\n{'a', 'b'}  # set\n{'first_letter': 'a', 'second_letter': 'b'}  # dictionary\n\n{} is used to denote a set or a dictionary (with key-value pairs).\n\n\n\nnum_tup = (1, 2, 3)\nsum(num_tup)\n\n() is used to denote\n\na tuple, or\nthe arguments to a function, e.g., function(x) where x is the input passed to the function."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-10",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-10",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\nstring_one = \"This is an example \"\nstring_two = \"of string concatenation\"\nstring_full = string_one + string_two\nprint(string_full)\n\nAll of the basic operators we see in mathematics are available to use:\n\n\n\n\n+ for addition\n- for subtraction\n\n\n\n* for multiplication\n** for powers\n\n\n\n/ for division\n// for integer division\n\n\n\n\nThese work as you’d expect on numbers.\nThese operators are sometimes defined for other built-in data types too.\n\nWe can ‘sum’ strings (which really concatenates them)."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-11",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-11",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\n\n\nlist_one = [\"apples\", \"oranges\"]\nlist_two = [\"pears\", \"satsumas\"]\nlist_full = list_one + list_two\nprint(list_full)\n\nIt works for lists too:\n\n\nstring = \"apples, \"\nprint(string * 3)\n\nWe can multiply strings!"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-12",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-12",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\nQ. Classwork 1.1\nUsing Python operations only, calculate below: \\[\\frac{2^5}{7 \\cdot (4 - 2^3)}\\]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-13",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-13",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n10 == 20\n10 == '10'\n\nBoolean data have either True or False value."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-14",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-14",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\n\n\n\n\n\n\n\n\nExisting booleans can be combined, which create a boolean when executed."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-15",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-15",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nConditions are expressions that evaluate as booleans."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-16",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-16",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nboolean_condition1 = 10 == 20\nprint(boolean_condition1)\n\nboolean_condition2 = 10 == '10'\nprint(boolean_condition2)\n\nThe == is an operator that compares the objects on either side and returns True if they have the same values\nQ. What does not (not True) evaluate to?\nQ. Classwork 1.2"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-17",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-17",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nname = \"Geneseo\"\nscore = 99\n\nif name == \"Geneseo\" and score &gt; 90:\n    print(\"Geneseo, you achieved a high score.\")\n\nif name == \"Geneseo\" or score &gt; 90:\n    print(\"You could be called Geneseo or have a high score\")\n\nif name != \"Geneseo\" and score &gt; 90:\n    print(\"You are not called Geneseo and you have a high score\")\n\nThe real power of conditions comes when we start to use them in more complex examples, such as if statements."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-18",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-18",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nname_list = [\"Lovelace\", \"Smith\", \"Hopper\", \"Babbage\"]\n\nprint(\"Lovelace\" in name_list)\n\nprint(\"Bob\" in name_list)\n\nOne of the most useful conditional keywords is in.\n\nThis one must pop up ten times a day in most coders’ lives because it can pick out a variable or make sure something is where it’s supposed to be.\n\nQ. Check if “a” is in the string “Sun Devil Arena” using in. Is “a” in “Anyone”?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-19",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-19",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nscore = 98\n\nif score == 100:\n    print(\"Top marks!\")\nelif score &gt; 90 and score &lt; 100:\n    print(\"High score!\")\nelif score &gt; 10 and score &lt;= 90:\n    pass\nelse:\n    print(\"Better luck next time.\")\n\nOne conditional construct we’re bound to use at some point, is the if-else chain:"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-20",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-20",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nCasting Variables\n\n\norig_number = 4.39898498\ntype(orig_number)\n\nmod_number = int(orig_number)\nmod_number\ntype(mod_number)\n\n\n\nSometimes we need to explicitly cast a value from one type to another.\n\nWe can do this using built-in functions like str(), int(), and float().\nIf we try these, Python will do its best to interpret the input and convert it to the output type we’d like and, if they can’t, the code will throw a great big error."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-21",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-21",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nTuples and (im)mutability\n\n\nA tuple is an object that is defined by parentheses and entries that are separated by commas, for example (15, 20, 32). (They are of type tuple.)\nTuples are immutable, while lists are mutable.\nImmutable objects, such as tuples and strings, can’t have their elements changed, appended, extended, or removed.\n\nMutable objects, such as lists, can do all of these things.\n\nIn everyday programming, we use lists and dictionaries more than tuples."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-22",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-22",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nIndentation\n\nWe have seen that certain parts of the code examples are indented.\nCode that is part of a function, a conditional clause, or loop is indented.\nIndention is actually what tells the Python interpreter that some code is to be executed as part of, say, a loop and not to executed after the loop is finished."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-23",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-23",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nIndentation\nx = 10\n\nif x &gt; 2:\n    print(\"x is greater than 2\")\n\nHere’s a basic example of indentation as part of an if statement.\nThe standard practice for indentation is that each sub-statement should be indented by 4 spaces."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-24",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-24",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nfor Loops\nname_list = [\"Ben\", \"Chris\", \"Kate\", \"Mary\"]\n\nfor name in name_list:\n    print(name)\n\nA loop is a way of executing a similar piece of code over and over in a similar way.\n\nThe most useful loop is for loops.\n\nAs long as our object is an iterable, then it can be used in this way in a for loop.\nLists, tuples, strings, and dictionaries are iterable."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-25",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-25",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nDictionaries\ncities_to_temps = {\"Paris\": 28, \"London\": 22, \"Seville\": 36, \"Wellesley\": 29}\n\ncities_to_temps.keys()\ncities_to_temps.values()\ncities_to_temps.items()\n\nAnother built-in Python type that is enormously useful is the dictionary.\n\nThis provides a mapping one set of variables to another (either one-to-one or many-to-one).\nIf you need to create associations between objects, use a dictionary.\n\nWe can obtain keys, values, or key-value paris from dictionaries."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-26",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-26",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nRunning on Empty\n\nBeing able to create empty containers is sometimes useful, especially when using loops.\nThe commands to create empty lists, tuples, dictionaries, and sets are lst = [], tup=(), dic={}, and st = set() respectively.\nQ. What is the type of an empty list?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-27",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-27",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nSlicing Methods\n\n\n\n\nWith slicing methods, we can get subset of the data object.\nSlicing methods can apply for strings, lists, arrays, and DataFrames.\nThe above example describes indexing in Python"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-28",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-28",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nStrings\nstring = \"cheesecake\"\nprint( string[-4:] )\n\nFrom strings, we can access the individual characters via slicing and indexing.\n\n\n\nstring = \"cheesecake\"\nprint(\"String has length:\")\nprint( len(string) )\n\nlist_of_numbers = range(1, 20)\nprint(\"List of numbers has length:\")\nprint( len(list_of_numbers) )\n\n\n\nBoth lists and strings will allow us to use the len() command to get their length:"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-29",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-29",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nStrings and Slicing\n\nWe can extract a substring (a part of a string) from a string by using a slice.\nWe define a slice by using square brackets ([]), a start index, an end index, and an optional step count between them.\n\nWe can omit some of these.\n\nThe slice will include characters from index start to one before end:"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-30",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-30",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nGet a Substring with a Slice\n\n[:][ start :][: end ][ start : end ][ start : end : step ]\n\n\nletters = 'abcdefghij'\nletters[:]\n\n[:] extracts the entire sequence from start to end.\n\n\n\nletters = 'abcdefghij'\nletters[4:]\nletters[2:]\nletters[-3:]\nletters[-50:]\n\n[ start :] specifies from the start index to the end.\n\n\n\nletters = 'abcdefghij'\nletters[:3]\nletters[:-3]\nletters[:70]\n\n[: end ] specifies from the beginning to the end index minus 1.\n\n\n\nletters = 'abcdefghij'\nletters[2:5]\nletters[-26:-24]\nletters[35:37]\n\n[ start : end ] indicates from the start index to the end index minus 1.\n\n\n\nletters = 'abcdefghij'\nletters[2 : 6 : 2]   # From index 2 to 5, by steps of 2 characters\nletters[ : : 3]     # From the start to the end, in steps of 3 characters\nletters[ 6 : : 4 ]    # From index 19 to the end, by 4\nletters[ : 7 : 5 ]    # From the start to index 6 by 5:\nletters[-1 : : -1 ]   # Starts at the end and ends at the start\nletters[: : -1 ]\n\n[ start : end : step ] extracts from the start index to the end index minus 1, skipping characters by step."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-31",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-31",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\n\nPython is\n\na zero-indexed language (things start counting from zero);\nleft inclusive;\nright exclusive when we are specifying a range of values."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-32",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-32",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\nlist_example = ['one', 'two', 'three']\nlist_example[ 0 : 1 ]\nlist_example[ 1 : 3 ]\n\n\n\n\nWe can think of items in a list-like object as being fenced in.\n\nThe index represents the fence post."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-33",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-33",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\n\n[index]slice\n\n\nGet an Item by [index]\nsuny = ['Geneseo', 'Brockport', 'Oswego', 'Binghamton', \n        'Stony Brook', 'New Paltz'] \n\nWe can extract a single value from a list by specifying its index:\n\n\n\nsuny[0]\nsuny[1]\nsuny[2]\nsuny[7]\n\nsuny[-1]\nsuny[-2]\nsuny[-3]\nsuny[-7]\n\n\n\n\nGet an Item with a Slice\n\nWe can extract a subsequence of a list by using a slice:\n\nsuny = ['Geneseo', 'Brockport', 'Oswego', 'Binghamton', \n        'Stony Brook', 'New Paltz'] \nsuny[0:2]    # A slice of a list is also a list.\n\n\nsuny[ : : 2]\nsuny[ : : -2]\nsuny[ : : -1]\n\nsuny[4 : ]\nsuny[-6 : ]\nsuny[-6 : -2]\nsuny[-6 : -4]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-34",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-34",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\n\nQ. Classwork 1.3"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-35",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-35",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nFunctions\nint(\"20\") \nfloat(\"14.3\")\nstr(5)\nint(\"xyz\")\n\nA function can take any number and type of input parameters and return any number and type of output results.\nPython ships with more than 65 built-in functions.\nPython also allows a user to define a new function.\nWe will mostly use built-in functions."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-36",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-36",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nFunctions, Arguments, and Parameters\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\")\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\", sep = \"!\")\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\", sep=\" \")\n\nWe invoke a function by entering its name and a pair of opening and closing parentheses.\nMuch as a cooking recipe can accept ingredients, a function invocation can accept inputs called arguments.\nWe pass arguments sequentially inside the parentheses (, separated by commas).\nA parameter is a name given to an expected function argument.\nA default argument is a fallback value that Python passes to a parameter if the function invocation does not explicitly provide one."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-37",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-37",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nFunctions, Arguments, and Parameters\n\nQ. Classwork 1.4"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-38",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-38",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nImporting Modules, Packages, and Libraries\n\nPython is a general-purpose programming language and is not specialized for numerical or statistical computation.\nThe core libraries that enable Python to store and analyze data efficiently are:\n\npandas\nnumpy\nmatplotlib and seaborn"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-39",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-39",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nImporting Modules, Packages, and Libraries\n\npandasnumpymatplotlib and seaborn\n\n\n\n\n\n\npandas provides Series and DataFrames which are used to store data in an easy-to-use format.\n\n\n\n\n\n\n\nnumpy, numerical Python, provides the array block (np.array()) for doing fast and efficient computations;\n\n\n\n\n\n\n\nmatplotlib provides graphics. The most important submodule would be matplotlib.pyplot.\nseaborn provides a general improvement in the default appearance of matplotlib-produced plots."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-40",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-40",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nImporting Modules, Packages, and Libraries\n\nA module is basically a bunch of related codes saved in a file with the extension .py.\nA package is basically a directory of a collection of modules.\nA library is a collection of packages\nWe refer to code of other modules/pacakges/libraries by using the Python import statement.\n\nThis makes the code and variables in the imported module available to our programming codes.\nWe can use the as keyword when importing the modules using their canonical names.\n\nQ. Classwork 1.5"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-41",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-41",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nImporting Modules, Packages, and Libraries\n\nPython is a general-purpose programming language and is not specialized for numerical or statistical computation.\nThe core libraries that enable Python to store and analyze data efficiently are:\n\npandas\nnumpy\nmatplotlib and seaborn"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-42",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-42",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nImporting Modules, Packages, and Libraries\n\npandasnumpymatplotlib and seaborn\n\n\n\n\n\n\npandas provides Series and DataFrames which are used to store data in an easy-to-use format.\n\n\n\n\n\n\n\nnumpy, numerical Python, provides the array block (np.array()) for doing fast and efficient computations;\n\n\n\n\n\n\n\nmatplotlib provides graphics. The most important submodule would be matplotlib.pyplot.\nseaborn provides a general improvement in the default appearance of matplotlib-produced plots."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-43",
    "href": "danl-lec/danl-m1-lec-01-2024-0206.html#python-basics-43",
    "title": "Lecture 1",
    "section": "Python Basics",
    "text": "Python Basics\nImporting Modules, Packages, and Libraries\n\nA module is basically a bunch of related codes saved in a file with the extension .py.\nA package is basically a directory of a collection of modules.\nA library is a collection of packages\nWe refer to code of other modules/pacakges/libraries by using the Python import statement.\n\nThis makes the code and variables in the imported module available to our programming codes.\nWe can use the as keyword when importing the modules using their canonical names.\n\nQ. Classwork 1.5"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#pandas-data-structures-basics-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#pandas-data-structures-basics-1",
    "title": "Lecture 3",
    "section": "Pandas Data Structures Basics",
    "text": "Pandas Data Structures Basics\nCreate Your Own Data\n\nKnowing how to create Series or DataFrames without loading data from a file is a useful skill.\nSeries is a one-dimensional container.\nA Series is very similar to a Python list, except that each element must be the same dtype (object, int64, float64, or datetime64).\n\nThis is the same behavior as the NumPy array (ndarray).\nIf a column contains the number 1 and the sequence of letters “pizza”, the entire dtype of the column will be a string (which is object).\n\nA DataFrame can be thought of as a dictionary of Series objects.\n\nEach key is the column name and the value is the Series."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-1",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nNumPy Array\n\nPython library NumPy introduces an N-dimensional array object, or ndarray.\n\nPandas implicitly uses ndarray, so here let’s see what ndarray is.\nThe easiest way to create an array is to use the .array() method.\n\n\n\n\nimport pandas as np\ndata1 = [6, 7.5, 8, 0, 1]\narr1 = np.array(data1) \narr1\narr2.ndim\narr2.shape\n\ndata2 = [ [1, 2, 3, 4], \n          [5, 6, 7, '8'] ]\narr2 = np.array(data2)\narr2\narr2.ndim\narr2.shape"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-2",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a Series\n\nA Series is a data structure in pandas.\n\nContaining a sequence of values and a corresponding labels, called the index,\nA Series displays the index on the left and the values on the right,\nThe default index consists of the integers 0 through N-1.\n\npd.Series() creates a one-dimensional container including values and an index.\n\nimport pandas as pd\ns = pd.Series( ['banana', 42] )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-3",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a Series\n\nThe “row number” is shown on the left of the Series.\nThis is actually the index for the Series.\nIt is similar to the row name and row index for DataFrame.\n\n## manually assign index values to a series\n## by passing a Python list\ns = pd.Series(\n  data =[\"Wes McKinney\", \"Creator of Pandas\"],\n  index =[\"Person\", \"Who\"],\n)\n\ns"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-4",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a Series\n\nPandas Series can also be created from ndarrys, tuples, and dictionaries.\nQ. Use dictdata to create a Pandas Series.\n\ndictdata = {\n    \"Name\": [\"William Nordhaus\", \"Ronald Coase\"],\n    \"Occupation\": [\"Economist\", \"Economist\"],\n    \"Born\": [\"1941-05-31\", \"1910-12-29\"],\n    \"Died\": [\"\", \"2013-09-02\"],\n    \"Age\": [81, 102],\n  }"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-5",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#create-your-own-data-5",
    "title": "Lecture 3",
    "section": "Create Your Own Data",
    "text": "Create Your Own Data\nCreate a DataFrame\n\nA DataFrame can be thought of as a dictionary of Series objects.\n\nDictionaries are one of the most common ways of creating a DataFrame.\nThe key represents the column name, and the values are the contents of the column.\n\n\neconomists = pd.DataFrame(\n {  \"Name\": [\"William Nordhaus\", \"Ronald Coase\"],\n    \"Occupation\": [\"Economist\", \"Economist\"],\n    \"Born\": [\"1941-05-31\", \"1910-12-29\"],\n    \"Died\": [\"\", \"2013-09-02\"],\n    \"Age\": [81, 102]  } )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\n\nLet’s re-create our example DataFrame.\n\n## create our example dataframe with a row index label\neconomists = pd.DataFrame(\n  data = {\n    \"Occupation\": [\"Economist\", \"Economist\"],\n    \"Born\": [\"1941-05-31\", \"1910-12-29\"],\n    \"Died\": [\"\", \"2013-09-02\"],\n    \"Age\": [81, 102] },\n  index =  [\"William Nordhaus\", \"Ronald Coase\"],\n  columns =[\"Occupation\", \"Born\", \"Died\", \"Age\"] \n)\n\nQ. Select an economist from economists by the row index label to get a Series."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-1",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nSeries Attributes\n\nWhen a series is printed (i.e., the string representation), the index is printed as the first “column”, and the values are printed as the second “column”.\nThere are many attributes and methods associated with a Series object.\n\nfirst_row = economists.loc['William Nordhaus']\ntype(first_row)\nfirst_row.index\nfirst_row.values\ntype(first_row.values)\nfirst_row.shape\nfirst_row.size\nfirst_row.dtypes"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-2",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nAttributes vs. Methods\n\nAttributes can be thought of as features of an object (in this example, our object is a Series).\nMethods can be thought of as some calculation or operation that is performed on an object.\n\nMethods or functions have round parentheses (()), while attributes do not.\n\nThe subsetting syntax .loc[] and .iloc[] consists of all attributes."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-3",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nSeries Methods\n\nLet’s first get a series of the Age column from our economists DataFrame.\n\nages = economists['Age']\n\nWhen we have a vector of numbers, there are common calculations we can perform.\n\nages.mean()\nages.min()\nages.max()\nages.std()\n\n.mean(), .min(), .max(), and .std() are also methods in np.ndarray."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#workflow",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#workflow",
    "title": "Lecture 3",
    "section": "Workflow",
    "text": "Workflow\nWorking Directory\n\nIn Spyder, we can set the working directory.\n\nWindows: Tools &gt; Preferences &gt; Working directory &gt; The following directory\nMac: python &gt; Preferences &gt; Working directory &gt; The following directory\n\nDownload the CSV file, scientists.csv from the Files section in our Canvas.\n\nCreate the data folder in your working directory.\nThen move the scientists.csv file to the data folder in your working directory."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#descriptive-statistics",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#descriptive-statistics",
    "title": "Lecture 3",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n.describe() Methods\n\nLet’s load the CSV file, scientists.csv.\n\n## If we set the working directory, we do not need to use the absolute path\nscientists = pd.read_csv('data/scientists.csv')  \nages = scientists['Age']\n\nThe .describe() method calculates multiple descriptive statistics for numeric variables.\n\nscientists.describe()\nages.describe()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-4",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nBoolean Subsetting on Series\n\nWe can not only subset values using labels and indices, but also supply a vector of boolean values.\nBoolean subsetting of numeric Series works as follows:\n\nSeries[ Series &gt; VALUE  ]\nSeries[ Series == VALUE  ]\nSeries[ Series &lt; VALUE  ]\n\nQ. What if we wanted to subset our ages by identifying those above the mean?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-5",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-5",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nBoolean Subsetting on Series\n\nLet’s look at what ages &gt; ages.mean() returns.\n\nages &gt; ages.mean()\n\ncond = ages &gt; ages.mean()\nages[cond]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-6",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-6",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nMany of the methods that work on Series (and also DataFrames) are “vectorized”, meaning that they work on the entire vector simultaneously.\nages &gt; ages.mean() returns a vector without any for loops.\n\n\nsame lengthw/ scalarsw/ different lengthw/ different length (e.g.,)\n\n\n\nIf we perform an operation between two vectors of the same length, the resulting vector will be an element-by-element calculation of the vectors.\n\nages + ages\nages * ages\n\n\n\nWhen we perform an operation on a vector using a scalar, the scalar will be recycled across all the elements in the vector.\n\nages + 100\nages * 2\n\n\n\nWhen we are working with vectors of different lengths, the behavior will depend on the type() of the vectors.\n\nWith a Series, the vectors will perform an operation matched by the index.\nWith other types(), the shapes must match.\n\n\n\n\nages + pd.Series( [1, 100] )\nages + np.array( [1, 100] )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-7",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-series-7",
    "title": "Lecture 3",
    "section": "The Series",
    "text": "The Series\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nWhat’s convenient in Pandas is how data alignment is almost always automatic. - If possible, things will always align themselves with the index label when actions are performed.\nLet’s consider .sort_index() method:\n\nSeries.sort_index(ascending=False) sorts Series by index in descending order.\n\nrev_ages = ages.sort_index(ascending =False) \nrev_ages\nQ. What is ages + rev_ages?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nParts of a DataFrame\n\nThe DataFrame is the most common Pandas object.\n\nIt can be thought of as Python’s way of storing spreadsheet-like data.\nMany of the features of the Series data structure carry over into the DataFrame.\n\nThere are 3 main parts of a Pandas DataFrame object:\n\n\n.index, (2) .columns, and (3) .values\n\n\n\nscientists.index\nscientists.columns\nscientists.values"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-1",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nBoolean Subsetting on DataFrames\n\nBoolean subsetting of DataFrames works like boolean subsetting a Series.\n\nDataFrame[ DataFrame['VARIABLE_NAME'] &gt; VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] == VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] &lt; VALUE  ]\n\n\n## boolean vectors will subset rows\nscientists.loc[ scientists['Age'] &gt; scientists['Age'].mean() ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-2",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nSubsetting Multiple Rows and Columns"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-3",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nPandas supports broadcasting because the Series and DataFrame objects are built on top of the numpy library.\n\nBroadcasting describes what happens when performing operations between array-like objects.\nThese behaviors depend on the type of object, its length, and any labels associated with the object."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#the-dataframe-4",
    "title": "Lecture 3",
    "section": "The DataFrame",
    "text": "The DataFrame\nOperations Are Automatically Aligned and Vectorized (Broadcasting)\n\nw/ scalarsw/ Series\n\n\n\nWhen we perform an action on a dataframe with a scalar, it will try to apply the operation on each cell of the dataframe.\n\nscientists * 2\n\n\n\nBy default, arithmetic operations between DataFrames and Series match the index of the Series on the DataFrame’s columns,\nThe operations will be broadcasted along the rows.\n\npd.Series([10]) + scientists[['Age']]\npd.Series([10], index = ['Age']) + scientists[['Age']]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nAdd Additional Columns\n\nThe type of the Born and Died columns is object, meaning they are strings or a sequence of characters.\n\nscientists.dtypes\n\nWe can convert the strings to a proper datetime type so we can perform common date and time operations.\n\n## format the 'Born' column as a datetime\nborn_datetime = pd.to_datetime( scientists['Born'] )\n\ndied_datetime = pd.to_datetime( scientists['Died'] )\n\nMore examples with datetimes would be discussed later in March or April."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-1",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-1",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nAdd Additional Columns\n\nWe can create a new set of columns that contain the datetime representations of the object (string) dates.\n\nscientists['born_dt'], scientists['died_dt'] = (\n  born_datetime,\n  died_datetime\n)\n\nscientists.head()\nscientists.shape\nscientists.dtypes"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-2",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-2",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDirectly Change a Column\n\n(1)(2)(3)(4)(5)\n\n\n\nLet’s look at the original Age values.\n\nscientists['Age']\n\n\n\nLet’s shuffle the values using .sample().\n\n## the frac=1 tells pandas to randomly select 100% of the values\n## the random_state makes the randomization the same each time\nscientists['Age'].sample(frac=1, random_state = 210)\n\n\n\nLet’s assign scientists['Age'] to the shuffled one.\n\nscientists['Age'] = (\n  scientists['Age']\n  .sample(frac=1, random_state = 210)\n)\nscientists['Age']\n\nHow is scientists['Age']?\n\n\n\n\nWe tried to randomly shuffle the values, but when we assigned the values back into the dataframe, it reverted back to the original order.\n\nThat’s because Pandas will try to automatically join on the .index values on many operations, for this example to get around this problem we need to remove that .index information.\nOne way of doing that, is to assign just the .values of the shuffled values that does not have any .index value associated with it.\n\n\n\n\nscientists['Age'] = (\n  scientists['Age']\n  .sample(frac=1, random_state = 210)\n  .values    ## remove the index so it doesn't auto align the values\n)\n\nscientists['Age']\n\nHow is scientists['Age'] now?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-3",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-3",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDirectly Change a Column\n\nHere is how we re-calculate the real ages.\n\n## subtracting dates will give us number of days\nscientists['age_days'] =  scientists['died_dt'] - scientists['born_dt']\n\n## we can convert the value to just the year\n## using the astype method\nscientists['age_years'] = (\n  scientists['age_days']\n  .astype('timedelta64[Y]')\n)\n\nscientists"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-4",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-4",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nModifying Columns with .assign()\n\n(1)(2)lambda (1)lambda (2)\n\n\n\nLet’s redo the age_years column creation, but this time using .assign().\n\nscientists = scientists.assign(\n  ## new columns on the left of the equal sign\n  ## how to calculate values on the right of the equal sign\n  ## separate new columns with a comma\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = scientists['age_days'].astype('timedelta64[Y]')\n)\n\n\n\nWhen calculating age_year_assign, we did not use age_days_assign.\n\nTo be able to use age_days_assign when calculating age_year_assign in the previous panel, we need to know about lambda functions.\n\n\nscientists = scientists.assign(\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = lambda some_df: some_df['age_days_assign'].astype('timedelta64[Y]')\n)\n\nWe will cover lambda functions in detail later.\n\n\n\n\nPython has support for so-called anonymous or lambda functions.\n\nLambda functions are a way of writing functions consisting of a single statement, the result of which is the return value.\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\n\nshort_function(2)\nequiv_anon(2)\n\n\n\nThe power of lambda is better shown when we use them as an anonymous function inside another function.\n\nSay we have a function definition that takes one argument, and that argument will be multiplied with an unknown number:\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\n\nshort_function(2)\nequiv_anon(2)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-5",
    "href": "danl-lec/danl-m1-lec-03-2024-0220.html#making-changes-to-series-and-dataframes-5",
    "title": "Lecture 3",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDropping Values\n\nColumnsRows\n\n\n\nTo drop a column, we can select columns to drop with the .drop() method with axis = 1 or axis = \"columns\" on our dataframe.\n\n## all the current columns in our data\nscientists.columns\n\n## drop the shuffled age column\n## we provide the axis=1 argument to drop column-wise\nscientists_dropped = scientists.drop( ['Age'], axis =\"columns\")\nscientists_dropped.columns\n\n\n\nTo drop rows, we can select rows by index to drop with the .drop() method with axis = 0, which is default.\n\n## all the current columns in our data\nscientists.columns\n\n## drop rows by their indices\nscientists_rows_dropped = scientists.drop( [2, 4, 6] )\nscientists_rows_dropped"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes",
    "title": "Lecture 4",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nModifying Columns with .assign()\n\n(0)(1)(2)\n\n\n\nLet’s create a new set of columns that contain the datetime representations of the object (string) dates.\n\n## format the 'Born' column as a datetime\nborn_datetime = pd.to_datetime( scientists['Born'] )\ndied_datetime = pd.to_datetime( scientists['Died'] )\n\nscientists['born_dt'], scientists['died_dt'] = (\n  born_datetime,\n  died_datetime\n)\n\nscientists['age_days'] =  scientists['died_dt'] - scientists['born_dt']\n\n\n\nLet’s create the age_year_assign column using .assign().\n\nscientists = scientists.assign(\n  ## new columns on the left of the equal sign\n  ## how to calculate values on the right of the equal sign\n  ## separate new columns with a comma\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = scientists['age_days'].astype('timedelta64[Y]')\n)\n\n\n\nIn the previous panel, we had to use age_days to create age_year_assign.\n\nTo be able to use age_days_assign when calculating age_year_assign in the previous panel, we need to know about lambda functions.\n\n\nscientists = scientists.drop( ['age_days'], axis = 1)  ## to drop age_days\nscientists = scientists.assign(\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = lambda some_df: \n    some_df['age_days_assign'].astype('timedelta64[Y]') \n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-1",
    "title": "Lecture 4",
    "section": "Pandas Data Structures Basics",
    "text": "Pandas Data Structures Basics\nLambda functions\n\n(1)(2)(3)(4)(5)\n\n\n\nPython has support for so-called anonymous or lambda functions.\n\nLambda functions are a way of writing functions consisting of a single statement, the result of which is the return value.\nA syntax for a lambda function is lambda ARGUMENTS : EXPRESSION\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\nshort_function(2)\nequiv_anon(2)\n\n\n\nA lambda function can have multiple arguments:\n\nfn_two = lambda a, b : a * b\nfn_two(1, 2)\n\nfn_three = lambda a, b, c : a + b + c\nfn_two(1, 2, 3)\n\n\n\nThe power of lambda is better shown when we use them as an anonymous function inside another function.\n\nSay we have a function definition that takes one argument, and that argument will be multiplied with an unknown number:\n\n\ndef myfunc(n):\n  return lambda a : a * n\n\n\n\n\n\nUse that function definition to make a function that always doubles the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_doubler = myfunc(2)\nmy_doubler(10)\n\n\nUse the same function definition to make a function that always triples the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_tripler = myfunc(3)\nmy_tripler(10)\n\n\n\n\n\nHere is the example of applying a lambda function to single variable in DataFrame using .assign()\n\nvalues= [['Rohan',182],['Elvish',100],['Deepak',198],\n         ['Soni',160],['Radhika',140],['Vansh',180]]\ndf = pd.DataFrame(values, columns = ['name','tot_marks'])\n \n## Applying lambda function to find percentage of 'tot_marks' column\n## using df.assign()\ndf = df.assign( percentage = \n  lambda some_name: (some_name['tot_marks'] /200 * 100) )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe",
    "title": "Lecture 4",
    "section": "The DataFrame",
    "text": "The DataFrame\nSubsetting Multiple Rows and Columns"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-1",
    "title": "Lecture 4",
    "section": "The DataFrame",
    "text": "The DataFrame\nBoolean Subsetting\n\nSeriesDataFrameQ.\n\n\n\nWe can not only subset values using labels and indices, but also supply a vector of boolean values.\nBoolean subsetting of numeric Series works as follows:\n\nSeries[ Series &gt; VALUE  ]\nSeries[ Series == VALUE  ]\nSeries[ Series &lt; VALUE  ]\n\n\n\n\n\nBoolean subsetting of DataFrames works like boolean subsetting a Series.\n\nDataFrame[ DataFrame['VARIABLE_NAME'] &gt; VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] == VALUE  ]\nDataFrame[ DataFrame['VARIABLE_NAME'] &lt; VALUE  ]\n\n\nimport pandas as pd\nscientists = read_csv('data/scientists.csv')  \n\n## What if we want to subset our ages by identifying those above the mean?\nscientists.loc[ scientists['Age'] &gt; scientists['Age'].mean() ]\n\n\n\nConsider the two Series, area and pop:\n\narea = pd.Series({'California': 423967, 'Texas': 695662,\n                  'New York': 141297, 'Florida': 170312,\n                  'Illinois': 149995})\npop = pd.Series({'California': 38332521, 'Texas': 26448193,\n                 'New York': 19651127, 'Florida': 19552860,\n                 'Illinois': 12882135})\n\nUse boolean subsetting to find states whose population density is greater than 100 or less than 50."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-dataframe-2",
    "title": "Lecture 4",
    "section": "The DataFrame",
    "text": "The DataFrame\nSubsetting Rows with .query()\n\nWe can also subset rows based on a condition using DataFrame.query():\n\n\nData.query() (1)(2)(3)(4)\n\n\ndf = pd.DataFrame(\n    ## array with numbers from 0 to 35, 6 rows and 6 columns\n    data = np.reshape( range(36), (6, 6) ),\n    index = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"],\n    columns = [\"col\" + str(i) for i in range(6)],\n    dtype = float,\n)\ndf[\"col6\"] = [\"apple\", \"orange\", \"pineapple\", \"mango\", \"kiwi\", \"lemon\"]\ndf\n\n\n\nSelect rows if a value of col6 is “kiwi” or “pineapple”:\n\ndf.query(\"col6 == 'kiwi' or col6 == 'pineapple'\")\n\nSelect rows if a value of col0 is greater than 6:\n\ndf.query(\"col0 &gt; 6\")\n\n\n\nSelect flights that departed on January 1:\n\n## download NY_flights.csv from the Files section in Canvas\nflights = pd.read_csv(\"data/NY_flights.csv\")\nflights.head()\n\nflights.query(\"month == 1 and day == 1\")\n\n\n\nSelect penguins whose bill_length_mm is smaller than 1.8 * bill_depth_mm.\n\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\npenguins.query('bill_length_mm &lt; bill_depth_mm*1.8')\n\n\n\nObjects not in DataFrame can be referenced with an @ character like@a + b.\n\noutside_var = 21\npenguins.query('bill_depth_mm &gt; @outside_var')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe",
    "title": "Lecture 4",
    "section": "The Series and the DataFrame",
    "text": "The Series and the DataFrame\nSorting and Ranking\n\nLet’s consider .sort_index() and .sort_values() methods:\n\n\nSeries\n\n\n\nSeries.sort_index(ascending=False) sorts Series by index in descending order.\nSeries.sort_values(ascending=False) sorts Series by value in descending order.\n\nrev_ages = ages.sort_index(ascending =False) \nsorted_ages = ages.sort_values(ascending =False) \n] ## DataFrame - DataFrame.sort_index(ascending=False) sorts DataFrame by index in descending order. - DataFrame.sort_values(by = \"VAR\", ascending=False) sorts DataFrame by value of VAR in descending order.\nrev_ages = scientists.sort_index(ascending =False) \nsorted_df = scientists.sort_values(by = 'Age', \n                                   ascending =False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#the-series-and-the-dataframe-1",
    "title": "Lecture 4",
    "section": "The Series and the DataFrame",
    "text": "The Series and the DataFrame\nSorting and Ranking\n\nIn Pandas, there are a variety of ranking functions with .rank().\n\n\nexamplemethod (1)method (2)na_optionpct\n\n\n\nConsider the following DataFrame.\n\nimport numpy as np\ndf = pd.DataFrame(data={'Animal': ['fox', 'Kangaroo', 'deer',\n                                   'spider', 'snake'],\n                        'Number_legs': [4, 2, 4, 8, np.nan]})\ndf\n\n\n\n.rank(method = \"min\", ascending=False) does give the largest values the smallest ranks.\n.rank(method = \"dense\", ascending=False) does give the largest values the smallest ranks without any gaps between ranks when breaking ties.\n.rank(method = \"average\", ascending=False) calculates the average rank for each unique value.\n\n\n\ndf['default_rank'] = df['Number_legs'].rank(ascending = False)  ## method = 'average'\ndf['min_rank'] = df['Number_legs'].rank(method='min', ascending = False)\ndf['dense_rank'] = df['Number_legs'].rank(method='dense', ascending = False)\ndf\n\n\n\nna_option = keep assigns NaN rank to NaN values (default).\nna_option = top assign smallest rank to NaN values if ascending\nna_option = bottom assign highest rank to NaN values if ascending\n\ndf['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')\ndf['NA_top'] = df['Number_legs'].rank(na_option='top')\ndf\n\n\n\npct = True displays the returned rankings in percentile form.\n\n\ndf['pct_rank'] = df['Number_legs'].rank(pct=True)\ndf"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-1",
    "title": "Lecture 4",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nDropping Values\n\nColumnsRows\n\n\n\nTo drop a column, we can select columns to drop with the .drop() method with axis = 1 or axis = \"columns\" on our dataframe.\n\n## all the current columns in our data\nscientists.columns\n\n## drop the shuffled age column\n## we provide the axis=1 argument to drop column-wise\nscientists_dropped = scientists.drop( ['Age'], axis =\"columns\")\nscientists_dropped.columns\n\n\n\nTo drop rows, we can select rows by index to drop with the .drop() method with axis = 0, which is default.\n\n## all the current columns in our data\nscientists.columns\n\n## drop rows by their indices\nscientists_rows_dropped = scientists.drop( [2, 4, 6] )\nscientists_rows_dropped"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#making-changes-to-series-and-dataframes-2",
    "title": "Lecture 4",
    "section": "Making Changes to Series and DataFrames",
    "text": "Making Changes to Series and DataFrames\nModifying Columns with .assign()\n\n(0)(1)(2)\n\n\n\nLet’s create a new set of columns that contain the datetime representations of the object (string) dates.\n\n## format the 'Born' column as a datetime\nborn_datetime = pd.to_datetime( scientists['Born'] )\ndied_datetime = pd.to_datetime( scientists['Died'] )\n\nscientists['born_dt'], scientists['died_dt'] = (\n  born_datetime,\n  died_datetime\n)\n\nscientists['age_days'] =  scientists['died_dt'] - scientists['born_dt']\n\n\n\nLet’s create the age_year_assign column using .assign().\n\nscientists = scientists.assign(\n  ## new columns on the left of the equal sign\n  ## how to calculate values on the right of the equal sign\n  ## separate new columns with a comma\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = scientists['age_days'].astype('timedelta64[Y]')\n)\n\n\n\nIn the previous panel, we had to use age_days to create age_year_assign.\n\nTo be able to use age_days_assign when calculating age_year_assign in the previous panel, we need to know about lambda functions.\n\n\nscientists = scientists.drop( ['age_days'], axis = 1)  ## to drop age_days\nscientists = scientists.assign(\n  age_days_assign = scientists['died_dt'] - scientists['born_dt'],\n  age_year_assign = lambda some_df: \n    some_df['age_days_assign'].astype('timedelta64[Y]') \n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#pandas-data-structures-basics-2",
    "title": "Lecture 4",
    "section": "Pandas Data Structures Basics",
    "text": "Pandas Data Structures Basics\nLambda functions\n\n(1)(2)(3)(4)(5)\n\n\n\nPython has support for so-called anonymous or lambda functions.\n\nLambda functions are a way of writing functions consisting of a single statement, the result of which is the return value.\nA syntax for a lambda function is lambda ARGUMENTS : EXPRESSION\n\n\ndef short_function(x):\n  return x * 2\n\nequiv_anon = lambda x: x * 2\nshort_function(2)\nequiv_anon(2)\n\n\n\nA lambda function can have multiple arguments:\n\nfn_two = lambda a, b : a * b\nfn_two(1, 2)\n\nfn_three = lambda a, b, c : a + b + c\nfn_two(1, 2, 3)\n\n\n\nThe power of lambda is better shown when we use them as an anonymous function inside another function.\n\nSay we have a function definition that takes one argument, and that argument will be multiplied with an unknown number:\n\n\ndef myfunc(n):\n  return lambda a : a * n\n\n\n\n\n\nUse that function definition to make a function that always doubles the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_doubler = myfunc(2)\nmy_doubler(10)\n\n\nUse the same function definition to make a function that always triples the number we send in:\n\ndef myfunc(n):\n  return lambda a : a * n\n\nmy_tripler = myfunc(3)\nmy_tripler(10)\n\n\n\n\n\nHere is the example of applying a lambda function to single variable in DataFrame using .assign()\n\nvalues= [['Rohan',182],['Elvish',100],['Deepak',198],\n         ['Soni',160],['Radhika',140],['Vansh',180]]\ndf = pd.DataFrame(values, columns = ['name','tot_marks'])\n \n## Applying lambda function to find percentage of 'tot_marks' column\n## using df.assign()\ndf = df.assign( percentage = \n  lambda some_name: (some_name['tot_marks'] /200 * 100) )"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-1",
    "title": "Lecture 4",
    "section": "Exporting and Importing Data",
    "text": "Exporting and Importing Data\nComma-Separated Values (CSV)\n\nComma-separated values (CSV) are the most flexible data storage type.\n\nFor each row, the column information is separated with a comma.\n\nTo export Series or DataFrame as a csv file, we use the to_csv() method.\n\n## index =False  does not write the row names in the CSV output\nscientists.to_csv('output/scientists_df_no_index.csv', \n                   index =False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exporting-and-importing-data-2",
    "title": "Lecture 4",
    "section": "Exporting and Importing Data",
    "text": "Exporting and Importing Data\nExcel\n\nThe more of your work you can do in Python and/or R, the easier it will be to scale up to larger projects, catch and fix mistakes, and collaborate.\nHowever, Excel’s popularity and market share are unrivaled.\nTo export Series or DataFrame as an .xlsx file, we use the to_excel() method.\n\n## saving a DataFrame into Excel format\nscientists.to_excel(\n  \"output/scientists_df.xlsx\",\n  sheet_name = \"scientists\",\n  index = False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nHere we discuss how to use summary statistics and visualization to explore your data in a systematic way, a task that statisticians call exploratory data analysis (EDA).\nEDA is an iterative cycle. We:\n\nGenerate questions about your data.\nSearch for answers by visualizing, transforming, and modelling our data.\nUse what we learn to refine our questions and/or generate new questions.\n\nHere, we focus on the visualization part."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-1",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nTidy data.frame\n\n\nIn a tidy DataFrame,\n\nA variable is in a column.\nAn observation is in a row.\nA value are in a cell."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-2",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nTidy data.frame\n\nA variable is a quantity, quality, or property that we can measure/count.\nAn observation is a set of measurements made under similar conditions (e.g, similar unit of entity, time, and/or geography).\n\nWe usually make all of the measurements in an observation at the same time and on the same object.\nAn observation will contain several values, each associated with a different variable.\nWe sometimes refer to an observation as a data point.\n\nThe value of a variable may change from measurement to measurement."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-3",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-3",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nCategorical/Discrete vs. Continuous Variables\n\nA discrete/categorical variable is a variable whose value is obtained by counting and is whole numbers.\n\nNumber of red marbles in a jar\nNumber of heads when flipping three coins\nStudents’ letter grade\nUS state/county\n\nA continuous variable is a variable whose value is obtained by measuring and can have a decimal or fractional value.\n\nHeight/weight of students\nTime it takes to get to school\nFuel efficiency of a vehicle (e.g., miles per gallon)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-4",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#exploratory-data-analysis-4",
    "title": "Lecture 4",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nMaking Discoveries from a Data Set\n\nDistributionRelationshipStepsTypesSummary Stat.\n\n\n\nWhat type of variation occurs within a variable?\nVariation is the tendency of the values of a variable to change from measurement to measurement.\n\nWe can see variation easily in real life; if we measure any continuous variable twice, we will be likely to get two different values.\nWhich values are the most common? Why?\nWhich values are rare? Why? Does that match your expectations?\nCan we see any unusual patterns? What might explain them?\n\n\n\n\n\nCo-variation is the tendency for the values of two or more variables to vary together in a related way.\nWhat type of co-variation occurs between variables?\n\n\n\n\nFigure out whether variables of interests are categorical or continuous.\nThink which geometric objects, aesthetic mappings, and faceting are appropriate to visualize distributions and relationships.\nIf needed, transform a given DataFrame (e.g., subset of observations, new variables, summarized data) and try new visualizations.\n\n\n\n\nA distribution of a categorical variable (e.g., bar charts and more)\nA distribution of a continuous variable (e.g., histograms and more)\nA relationship between two categorical variables (e.g., bar charts and more)\nA relationship between two continuous variables (e.g., scatter plots and more)\nA relationship between a categorical variable and a continuous variable (e.g., boxplots and more)\nA time trend of a categorical variable (e.g., bar plots and more)\nA time trend of a continuous variable (e.g., line plots and more)\n\n\n\n\nUse skim(DataFrame) or .describe() to know:\n\nMean (Average, Expected Value);\nStandard Deviation (SD)\nMinimum, First Quartile (Q1), Median (Q2), Third Quartile (Q3), and Maximum."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization",
    "title": "Lecture 4",
    "section": "Data Visualization",
    "text": "Data Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraphs and charts let us explore and learn about the structure of the information we have in DataFrame.\nGood data visualizations make it easier to communicate our ideas and findings to other people."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#seaborn",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#seaborn",
    "title": "Lecture 4",
    "section": "seaborn",
    "text": "seaborn\n\nseaborn is a Python data visualization library based on matplotlib.\n\nIt allows us to easily create beautiful but complex graphics using a simple interface.\nIt also provides a general improvement in the default appearance of matplotlib-produced plots, and so I recommend using it by default.\n\n\nimport seaborn as sns\nsns.set_theme(rc={'figure.dpi': 600, \n                  'figure.figsize': (5, 3.75)})   ## better quality"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-1",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-1",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nGetting started with seaborn\n\nLet’s get the names of DataFrames provided by the seaborn library:\n\nimport seaborn as sns\nprint( sns.get_dataset_names() )\n\nLet’s use the titanic and tips DataFrames:\n\ntitanic = sns.load_dataset('titanic')\ntitanic.head()\ntips = sns.load_dataset('tips')\ntips.head()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-2",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-2",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nBar Chart\n\nA bar chart is used to plot the frequency of the different categories.\n\nIt is useful to visualize how values of a categorical variable are distributed.\nA variable is categorical if it can only take one of a small set of values.\n\nWe use sns.countplot() function to plot a bar chart:\n\n\n\nsns.countplot(data = titanic,\n              x =  'sex')\n\n\nMapping\n\ndata: DataFrame.\nx: Name of a categorical variable (column) in DataFrame"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-3",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-3",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nBar Chart\n\nWe can further break up the bars in the bar chart based on another categorical variable.\n\nThis is useful to visualize the relationship between the two categorical variables.\n\n\n\n\nsns.countplot(data = titanic,\n              x = 'sex'\n              hue = 'survived')\n\n\nMapping\n\nhue: Name of a categorical variable"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-4",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-4",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nHistogram\n\nA histogram is a continuous version of a bar chart.\n\nIt is used to plot the frequency of the different values.\nIt is useful to visualize how values of a continuous variable are distributed.\n\nWe use sns.histplot() function to plot a histogram: :::: {.columns} ::: {.column width=“50%”}\n\nsns.histplot(data = titanic,\n             x =  'age', \n             bins = 5)\n:::\n\n\nMapping\n\nbins: Number of bins\n\n\n\n::::"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-5",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-5",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nHistogram\n\nA boxplot computes a summary of the distribution and then display a specially formatted box.\n\nIt is useful to visualize how values of a continuous variable are distributed across different values of another (categorical) variable.\n\nWe use sns.histplot() function to plot a histogram: :::: {.columns} ::: {.column width=“50%”}\n\nsns.boxplot(data = tips,\n             y='total_bill')\n:::\n\nsns.boxplot(data = tips,\n            x='time', y='total_bill')\n\n::::"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-6",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-6",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nScatter plot\n\nA scatter plot is used to display the relationship between two continuous variables.\n\nWe can see co-variation as a pattern in the scattered points.\n\nWe use sns.scatterplot() function to plot a scatter plot:\n\n\n\nsns.scatterplot(data = tips,\n                x = 'total_bill', \n                y = 'tip')\n\n\nMapping\n\nx: Name of a continuous variable on the horizontal axis\ny: Name of a continuous variable on the vertical axis"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-7",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-7",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nScatter plot\n\nTo the scatter plot, we can add a hue-VARIABLE mapping to display how the relationship between two continuous variables varies by VARIABLE.\nSuppose we are interested in the following question:\n\nQ. Does a smoker and a non-smoker have a difference in tipping behavior?\n\n\nsns.scatterplot(data = tips,\n                x = 'total_bill', \n                y = 'tip',\n                hue = 'smoker')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-8",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-8",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nFitted line\n\nFrom the scatter plot, it is often difficult to clearly see the relationship between two continuous variables.\n\nsns.lmplot() adds a line that fits well into the scattered points.\nOn average, the fitted line describes the relationship between two continuous variables.\n\n\nsns.lmplot(data = tips,\n           x = 'total_bill', \n           y = 'tip')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-9",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-9",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nTransparency with alpha\n\nIn a scatter plot, adding transparency with alpha helps address many data points on the same location.\n\nWe can map alpha to number between 0 and 1.\n\n\n\n\nsns.scatterplot(x = 'total_bill', \n                y = 'tip',\n                hue = 'smoker',\n                alpha = .25)\n\nsns.lmplot(data = tips,\n           x = 'total_bill', \n           y = 'tip',\n           scatter_kws = {'alpha' : 0.2})"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-10",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-10",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nScatter plot\n\nTo the scatter plot, we can add a hue-VARIABLE mapping to display how the relationship between two continuous variables varies by VARIABLE.\nUsing the fitted lines, let’s answer the following question:\n\nQ. Does a smoker and a non-smoker have a difference in tipping behavior?\n\n\nsns.scatterplot(data = tips,\n                x = 'total_bill', \n                y = 'tip',\n                hue = 'smoker')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-11",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-11",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nLine cahrt\n\nA line chart is used to display the trend in a continuous variable or the change in a continuous variable over other variable.\n\nIt draws a line by connecting the scattered points in order of the variable on the x-axis, so that it highlights exactly when changes occur.\n\nWe use sns.lineplot() function to plot a line plot:\n\n\n\npath_csv = 'https://bcdanl.github.io/data/dji.csv'\ndow = pd.read_csv(path_csv, index_col=0, parse_dates=True)\nsns.lineplot(data = dow,\n             x =  'Date', \n             y =  'Close')\n\n\nMapping\n\nx: Name of a continuous variable (often time variable) on the horizontal axis\ny: Name of a continuous variable on the vertical axis"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-12",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-12",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nLine cahrt\n\nFor line charts, we often need to group or connect observations to visualize the number of distinct lines.\n\nhealthexp = ( sns.load_dataset(\"healthexp\")\n             .sort_values([\"Country\", \"Year\"])\n             .query(\"Year &lt;= 2020\") )\nhealthexp.head()\n\nsns.lineplot(data = dow,\n             x =  'Year', \n             y =  'Life_Expectancy',\n             color = 'Country')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-13",
    "href": "danl-lec/danl-m1-lec-04-2024-0227.html#data-visualization-with-seaborn-13",
    "title": "Lecture 4",
    "section": "Data Visualization with seaborn",
    "text": "Data Visualization with seaborn\nFaceting\n\nFaceting allows us to plot subsets (facets) of our data across subplots.\n\n\nStep 1. FacetGrid()Step2. FacetGrid().map()\n\n\n\nFirst, we create a FacetGrid() object with the data we will be using and define how it will be subset with the row and col arguments:\n\ng = sns.FacetGrid(\n      data = titanic,\n      row='class',\n      col='sex')\n\n\n\nSecodn, we use the FacetGrid().map() method to run a plotting function on each of the subsets, passing along any necessary arguments.\n\ng.map(sns.histplot, 'age', kde=True)  ## kde: kernel density (probability density function)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-libraries",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-libraries",
    "title": "Lecture 2",
    "section": "pandas Libraries",
    "text": "pandas Libraries\n\nPython is a general-purpose programming language and is not specialized for numerical or statistical computation.\nA library is a collection of packages that includes a bunch of related Python codes.\n\n\n\npandas is a Python library that provides high-performance data structures and data analysis tools:\n\nData manipulation and analysis\nDataFrame and Series objects\nExport and import data from files and web\nHandling of missing data"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#importing-pandas-libraries",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#importing-pandas-libraries",
    "title": "Lecture 2",
    "section": "Importing pandas Libraries",
    "text": "Importing pandas Libraries\n\nWe refer to code of libraries by using the Python import statement.\n\nThis makes the code and variables in the imported module available to our programming codes.\nWe can use the as keyword when importing the libraries using their canonical names.\n\n\nimport pandas as pd"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas",
    "title": "Lecture 2",
    "section": "Pandas",
    "text": "Pandas\nLoad Data Sets\n\nWhen given a data set, we first load it and begin looking at its structure and contents.\nThe simplest way of looking at a data set is to look at and subset specific rows and columns.\nWe can see what type of information is stored in each column, and can start looking for patterns by aggregating descriptive statistics."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-1",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas-1",
    "title": "Lecture 2",
    "section": "Pandas",
    "text": "Pandas\nLoad Data Sets\n\nWith the library loaded we can use the read_csv() function to load a CSV data file.\nIn order to access the read_csv() function from pandas, we use something called “dot” notation.\n\nWe write pd.read_csv() to say: within the pandas library we just loaded, look inside for the read_csv() function.\n\n\n# *.tsv is a file of tab-separated values.\n# we can use the sep parameter and indicate a tab with \\t\ndf  = pd.read_csv('https://bcdanl.github.io/data/gapminder.tsv', sep='\\t')\ndf"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas---load-data-sets",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#pandas---load-data-sets",
    "title": "Lecture 2",
    "section": "Pandas - Load Data Sets",
    "text": "Pandas - Load Data Sets\n\ntype().shape.columns.dtypes\n\n\n\nWe can check to see if we are working with a Pandas Dataframe by using the built-in type() function (i.e., it comes directly from Python, not a separate library such as Pandas).\n\ntype(df)\n\nThe type() function is handy when we begin working with many different types of Python objects and need to know what object we are currently working on.\n\n\n\n\nEvery DataFrame object has a .shape attribute that will give us the number of rows and columns of the DataFrame.\n\ndf.shape\n#  It’s written as df.shape and not df.shape()\n\nSince .shape is an attribute of the DataFrame object, and not a function or method of the DataFrame object, it does not have round parentheses after the period.\n\n\n\n\nTo get a gist of what information the data set contains, we look at the column names.\nThe column names are given using the .column attribute of the DataFrame object.\n\ndf.columns\n\nQ. What is the type of the column names?\n\n\n\n\nEach column (i.e., Series) has to be the same type, whereas each row can contain mixed types.\nWe can use the .dtypes attribute or the .info() method to see the type of each column in DataFrame.\n\ndf.dtypes\ndf.info()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSelect and Subset Columns by Name\n\nhead()[ ][ [] ]\n\n\n\nWe can use the .head() method of a DataFrame to look at the first 5 rows of our data.\n\nThis is useful to see if our data loaded properly, and to get a better sense of the columns and contents.\n\n\ndf.head()\n\n\n\nIf we want only a specific column from our data, we can access the data using square brackets, [ ].\n\n# just get the country column and save it to its own variable\ncountry_df = df['country']\n# show the first 5 observations\ncountry_df.head()\n\n\n\nIn order to specify multiple columns by the column name, we need to pass in a Python list between the square brackets.\n\n# Looking at country, continent, and year\nsubset = df[['country', 'continent', 'year']]\nsubset"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-1",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-1",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSingle Value Returns DataFrame or Series\n\nSeriesDataFrame\n\n\n\nWhen we first select a single column, a Series object comes out.\n\ncountry_df = df['country']\ntype(country_df)\ncountry_df\n\n\n\nCompare df['country'] with df[ ['country'] ]\n\ncountry_df_list = df[ ['country'] ]\ntype(country_df_list)\ncountry_df_list\n\nIf we use a list to subset, we will always get a DataFrame object back."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-2",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-2",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nUsing Dot Notation to Pull a Column of Values\n\nThere is a shorthand notation where we can pull the column by treating it as a DataFrame attribute, or using dot (.).\n\ndf['country']\ndf.country\n\nWe do have to be mindful of what our columns are named if we want to use the dot notation.\n\nIf there is a column named shape, the df.shape will return the number of rows and columns\nIf our column name has spaces or special characters, we will not be able to use the dot notation to select that column of values."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-3",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-3",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubset Rows\n\nRows can be subset in multiple ways, by row name or row index."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-4",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-4",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubset Rows by index Label: .loc[]\n\nIndex.loc[]last row.tail()multiple rows\n\n\n\nLet’s take a look at our gapminder data:\n\ndf\n\nWe can see on the left side of the printed DataFrame, what appear to be row numbers.\n\nThis column-less row of values is the “index” label of the DataFrame.\n\nBy default, Pandas fills in the index labels with the row numbers starting from 0.\n\n\n\n\nWe can use the .loc[] attribute on the DataFrame to subset rows based on the index label.\n\n:::: {.columns} ::: {.column width=“50%”}\n# get the first row\ndf.loc[0]\n\n# get the 100th row\ndf.loc[99]\n\n# get the last row\ndf.loc[ -1]\n\nDoes df.loc[ -1] work?\n\n\n\n\n\nHere, passing -1 to the .loc[] causes an error.\n\nIt is actually looking for the row index label (i.e., row number -1), which does not exist in our example DataFrame.\n\n\n# use the first value given from shape to get the number of rows\nnumber_of_rows = df.shape[0]\n# subtract 1 from the value since we want the last index value\nlast_row_index = number_of_rows - 1\n\n# finally do the subset using the index of the last row\ndf.loc[last_row_index]\n\n\n\nWe can use the .tail() method to return the last n = 1 row, instead of the default 5.\n\ndf.tail(n = 1)\n\nUsing .tail() and .loc[] prints out the results differently.\n\ntype( df.tail(n = 1) )\ntype( df.loc[last_row_index] )\n\n\n\nWe can filter multiple rows.\n\ndf.loc[ [0, 99, 999] ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-5",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-5",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubset Rows by Row Number: .iloc[]\n\n.iloc[] does the same thing as .loc[], but is used to subset by the row index number.\nIn our current example, gapminder DataFrame, .iloc[] and .loc[] behave exactly the same way because the index labels are the same as the row numbers.\n\n\n\n# get the 2nd row\ndf.iloc[1]\n\n# get the 100th row\ndf.iloc[99]\n\n# get the last row\ndf.iloc[ -1]. # does it work?\n\n# get the first, 100th, and 1000th row\ndf.iloc[ [0, 99, 999] ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-6",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-6",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nMix It Up\n\nWe can use .loc[] and .iloc[] to obtain rows, columns, or both.\nThe general syntax for .loc[] and .iloc[] uses square brackets with a comma.\ndf.loc[ [rows], [columns] ]\ndf.iloc[ [rows], [columns] ]\nTo just subset columns, we can use the slicing methods.\n\nIf we are subsetting columns, we are getting all the rows for the specified column.\nSo, we need a method to capture all the rows.\nIf we have just a colon (:) when using slicing methods, it “slices” all the values in that axis."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-7",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-7",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSelecting Columns\n\nWe can write df.loc[:, [columns]] or df.iloc[:, [columns]] to subset the column(s).\n\n# subset columns with loc\nsubset_1 = df.loc[:, ['year', 'pop']]\nsubset_1\n\n# subset columns with iloc\n# iloc will allow us to use integers -1 to select the last column\nsubset_2 = df.iloc[:, [2, 4, -1]]\nsubset_2\n\n# do the followings work?\nsubset = df.loc[:, [2, 4, -1]] \nsubset = df.iloc[:, ['year', 'pop']]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-8",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-8",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with range()\n\nWe can use the built-in range() function to create a range of integer values.\n\nBy default, range(start, stop) creates all integer values between the beginning and the end (inclusive left, exclusive right).\nFYI, we can also pass in a 3rd parameter into range(start, stop, step), step, that allows us to change how to increment between the start and stop values (defaults to step=1).\n\n\n# create a range of integers from 0 to 4 inclusive\nsmall_range = list( range(5) )   # equivalent to range(0, 5)\nsmall_range\n# subset the dataframe with the range\nsubset = df.iloc[:, small_range]\nsubset"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-9",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-9",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with range()\n\nLet’s consider one more example with range():\n\n#  create a range from 3 to 5 inclusive\nsmall_range = list( range(3, 6) )\n\nsubset = df.iloc[:, small_range]\nsubset\n\nQ. What happens when you specify a range() that’s beyond the number of columns you have?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-10",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-10",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with Slicing :\n\nUnlike the range() function, the slicing method separates the values with the colon within the square bracket, [start : end : step ].\nLet’s see the columns of the DataFrame:\n\ndf.columns\n\nSee how range() and : are used to slice the first 3 columns.\n\nsmall_range = list( range(3) )\nsubset_1 = df.iloc[ : , small_range ]\n\nsubset_2 = df.iloc[ : , :3 ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-11",
    "href": "danl-lec/danl-m1-lec-02-2024-0213.html#look-at-columns-rows-and-cells-11",
    "title": "Lecture 2",
    "section": "Look at Columns, Rows, and Cells",
    "text": "Look at Columns, Rows, and Cells\nSubsetting with Slicing :\nQ. Let’s slice the columns 3 to 5 inclusive using (1) the range() function and (2) the slicing method.\nQ. What happens if we use the slicing method with 2 colons, but leave a value out? For example:\n df.iloc[: , 0 : 6 :   ]\n df.iloc[: , 0 :   : 2 ]\n df.iloc[: ,   : 6 : 2 ]\n df.iloc[: ,   :   : 2 ]\n df.iloc[: ,   :   :   ]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-1",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\n\n\nIn a tidy data.frame,\n\nA variable is in a column.\nAn observation is in a row.\nA value are in a cell.\n\n\nTidy data is a framework to structure data sets so they can be easily analyzed and visualized."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-2",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Values, Not Variables\nData can have columns that contain values instead of variables.\n\nLet’s consider data on income and religion in the United States from the Pew Research Center\n\nimport pandas as pd\npew = pd.read_csv('https://bcdanl.github.io/data/pew.csv')\n\nNot every column here is a variable.\n\nThe values that relate to income are spread across multiple columns.\n\n\nFor data analysis, the pew can be reshaped so that we have religion, income, and count variables."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-3",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Values, Not Variables\n## Show only the first few columns\npew.iloc[:,0:5]\nThe form of the data like pew is known as “wide” data. - To turn it into the “long” tidy data format, we will have to melt our DataFrame."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-4",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-4",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nPandas DataFrames have a method called .melt() that will reshape the DataFrame into a tidy format and it takes a few parameters: - id_vars is a container (list, tuple, ndarray) that represents the variables that will remain as is. - value_vars identifies the columns you want to melt down (or unpivot). - By default, it will melt all the columns not specified in the id_vars parameter. - var_name is a string for the new column name when the value_vars is melted down. - By default, it will be called variable. - value_name is a string for the new column name that represents the values for the var_name. - By default, it will be called value."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-5",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-5",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Values, Not Variables\n\n(1)(2)\n\n\n## we do not need to specify a value_vars since we want to pivot\n## all the columns except for the 'religion' column\npew_long = pew.melt(id_vars='religion')\npew_long\n\n## The .melt() method also exists as a pandas function, pd.melt()\n## The below line of code is the equivalent one:\n## melt function\npew_long = pd.melt(pew, id_vars='religion')\n\n\n\nWe can change the defaults so that the melted/unpivoted columns are named.\n\npew_long = pew.melt(\n  id_vars =\"religion\", var_name=\"income\", value_name =\"count\"\n)\n\npew_long"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-6",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-6",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nKeep Multiple Columns Fixed\n\n(1)(2)\n\n\nNot every data set will have one column to hold still while we unpivot the rest of the columns. - Let’s consider the Billboard data set.\nbillboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')\nEach week has its own column. - What if we want to create a faceted plot of the weekly ratings?\n\n\n## use a list to reference more than 1 variable\nbillboard_long = billboard.melt(\n  id_vars = [\"year\", \"artist\", \"track\", \"time\", \"date.entered\"],\n  var_name = \"week\",\n  value_name = \"rating\",\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-7",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#tidy-data-7",
    "title": "Lecture 6",
    "section": "Tidy Data",
    "text": "Tidy Data\nColumns Contain Multiple Variables\n\n(1)(2)(3)(4)(5)(6)(2)(3)(4)(5)\n\n\nSometimes columns in a data set may represent multiple variables. - Let’s look at the Ebola data set.\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv')\nebola.columns\nebola.iloc[ :5, [0, 1, 2, 10] ]\n\n\nThe column names Cases_Guinea and Deaths_Guinea actually contain two variables. - The individual status (cases and deaths, respectively) as well as the country name, Guinea. - The data is also arranged in a wide format that needs to be reshaped (with the .melt() method). - First, let’s fix the problem by melting the data into long format.\nebola_long = ebola.melt(id_vars=['Date', 'Day'])\n\n\n\nIn this case, we can split the column of interest based on the underscore, _.\n\nWe can use the .split() method that takes a string and “splits” it up based on a given delimiter.\nTo get access to the string methods, we need to use the .str. attribute.\n\n\n## split the column based on a delimiter\nvariable_split = ebola_long.variable.str.split('_')\nvariable_split\ntype(variable_split); type(variable_split[0])\n\n\n\nNow that the column has been split into various pieces, the next step is to assign those pieces to a new column.\n\nTo do so, we can use the .get() string method to “get” the index we want for each row.\n\n\nstatus_values = variable_split.str.get(0)\ncountry_values = variable_split.str.get(1)\n\n\n\nNow that we have the vectors we want, we can add them to our DataFrame.\n\nebola_long['status'] = status_values\nebola_long['country'] = country_values\nebola_long\n\n\n\nIn the .split() method, there is a parameter expand that defaults to False.\n\nWhen we set it to True, it will return a DataFrame where each result of the split is in separate columns. ```{.python} ## reset our ebola_long data ebola_long = ebola.melt(id_vars =[‘Date’, ‘Day’]) ## split the column by _ into a dataframe using expand variable_split = ebola_long.variable.str.split(’_’, expand=True)\n\n\nebola_long[[‘status’, ‘country’]] = variable_split\n\n\n\n::: \n\n\n\n## Tidy Data\n### Variables in Both Rows and Columns \n\n::: {.panel-tabset}\n## (1)\n\nWhat happens if a column of data actually holds two variables instead of one variable?\n  -  In this case, we will have to `pivot` the variable into separate columns, i.e., go from \"long\" data to \"wide\" data.\n\n```{.python}\nweather = pd.read_csv('https://bcdanl.github.io/data/weather.csv')\n\n\nThe weather data include minimum (tmin) and maximum (tmax) temperatures recorded for each day (d1, d2, … , d31) of the month (month). - The element column contains variables that may need to be pivoted wider to become new columns, - The day variables may need to be melted into row values.\nLet’s first fix the day values.\nweather_melt = weather.melt(\n  id_vars=[\"id\", \"year\", \"month\", \"element\"],\n  var_name=\"day\",\n  value_name=\"temp\",\n)\n\n\nNext, we need to pivot up the variables stored in the element column.\nweather_tidy = weather_melt.pivot_table(\n    index = ['id', 'year', 'month', 'day'],\n    columns = 'element',\n    values = 'temp'\n)\n\n\nWe can also flatten the hierarchical columns.\nweather_tidy_flat = weather_tidy.reset_index()\n\n\nFor day variable, we can replace ‘d’ with ““. - Then, convert day from string to integer. - Then, sort weather_tidy_flat by ['year', 'month', 'day'].\nweather_tidy_flat['day'] = weather_tidy_flat.day.str.replace('d', \"\")\nweather_tidy_flat['day'] = weather_tidy_flat['day'].astype(int)\nweather_tidy_flat = weather_tidy_flat.sort_values(by = ['year', 'month', 'day'])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-1",
    "title": "Lecture 6",
    "section": "Data Types",
    "text": "Data Types\nLet’s consider the built-in tips DataFrame from the seaborn library.\nTo get a list of the data types stored in each column of our DataFrame, we call the dtypes attribute.\nimport seaborn as sns\ntips = sns.load_dataset(\"tips\")\ntips.dtypes\n\nThe category data type represents categorical variables.\n\nIt differs from the generic object data type that stores arbitrary Python objects (usually strings)."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-2",
    "title": "Lecture 6",
    "section": "Data Types",
    "text": "Data Types\nConverting Types\n## convert the category sex column into a string dtype\ntips['sex_str'] = tips['sex'].astype(str)\n\n## convert total_bill into a string\ntips['total_bill'] = tips['total_bill'].astype(str)\n\n## convert it back to a float\ntips['total_bill'] = tips['total_bill'].astype(float)\n\n## convert sex into a string\ntips['sex'] = tips['sex'].astype(str)\n\n## convert it back to a category\ntips['sex'] = tips['sex'].astype(category)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-types-3",
    "title": "Lecture 6",
    "section": "Data Types",
    "text": "Data Types\nto_numeric() method\n\n(1)(2)(3)(4)\n\n\nWhen converting variables into numeric values (e.g., int, float), we can also use the Pandas to_numeric() function. - It handles non-numeric values better.\n\n## subset the tips data\ntips_sub_miss = tips.head(10).copy()\n## assign some 'missing' values\ntips_sub_miss.loc[[1, 3, 5, 7], 'total_bill'] = 'missing'\n\n\n## this will cause an error\ntips_sub_miss['total_bill'].astype(float)\n\n## this will cause an error\npd.to_numeric(tips_sub_miss['total_bill'])\n\n\nThe to_numeric() function has a parameter called errors that governs what happens when the function encounters a value that it is unable to convert to a numeric value. - 'raise': Default. - 'coerce': Invalid parsing will be set as NaN - 'ignore': Invalid parsing will return the input as is.\n\n\ntips_sub_miss[\"total_bill\"] = pd.to_numeric(\n    tips_sub_miss[\"total_bill\"], errors=\"ignore\"\n)\n\ntips_sub_miss[\"total_bill\"]=pd.to_numeric(\n    tips_sub_miss[\"total_bill\"], errors=\"coerce\"\n)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-assembly-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-assembly-1",
    "title": "Lecture 6",
    "section": "Data Assembly",
    "text": "Data Assembly\n\nSometimes, we better combine various DataFrames together to analyze a set of data.\n\nThe data may have been split up into separate DataFrames to reduce the amount of redundant information.\ne.g., DataFrame for county-level data and DataFrame for geographic information, such as longitude and latitude"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\n\nConcatenation can be thought of as appending a row or column to our data. - This approach is possible if our data was split into parts or if we performed a calculation that we want to append to our existing data set. - Let’s consider the following example DataFrames:\ndf1 = pd.read_csv('https://bcdanl.github.io/data/concat_1.csv')\ndf2 = pd.read_csv('https://bcdanl.github.io/data/concat_2.csv')\ndf3 = pd.read_csv('https://bcdanl.github.io/data/concat_3.csv')\n\nWe will be working with .index and .columns in this Section.\n\ndf1.index\ndf1.columns\ndf1.values"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-1",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\nConcatenating the DataFrames on top of each other uses the concat() function. - All of the DataFrames to be concatenated are passed in a list.\nrow_concat = pd.concat([df1, df2, df3])\nrow_concat\n\nThe row names (i.e., the row indices) are simply a stacked version of the original row indices.\n\nrow_concat.iloc[3, :]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-2",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\n\nLet’s consider a new Series and concatenate it with df1:\n\n## create a new row of data\nnew_row_series = pd.Series(['n1', 'n2', 'n3', 'n4'])\nnew_row_series\n\n\n## attempt to add the new row to a dataframe\ndf = pd.concat([df1, new_row_series])\ndf\n\nNot only did our code not append the values as a row, but it also created a new column completely misaligned with everything else.\nWhy?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-3",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\nTo fix the problem, we need turn our series into a DataFrame. - This data frame contains one row of data, and the column names are the ones the data will bind to.\nnew_row_df = pd.DataFrame(\n  ## note the double brackets to create a \"row\" of data\n  data =[[\"n1\", \"n2\", \"n3\", \"n4\"]],\n  columns =[\"A\", \"B\", \"C\", \"D\"],\n)\n\ndf = pd.concat([df1, new_row_df])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-4",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-4",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nIgnore the Index\n\nWe can use the ignore_index parameter to reset the row index after the concatenation if we simply want to concatenate or append data together.\n\nrow_concat_i = pd.concat([df1, df2, df3], ignore_index=True)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-5",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-5",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Columns\nConcatenating columns is very similar to concatenating rows. - The main difference is the axis parameter in the concat function. - The default value of axis is 0 (or axis = \"index\"), so it will concatenate data in a row-wise fashion. - If we pass axis = 1 (or axis = \"columns\") to the function, it will concatenate data in a column-wise manner.\ncol_concat = pd.concat([df1, df2, df3], axis = \"columns\")"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-6",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-6",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Columns\n\nAdding a single column to a dataframe can be done directly without using any specific Pandas function.\n\ncol_concat['new_col_list'] = ['n1', 'n2', 'n3', 'n4']\n\nWe can reset the column indices so we do not have duplicated column names.\n\npd.concat([df1, df2, df3], axis=\"columns\", ignore_index=True)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-7",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-7",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\nWhat would happen when the row and column indices are not aligned?\n\nLet’s modify our DataFrames for the next few examples.\n\n## rename the columns of our dataframes\ndf1.columns = ['A', 'B', 'C', 'D']\ndf2.columns = ['E', 'F', 'G', 'H']\ndf3.columns = ['A', 'C', 'F', 'H']\n\nIf we try to concatenate these DataFrames as we did, the DataFrames now do much more than simply stack one on top of the other.\n\nrow_concat = pd.concat([df1, df2, df3])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-8",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-8",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\n\nWe can set join = 'inner' to keep only the columns that are shared among the data sets.\n\npd.concat([df1, df2, df3], join ='inner')\n\nIf we use the DataFrames that have columns in common, only the columns that all of them share will be returned.\n\npd.concat([df1, df3], join ='inner',  ignore_index =False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-9",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-9",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\n\nLet’s modify our DataFrames further.\n\n## re-indexing the rows of our dataframes\ndf1.index = [0, 1, 2, 3]\ndf2.index = [4, 5, 6, 7]\ndf3.index = [0, 2, 5, 7]"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-10",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#data-concatenation-10",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nConcatenate with Different Indices\n\nWhen we concatenate along axis=\"columns\" (axis=1), the new DataFrames will be added in a column-wise fashion and matched against their respective row indices.\n\ncol_concat = pd.concat([df1, df2, df3], axis=\"columns\")\n\nJust as we did when we concatenated in a row-wise manner, we can choose to keep the results only when there are matching indices by using join=\"inner\".\n\npd.concat([df1, df3], axis =\"columns\", join='inner')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data",
    "title": "Lecture 6",
    "section": "Relational data",
    "text": "Relational data\nWhy is one data set sometimes scattered across multiple files? - The size of the files can be huge. - The data collection process can be scattered across time and space.\nSometimes we may have two or more DataFrames (tables) that we want to combine based on common data values. - This task is known in the database world as performing a “join.” - We can do this with the .merge() method in Pandas."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-1",
    "title": "Lecture 6",
    "section": "Relational data",
    "text": "Relational data\n\nThe variables that are used to connect each pair of tables are called keys."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#relational-data-2",
    "title": "Lecture 6",
    "section": "Relational data",
    "text": "Relational data\nMerges\n\n\n\nx = pd.DataFrame({\n    'key': [1, 2, 3],\n    'val_x': ['x1', 'x2', 'x3']\n})\n\ny = pd.DataFrame({\n    'key': [1, 2, 4],\n    'val_y': ['y1', 'y2', 'y3']\n})\n\n\n\nThe colored column represents the “key” variable.\nThe grey column represents the “value” column."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-1",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-1",
    "title": "Lecture 6",
    "section": "Merges",
    "text": "Merges\n\ninnerleftrightouter full\n\n\n\nAn inner join matches pairs of observations whenever their keys are equal:\n\n\n\n\n\n\n\n\n\n\n## the default value for 'how' is 'inner'\n## so it doesn't actually need to be specified\nmerge_inner = pd.merge(x, y, on='key', how='inner')\nmerge_inner_x = x.merge(y, on='key', how='inner')\nmerge_inner_x_how = x.merge(y, on='key')\n\n\n\nA left join keeps all observations in x.\n\n\n\n\n\n\n\n\n\n\nmerge_left = pd.merge(x, y, on='key', how='left')\nmerge_left_x = x.merge(y, on='key', how='left')\n\nThe most commonly used join is the left join.\n\n\n\n\nA right join keeps all observations in y.\n\n\n\n\n\n\n\n\n\n\nmerge_right = pd.merge(x, y, on='key', how='right')\nmerge_right_x = x.merge(y, on='key', how='right')\n\n\n\nA full join keeps all observations in x and y.\n\n\n\n\n\n\n\n\n\n\nmerge_outer = pd.merge(x, y, on='key', how='outer')\nmerge_outer_x = x.merge(y, on='key', how='outer')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-2",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-2",
    "title": "Lecture 6",
    "section": "Merges",
    "text": "Merges\nDuplicate keys\n\none-to-manymany-to-many\n\n\n\nOne data frame has duplicate keys (a one-to-many relationship).\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = pd.DataFrame({\n    'key': [1, 2, 3],\n    'val_x': ['x1', 'x2', 'x3'] })\n\n\n\ny = pd.DataFrame({\n    'key': [1, 2, 4],\n    'val_y': ['y1', 'y2', 'y3'] })\none_to_many = x.merge(y, on='key', how='left')\n\n\n\n\n\n\nBoth data frames have duplicate keys (many-to-many relationship).\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = pd.DataFrame({\n  'key': [1, 2, 2, 3],\n  'val_x': ['x1', 'x2', 'x3', 'x4'] })\n\n\n\ny = pd.DataFrame({\n  'key': [1, 2, 2, 3],\n  'val_y': ['y1', 'y2', 'y3', 'y4'] })\nmany_to_many = x.merge(y, on='key', how='left')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-3",
    "href": "danl-lec/danl-m1-lec-06-2024-0312.html#merges-3",
    "title": "Lecture 6",
    "section": "Merges",
    "text": "Merges\nDefining the key columns\n\nIf the left and right columns do not have the same name for the key columns, we can use the left_on and right_on parameters instead.\n\n\n\n\nx = pd.DataFrame({\n  'key_x': [1, 2, 3],\n  'val_x': ['x1', 'x2', 'x3']\n})\n\n\n\ny = pd.DataFrame({\n  'key_y': [1, 2],\n  'val_y': ['y1', 'y2'] })\n\nkeys_xy = x.merge(y, \n                  left_on='key_x', \n                  right_on = 'key_y', \n                  how='left')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-1",
    "title": "Lecture 7",
    "section": "Missing Data",
    "text": "Missing Data\nRarely will we be given a data set without any missing values.\nPandas usually displays missing values as NaN. - NaN is the actual representation of “Not a Number” values. - I would prefer calling it “Not Available” (NA), for which some other programming languages as well as Pandas use to represent missing values."
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-2",
    "title": "Lecture 7",
    "section": "Missing Data",
    "text": "Missing Data\n\nThe NaN value in Pandas comes from Numpy.\n\nfrom numpy import NaN\nNaN == True\nNaN == 0\nNaN == \"\nNaN == NaN"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-3",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#missing-data-3",
    "title": "Lecture 7",
    "section": "Missing Data",
    "text": "Missing Data\n\nPandas has functions to test for missing values, isnull().\nPandas also has functions for testing non-missing values, notnull().\n\nimport pandas as pd\npd.isnull(NaN)\npd.notnull(NaN)\npd.notnull(210)\npd.notnull('missing')"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from",
    "title": "Lecture 7",
    "section": "Where Can Missing Values Come From?",
    "text": "Where Can Missing Values Come From?\nLoad Data\n\nread_csv()na_valueskeep_default_na\n\n\n\nWhen we load the data, Pandas automatically finds the missing data cell and give us a DataFrame with the NaN value in the appropriate cell.\nIn the read_csv() function, three parameters are related to reading missing values: na_values and keep_default_na.\n\n\n\nThe na_values parameter allows us to specify additional missing or NaN values.\n\nWe can pass in either a Python str (i.e., string) or a list-like object to be automatically coded as missing values when the file is read.\n\npath = 'https://bcdanl.github.io/data/survey_visited.csv'\nsurvey_visited_0 = pd.read_csv(path)\nsurvey_visited_1 = pd.read_csv(path, na_values = [\"MSK-4\"])\n\n\nThe keep_default_na parameter is a bool (i.e., True or False boolean) that allows us to specify whether any additional values need to be considered as missing.\n\nkeep_default_na = False will only use the missing values specified in na_values.\n\nsurvey_visited_2 = pd.read_csv(path, keep_default_na = False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-1",
    "title": "Lecture 7",
    "section": "Where Can Missing Values Come From?",
    "text": "Where Can Missing Values Come From?\nMerged Data\n\nLet’s merge survey_visited_0 with survey.\n\npath = 'https://bcdanl.github.io/data/survey_survey.csv'\nsurvey = pd.read_csv(path)\nsurvey\n\nvs = survey_visited_0.merge(survey, left_on='ident', right_on='taken')\nvs"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#where-can-missing-values-come-from-2",
    "title": "Lecture 7",
    "section": "Where Can Missing Values Come From?",
    "text": "Where Can Missing Values Come From?\nReindexing\n\n(1)(2)\n\n\nAnother way to introduce missing values into our data is to reindex our dataframe.\n\nThis is useful when we want to add new indices to your dataframe, but still want to retain its original values.\nA common usage is when the index represents some time interval, and we want to add more dates.\n\ngapminder = pd.read_csv('https://bcdanl.github.io/data/gapminder.tsv', sep='\\t')\nlife_exp = gapminder.groupby(['year'])['lifeExp'].mean()\n\n\n\nWe can reindex the dataframe by using the .reindex() method.\n\n## subset\ny2000 = life_exp[life_exp.index &gt; 2000]\n\n## reindexing\ny2000.reindex(range(2000, 2010))"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data",
    "title": "Lecture 7",
    "section": "Working With Missing Data",
    "text": "Working With Missing Data\nFind and Count Missing Data\n\n.count()np.count_nonzero().value_counts(dropna=False).sum()\n\n\nOne way to look at the number of missing values is to count() them.\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv')\n## count the number of non-missing values\nebola.count()\nQ. Count the number of non-missing values for each variable in ebola.\n\n\nIf we want to count the total number of missing values in our DataFrame, or count the number of missing values for a particular column, we can use the np.count_nonzero() function from numpy in conjunction with the .isnull() method.\nnp.count_nonzero(ebola.isnull())\nnp.count_nonzero(ebola['Cases_Guinea'].isnull())\n\n\nAnother way to get missing data counts is to use the .value_counts() method, giving a frequency table of values in a Series. - If we use the dropna = False, we can also get a missing value count.\ncnts = ebola['Cases_Guinea'].value_counts(dropna=False)\ncnts\n\nThe results are sorted so we can subset the count vector to just look at the missing values.\n\ncnts.loc[pd.isnull(cnts.index)]\n\n\n\nIn Python, True values equate to the integer value 1, and False values equate to the integer value 0.\n\nWe can use this behavior to get the number of missing values by summing up a boolean vector with the .sum() method.\n\n\nebola.Cases_Guinea.isnull().sum()"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-1",
    "title": "Lecture 7",
    "section": "Working With Missing Data",
    "text": "Working With Missing Data\nClean Missing Data\n\nCleaning NaN.fillna().fillna(method=‘ffill’).fillna(method=‘bfill’).interpolate().dropna()Calculationskipna\n\n\nThere are many different ways we can deal with missing data. 1. We can replace the missing value with another value, 2. We can fill in the cells with the missing value using existing value, 3. We can drop the observations with missing values from our DataFrame.\n\n\n\nWe can use the .fillna() method to recode the missing values to another value.\n\n## fill the missing values to 0\nebola0 = ebola.fillna(0)\n\n\n\nWe can use built-in methods to fill forward (method = ffill).\n\nWhen we fill data forward, the last known value (from top to bottom) is used for the next missing value.\n\n\nebola_f = ebola.fillna(method='ffill')\n\n\n\nWe can also use built-in methods to fill backward (method = bfill).\n\nWhen we fill data backward, the newest value (from top to bottom) is used to replace the missing data.\n\n\nebola_b = ebola.fillna(method='bfill')\n\n\n\nInterpolation uses existing values to fill in missing values.\n\nBy default, .interpolate() treats the missing values as if they should be equally spaced apart.\n\nebola_linear = ebola.interpolate()\nThe .interpolate() method behaves kind of in a forward fill fashion.\n\n\n\nIf we want to keep the observations with only non-missing values, we can use .dropna()\nebola_dropna = ebola.dropna()\nWe are left with just one row of data!\n\n\nSuppose we wanted to look at the case counts for multiple regions.\n\n\nebola[\"Cases_multiple\"] = (\n  ebola[\"Cases_Guinea\"]\n  + ebola[\"Cases_Liberia\"]\n  + ebola[\"Cases_SierraLeone\"]\n)\n\nebola_subset = ebola.loc[:,\n    [\"Cases_Guinea\", \n     \"Cases_Liberia\", \n     \"Cases_SierraLeone\",\n     \"Cases_multiple\"] ]\n\n\n\n\n.mean() and .sum() can ignore missing values. - These functions will typically have a skipna parameter that will still calculate a value by skipping over the missing values.\nebola.Cases_Guinea.sum(skipna = True) ## default\nebola.Cases_Guinea.sum(skipna = False)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#working-with-missing-data-2",
    "title": "Lecture 7",
    "section": "Working With Missing Data",
    "text": "Working With Missing Data\nPandas Built-In NA Missing\nPandas 1.0 introduced a built-in &lt;NA&gt; value (pd.NA).\n\neconomists = pd.DataFrame(\n  {\n    \"Name\": [\"John Forbes Nash\", \"William Nordhaus\"],\n    \"Occupation\": [\"Mathematician\", \"Climate Economist\"],\n    \"Born\": [\"1928-06-13\", \"1941-05-31\"],\n    \"Died\": [\"2015-05-23\", \"\"],\n    \"Age\": [86, 81]\n  }\n)\n\neconomists.loc[1, \"Age\"] = pd.NA"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nPython’s datetime Object\nOne of the bigger reasons for using Pandas is its ability to work with timeseries data.\n\nWe can use datetime to get the current date and time.\n\nfrom datetime import datetime\nnow = datetime.now()\n\nWe can also create our own datetime manually.\n\n\nt1 = datetime.now()\nt2 = datetime(2000,1,1)\n\ndiff = t1 - t2\ntype(diff)"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-1",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-1",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nConverting to datetime\n\ndata.to_datatime()formatparse_dates\n\n\n\nLet’s load up our Ebola data set and convert the Date column into a proper datetime object.\n\nimport pandas as pd\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv')\nebola.info()\n\nThe Date column is encoded as a generic string object.\n\n\n\n\nWe can use .to_datatime() to create a new column, date_dt, that converts the Date column into a datetime.\n\nebola['date_dt'] = pd.to_datetime(ebola['Date'])\n\n\n\nThe to_datetime() method has a parameter called format that allows us to manually specify the format of the date.\n\n\n\nThe read_csv() function has several parameters about datetime. - We can parse the Date column directly by specifying the column we want in the parse_dates parameter.\nebola = pd.read_csv('https://bcdanl.github.io/data/country_timeseries.csv', parse_dates=[\"Date\"])"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-2",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-2",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nExtracting Date Components\nNow that we have a datetime object, we can extract various parts of the date, such as year, month, or day.\nLet’s consider the example datetime object.\nd = pd.to_datetime('2021-12-14')\ntype(d)\nd.year\nd.month\nd.day\nd.quarter"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-3",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-3",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nExtracting Date Components in DataFrame\n\nWe can extract various parts of the datetime column in DataFrame by accessing datetime methods using the .dt accessor.\n\nebola['date_dt'] = pd.to_datetime(ebola['Date'])\n\nebola = ebola.assign(\n    year = ebola[\"date_dt\"].dt.year,\n    month = ebola[\"date_dt\"].dt.month,\n    day = ebola[\"date_dt\"].dt.day\n)\n\nebola.info() ## what are the data types of year, month, and day?"
  },
  {
    "objectID": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-4",
    "href": "danl-lec/danl-m1-lec-07-2024-0319.html#dates-and-times-4",
    "title": "Lecture 7",
    "section": "Dates and Times",
    "text": "Dates and Times\nDate Ranges\n\n(1)(2)(3)\n\n\n\nIn our Ebola data set, we do not have an observation for every day in the date range.\n\nThis is quite common.\n\n\nebola = pd.read_csv(\n'https://bcdanl.github.io/data/country_timeseries.csv', parse_dates=[\"Date\"]\n)\n\nHere, 2015-01-01 is missing.\n\n\n\n\nIt’s common practice to create a date range to .reindex() a data set.\n\nWe can use the date_range().\n\n\nhead_range = pd.date_range(start='2014-12-31', end='2015-01-05')\nebola_5 = ebola.head()\nebola_5.index = ebola_5['Date']\nebola_5 = ebola_5.reindex(head_range)\n\n\nebola = pd.read_csv(\n  \"https://bcdanl.github.io/data/country_timeseries.csv\",\n  index_col=\"Date\",\n  parse_dates=[\"Date\"],\n)\n\nnew_idx = pd.date_range(ebola.index.min(), ebola.index.max())\nnew_idx = reversed(new_idx) ## to reverse new_idx\nebola = ebola.reindex(new_idx)"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html",
    "href": "danl-cw/danl-m1-cw-5.html",
    "title": "Classwork 5",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#load-libraries",
    "href": "danl-cw/danl-m1-cw-5.html#load-libraries",
    "title": "Classwork 5",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#load-dataframe",
    "href": "danl-cw/danl-m1-cw-5.html#load-dataframe",
    "title": "Classwork 5",
    "section": "Load DataFrame",
    "text": "Load DataFrame\n\nbillboard = pd.read_csv('https://bcdanl.github.io/data/billboard.csv')\nny_pincp = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_wide.csv')\ncovid = pd.read_csv('https://bcdanl.github.io/data/covid19_cases.csv')"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q1a",
    "href": "danl-cw/danl-m1-cw-5.html#q1a",
    "title": "Classwork 5",
    "section": "Q1a",
    "text": "Q1a\n\nDescribe how the distribution of rating varies across week 1, week 2, and week 3 using the faceted histogram. \n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q1b",
    "href": "danl-cw/danl-m1-cw-5.html#q1b",
    "title": "Classwork 5",
    "section": "Q1b",
    "text": "Q1b\n\nWhich artist(s) have the most number of tracks in billboard DataFrame? \n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q1c",
    "href": "danl-cw/danl-m1-cw-5.html#q1c",
    "title": "Classwork 5",
    "section": "Q1c",
    "text": "Q1c\n\nMake ny_pincp longer.\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q1d",
    "href": "danl-cw/danl-m1-cw-5.html#q1d",
    "title": "Classwork 5",
    "section": "Q1d",
    "text": "Q1d\n\nMake a wide-form DataFrame of covid whose variable names are from countriesAndTerritories and values are from cases.\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q1e",
    "href": "danl-cw/danl-m1-cw-5.html#q1e",
    "title": "Classwork 5",
    "section": "Q1e",
    "text": "Q1e\n\nUse the wide-form DataFrame of covid to find the top 10 countries for which their cases are highly correlated with USA’s cases using DataFrame.corr()\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#load-dataframe-for-q2a-and-q2b",
    "href": "danl-cw/danl-m1-cw-5.html#load-dataframe-for-q2a-and-q2b",
    "title": "Classwork 5",
    "section": "Load DataFrame for Q2a and Q2b",
    "text": "Load DataFrame for Q2a and Q2b\n\npaidsearch = pd.read_csv('https://bcdanl.github.io/data/paidsearch.csv')\n\n\nVariable description\n\ndma: an identification number of a designated market (DM) area i (e.g., Boston, Los Angeles)\ntreatment_period: 0 if date is before May 22, 2012 and 1 after.\nsearch_stays_on: 1 if the paid-search goes off in dma i, 0 otherwise.\nrevenue: eBay’s sales revenue for dma i and date t"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q2a",
    "href": "danl-cw/danl-m1-cw-5.html#q2a",
    "title": "Classwork 5",
    "section": "Q2a",
    "text": "Q2a\nSummarize the mean vale of revenue for each group of search_stays_on and for each date.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q2b",
    "href": "danl-cw/danl-m1-cw-5.html#q2b",
    "title": "Classwork 5",
    "section": "Q2b",
    "text": "Q2b\nCalculate the log difference between mean revenues in each group of search_stays_on. (This is the log of the average revenue in group of search_stays_on == 1 minus the log of the average revenue in group of search_stays_on == 0.)\n\nFor example, consider the following two observations:\nThe log difference of daily mean revenues between the two group of search_stays_on for date 1-Apr-12 is log(120277.57) - log(93650.68).\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#load-dataframe-for-q2c-q2d-and-q2e",
    "href": "danl-cw/danl-m1-cw-5.html#load-dataframe-for-q2c-q2d-and-q2e",
    "title": "Classwork 5",
    "section": "Load DataFrame for Q2c, Q2d, and Q2e",
    "text": "Load DataFrame for Q2c, Q2d, and Q2e\n\npaid_search = pd.read_csv('https://bcdanl.github.io/data/paid_search.csv')"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q2c",
    "href": "danl-cw/danl-m1-cw-5.html#q2c",
    "title": "Classwork 5",
    "section": "Q2C",
    "text": "Q2C\nSort paid_search by DM and May22_2012 in ascending order.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q2d",
    "href": "danl-cw/danl-m1-cw-5.html#q2d",
    "title": "Classwork 5",
    "section": "Q2d",
    "text": "Q2d\nFor each DM, calculate the difference between log_revenue before and after May22_2012.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-5.html#q2e",
    "href": "danl-cw/danl-m1-cw-5.html#q2e",
    "title": "Classwork 5",
    "section": "Q2e",
    "text": "Q2e\n\nConsider the DataFrame from Q2d.\nFor each group of no_paid_search, calculate the mean value of the difference between log_revenue before and after May22_2012 .\nWhat is the difference in the mean values?\nAfter the paid-search went off, sales revenue decreased by 0.66%\n\nWas eBay’s paid search worth it?\n\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html",
    "href": "danl-cw/danl-m1-cw-6.html",
    "title": "Classwork 6",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#load-libraries",
    "href": "danl-cw/danl-m1-cw-6.html#load-libraries",
    "title": "Classwork 6",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q1a",
    "href": "danl-cw/danl-m1-cw-6.html#q1a",
    "title": "Classwork 6",
    "section": "Q1a",
    "text": "Q1a\nWrite a Pandas code to join the two given DataFrames along rows and assign all data.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q1b",
    "href": "danl-cw/danl-m1-cw-6.html#q1b",
    "title": "Classwork 6",
    "section": "Q1b",
    "text": "Q1b\nWrite a Pandas code to join the two given DataFrames along columns and assign all data.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q1c",
    "href": "danl-cw/danl-m1-cw-6.html#q1c",
    "title": "Classwork 6",
    "section": "Q1c",
    "text": "Q1c\nConsider the following Pandas Series\nWrite a Pandas code to append rows to DataFrame student_data1 and display the combined data using DATAFRAME.append(SERIES, ignore_index = True)\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q2a",
    "href": "danl-cw/danl-m1-cw-6.html#q2a",
    "title": "Classwork 6",
    "section": "Q2a",
    "text": "Q2a\nMerge flights with weather.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q2b",
    "href": "danl-cw/danl-m1-cw-6.html#q2b",
    "title": "Classwork 6",
    "section": "Q2b",
    "text": "Q2b\nFind the airline that has the longest positive dep_delay on average.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-6.html#q2c",
    "href": "danl-cw/danl-m1-cw-6.html#q2c",
    "title": "Classwork 6",
    "section": "Q2c",
    "text": "Q2c\nFind the airline that has the largest proportion of flights with longer than 30-minute dep_delay.\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-m1-cw-3.html",
    "href": "danl-cw/danl-m1-cw-3.html",
    "title": "Classwork 3",
    "section": "",
    "text": "Load Data\n\nimport pandas as pd\ndf_ny = pd.read_csv('https://bcdanl.github.io/data/NY_pinc_pop.csv')\ndf_ny.head(10)\n\n\nVariable Description\n\nFIPS: ID number for a county\npincp: average personal income in a county X in year Y\npop_18_24: population 18 to 24 years\npop_25_over: population 25 years and over\n\n\n\n\nQ1a\n\nUse .sort_values() to find the top 5 rich counties in NY for each year.\n\nDo not use .apply().\n\n\n\nAnswer\n\n\nQ1b\n\nUse .rank() to find the top 5 rich counties in NY for each year.\n\nDo not use apply().\n\n\n\nAnswer\n\n\nQ1c\n\nUse apply() with a lambda function and .sort_values() to find the top 5 rich counties in NY for each year. \n\nAnswer\n\n\nQ1d\n\nWrite a function with def and .sort_values() that selects the top 5 pincp values.\nThen, use the defined function in apply() to find the top 5 rich counties in NY for each year. \n\nAnswer\n\n\nQ1e\n\nVisualize the yearly trend of the mean level of pincp. \n\nAnswer\n\n\n\n\n Back to topReuse© 2024 Byeong-Hak Choe | This post is licensed under &lt;a href='http://creativecommons.org/licenses/by-nc-sa/4.0/' target='_blank'&gt;CC BY-NC-SA 4.0&lt;/a&gt;.CitationFor attribution, please cite this work as:\nChoe, Byeong-Hak. 2024. “Classwork 3.” February 20, 2024."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DANL Module 1: Data Preparation and Management, 2024",
    "section": "",
    "text": "Welcome! 👋\n\\(-\\) Explore, Learn, and Grow with the DANL Microcredential! 🌟"
  },
  {
    "objectID": "index.html#bullet-lecture-slides",
    "href": "index.html#bullet-lecture-slides",
    "title": "DANL Module 1: Data Preparation and Management, 2024",
    "section": "\\(\\bullet\\,\\) Lecture Slides 🚀",
    "text": "\\(\\bullet\\,\\) Lecture Slides 🚀\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nLecture 1\n\n\nFebruary 6, 2024\n\n\n\n\nLecture 2\n\n\nFebruary 13, 2024\n\n\n\n\nLecture 3\n\n\nFebruary 20, 2024\n\n\n\n\nLecture 4\n\n\nFebruary 27, 2024\n\n\n\n\nLecture 5\n\n\nMarch 5, 2024\n\n\n\n\nLecture 6\n\n\nMarch 12, 2024\n\n\n\n\nLecture 7\n\n\nMarch 19, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-classwork",
    "href": "index.html#bullet-classwork",
    "title": "DANL Module 1: Data Preparation and Management, 2024",
    "section": "\\(\\bullet\\,\\) Classwork ⌨️",
    "text": "\\(\\bullet\\,\\) Classwork ⌨️\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nClasswork 1\n\n\nFebruary 6, 2024\n\n\n\n\nClasswork 2\n\n\nFebruary 13, 2024\n\n\n\n\nClasswork 3\n\n\nFebruary 20, 2024\n\n\n\n\nClasswork 4\n\n\nFebruary 27, 2024\n\n\n\n\nClasswork 5\n\n\nMarch 5, 2024\n\n\n\n\nClasswork 6\n\n\nMarch 12, 2024\n\n\n\n\nClasswork 7\n\n\nMarch 19, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-weekly-q-a",
    "href": "index.html#bullet-weekly-q-a",
    "title": "DANL Module 1: Data Preparation and Management, 2024",
    "section": "\\(\\bullet\\,\\) Weekly Q & A ❓",
    "text": "\\(\\bullet\\,\\) Weekly Q & A ❓\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1 - Q & A\n\n\nFebruary 6, 2024\n\n\n\n\nWeek 2 - Q & A\n\n\nFebruary 13, 2024\n\n\n\n\nWeek 3 - Q & A\n\n\nFebruary 20, 2024\n\n\n\n\nWeek 4 - Q & A\n\n\nFebruary 27, 2024\n\n\n\n\nWeek 5 - Q & A\n\n\nMarch 5, 2024\n\n\n\n\nWeek 6 - Q & A\n\n\nMarch 12, 2024\n\n\n\n\nWeek 7 - Q & A\n\n\nMarch 19, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-homework",
    "href": "index.html#bullet-homework",
    "title": "DANL Module 1: Data Preparation and Management, 2024",
    "section": "\\(\\bullet\\,\\) Homework 💻",
    "text": "\\(\\bullet\\,\\) Homework 💻\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nHomework 1\n\n\nFebruary 6, 2024\n\n\n\n\nHomework 2\n\n\nFebruary 13, 2024\n\n\n\n\nHomework 3\n\n\nFebruary 20, 2024\n\n\n\n\nHomework 4\n\n\nFebruary 27, 2024\n\n\n\n\nHomework 5\n\n\nMarch 5, 2024\n\n\n\n\nHomework 6\n\n\nMarch 12, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-06.html",
    "href": "danl-qa/danl-m1-qa-06.html",
    "title": "Week 6 - Q & A",
    "section": "",
    "text": "Welcome to our Week 6 Discussion Board! 👋 \nThis space is designed for you to engage with your classmates about the material covered in Week 6, including Lecture 6 slides, Classwork 6, and Homework Assignment 6.\nWhether you are looking to delve deeper into the slides, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Week 6 materials or need clarification on any points, don’t hesitate to ask here.\nLet’s collaborate and learn from each other!\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-05.html",
    "href": "danl-qa/danl-m1-qa-05.html",
    "title": "Week 5 - Q & A",
    "section": "",
    "text": "Welcome to our Week 1 Discussion Board! 👋 \nThis space is designed for you to engage with your classmates about the material covered in Week 5, including Lecture 5 slides, Classwork 5, and Homework Assignment 5.\nWhether you are looking to delve deeper into the slides, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Week 5 materials or need clarification on any points, don’t hesitate to ask here.\nLet’s collaborate and learn from each other!\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-m1-qa-02.html",
    "href": "danl-qa/danl-m1-qa-02.html",
    "title": "Week 2 - Q & A",
    "section": "",
    "text": "Welcome to our Week 2 Discussion Board! 👋 \nThis space is designed for you to engage with your classmates about the material covered in Week 2, including Lecture 2 slides, Classwork 2, and Homework Assignment 2.\nWhether you are looking to delve deeper into the slides, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Week 2 materials or need clarification on any points, don’t hesitate to ask here.\nLet’s collaborate and learn from each other!\n\n\n\n Back to top"
  },
  {
    "objectID": "listing-danl-m1-cw.html",
    "href": "listing-danl-m1-cw.html",
    "title": "DANL Module 1 - Classwork",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nClasswork 1\n\n\nPython Basics [Classwork 1 may be subject to change during the Module.]\n\n\nFebruary 6, 2024\n\n\n\n\nClasswork 2\n\n\nPandas Basics [Classwork 2 may be subject to change during the Module.]\n\n\nFebruary 13, 2024\n\n\n\n\nClasswork 3\n\n\nFiltering, Sorting, Ranking, and Visualizing DataFrames [Classwork 3 may be subject to change during the Module.]\n\n\nFebruary 20, 2024\n\n\n\n\nClasswork 4\n\n\nGroup Operations with GroupBy Objects [Classwork 4 may be subject to change during the Module.]\n\n\nFebruary 27, 2024\n\n\n\n\nClasswork 5\n\n\nReshaping and Pivoting DataFrames [Classwork 5 may be subject to change during the Module.]\n\n\nMarch 5, 2024\n\n\n\n\nClasswork 6\n\n\nMerging, Joining, and Concaternating DataFrames [Classwork 6 may be subject to change during the Module.]\n\n\nMarch 12, 2024\n\n\n\n\nClasswork 7\n\n\nMissing Data; Time-series Data [Classwork 7 may be subject to change during the Module.]\n\n\nMarch 19, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-5.html",
    "href": "danl-hw/danl-m1-hw-5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Direction\n\nWrite a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here.\n\n\n\n\nQuestion 1\nConcatenate the two weeks of sales data into one DataFrame. Assign the week1 DataFrame a key of “Week 1” and the week2 DataFrame a key of “Week 2”.\n\n\n\nQuestion 2\nFind the customers who ate at the restaurant both weeks.\n\n\n\nQuestion 3\nFind the customers who ate at the restaurant both weeks and ordered the same item each week.\n\nHINT You can join data sets on multiple columns by passing the on parameter a list of columns.\n\n\n\n\nQuestion 4\nIdentify which customers came in only on Week 1 and only on Week 2.\n\n\n\nQuestion 5\nEach row in the week1 DataFrame identifies a customer who purchased a food item. For each row, pull in the customer’s information from the customers DataFrame.\n\n\n\n\n Back to topReuse© 2024 Byeong-Hak Choe | This post is licensed under &lt;a href='http://creativecommons.org/licenses/by-nc-sa/4.0/' target='_blank'&gt;CC BY-NC-SA 4.0&lt;/a&gt;.CitationFor attribution, please cite this work as:\nChoe, Byeong-Hak. 2024. “Homework 5.” March 5, 2024."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-6.html",
    "href": "danl-hw/danl-m1-hw-6.html",
    "title": "Homework 6",
    "section": "",
    "text": "Direction\n\nWrite a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nImport Python libraries you need here.\n\n\nCiti Bike NYC is New York City’s official bike-sharing program. Residents and tourists can pick up and drop off bicycles at hundreds of locations around the city. Ride data is publicly available and released monthly by the city at https://www.citibikenyc.com/system-data. citibike.csv is a collection of ~1.9 million rides that cyclists took in June 2020. For simplicity’s sake, the data set has been modified from its original version and includes only two columns: each ride’s start time and end time. Let’s import the data set and assign it to a citi_bike variable:\n\nciti_bike = pd.read_csv(\"citibike.csv\")\n\n\n\nQuestion 1\nConvert the start_time and stop_time columns to store datetime (Timestamp) values instead of strings.\n\n\n\nQuestion 2\nCount the rides that occurred on each day of the week (Monday, Tuesday, and so on). Which weekday is the most popular for a bike ride? Use the start_time column as your starting point.\n\n\n\nQuestion 3\nCount the rides per week for each week within the month. To do so, round each date in the start_time column to its previous or current Monday. Assume that each week starts on a Monday and ends on a Sunday. Thus, the first week of June would be Monday, June 1 through Sunday, June 7.\n\n\n\nQuestion 4\nCalculate the duration of each ride, and save the results to a new duration column.\n\n\n\nQuestion 5\nFind the average duration of a bike ride.\n\n\n\nQuestion 6\nExtract the five longest bike rides by duration from the data set.\n\n\n\n\n Back to topReuse© 2024 Byeong-Hak Choe | This post is licensed under &lt;a href='http://creativecommons.org/licenses/by-nc-sa/4.0/' target='_blank'&gt;CC BY-NC-SA 4.0&lt;/a&gt;.CitationFor attribution, please cite this work as:\nChoe, Byeong-Hak. 2024. “Homework 6.” March 12, 2024."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html",
    "href": "danl-hw/danl-m1-hw-3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Write a Python code to answer each question.\nMake at least some simple comment (# ...) in each question.\nUse your working directory with the subfolder, data, so that the relative pathname of CSV files in the subfolder data is sufficient to import the CSV files."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q1a",
    "href": "danl-hw/danl-m1-hw-3.html#q1a",
    "title": "Homework 3",
    "section": "Q1a",
    "text": "Q1a\n\nCalculate the simple difference between the probability of survival when passengers are first-class and the probability of survival when they are not.\n\ncode-tools: true # title-block-banner: true\ncomments: hypothesis: theme: clean"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q1b",
    "href": "danl-hw/danl-m1-hw-3.html#q1b",
    "title": "Homework 3",
    "section": "Q1b",
    "text": "Q1b\n\nHow much does the probability of survival increase for first-class passengers relative to those who are not first-class passengers?\nSDP tells us what would happen to the probability of survival if non-first-class passengers were first-class.\n\nIn other words, SDP means the effect of being the first-class on the probability of survival from the Titanic Disaster."
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q1c",
    "href": "danl-hw/danl-m1-hw-3.html#q1c",
    "title": "Homework 3",
    "section": "Q1c",
    "text": "Q1c\n\nConsider the probability of survival in titanic_2.csv.\n\n\nAfter stratifying on gender and age, what happens to the difference in the probabilities of survival between first-class passengers and non-first-class passengers?\nExplain in your own words what stratifying on gender and age did for this difference in probabilities of survival between first-class passengers and non-first-class passengers. \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2a",
    "href": "danl-hw/danl-m1-hw-3.html#q2a",
    "title": "Homework 3",
    "section": "Q2a",
    "text": "Q2a\n\nHow many players have been recorded? \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2b.",
    "href": "danl-hw/danl-m1-hw-3.html#q2b.",
    "title": "Homework 3",
    "section": "Q2b.",
    "text": "Q2b.\n\nA column points (“P”) is missing in the data. The number of points of a player is defined as the sum of his goals (“G”) and assists (“A”).\nAdd the point column “P” to your DataFrame. \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2c.",
    "href": "danl-hw/danl-m1-hw-3.html#q2c.",
    "title": "Homework 3",
    "section": "Q2c.",
    "text": "Q2c.\n\nWho is the top scorer in terms of points? \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2d.",
    "href": "danl-hw/danl-m1-hw-3.html#q2d.",
    "title": "Homework 3",
    "section": "Q2d.",
    "text": "Q2d.\n\nHow many Russian (non-goalie) players had some ice time in there 2016/2017 regular season?\nHint: Nationality of a player can be found in “Nat”. Russians are indicated by “RUS”. \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2e.",
    "href": "danl-hw/danl-m1-hw-3.html#q2e.",
    "title": "Homework 3",
    "section": "Q2e.",
    "text": "Q2e.\n\nWhat are their names? \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2f.",
    "href": "danl-hw/danl-m1-hw-3.html#q2f.",
    "title": "Homework 3",
    "section": "Q2f.",
    "text": "Q2f.\n\nWho performed best among the Russian players in terms of points (“P”)? \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2g.",
    "href": "danl-hw/danl-m1-hw-3.html#q2g.",
    "title": "Homework 3",
    "section": "Q2g.",
    "text": "Q2g.\n\nHow many points (“P”) did he have? \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2h.",
    "href": "danl-hw/danl-m1-hw-3.html#q2h.",
    "title": "Homework 3",
    "section": "Q2h.",
    "text": "Q2h.\n\nHow well did he perform in the entire league? Put differently, what was his rank in terms of points? \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2i.",
    "href": "danl-hw/danl-m1-hw-3.html#q2i.",
    "title": "Homework 3",
    "section": "Q2i.",
    "text": "Q2i.\n\nFind the top ten scorers (in terms of points) and print them including their number of point and their respective team. \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q2j.",
    "href": "danl-hw/danl-m1-hw-3.html#q2j.",
    "title": "Homework 3",
    "section": "Q2j.",
    "text": "Q2j.\n\nWhat are the three countries with the most players originating from? \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q3a.",
    "href": "danl-hw/danl-m1-hw-3.html#q3a.",
    "title": "Homework 3",
    "section": "Q3a.",
    "text": "Q3a.\n\nFor each type of mine, calculate the total coal production for each pair of state-year. \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q3b.",
    "href": "danl-hw/danl-m1-hw-3.html#q3b.",
    "title": "Homework 3",
    "section": "Q3b.",
    "text": "Q3b.\n\nFind the top 5 coal-producing states for each year. \n\nAnswer"
  },
  {
    "objectID": "danl-hw/danl-m1-hw-3.html#q3c.",
    "href": "danl-hw/danl-m1-hw-3.html#q3c.",
    "title": "Homework 3",
    "section": "Q3c.",
    "text": "Q3c.\n\nVisualize the yearly trend of the total coal production from each type of mine. \n\nAnswer"
  },
  {
    "objectID": "listing-danl-m1-hw.html",
    "href": "listing-danl-m1-hw.html",
    "title": "DANL Module 1 - Homework",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nHomework 1\n\n\nPython Basics [Homework 1 may be subject to change during the Module.]\n\n\nFebruary 6, 2024\n\n\n\n\nHomework 2\n\n\nPandas Basics [Homework 2 may be subject to change during the Module.]\n\n\nFebruary 13, 2024\n\n\n\n\nHomework 3\n\n\nFiltering, Sorting, Ranking, and Visualizing DataFrames [Homework 3 may be subject to change during the Module.]\n\n\nFebruary 20, 2024\n\n\n\n\nHomework 4\n\n\nGroup Operations with GroupBy Objects [Homework 4 may be subject to change during the Module.]\n\n\nFebruary 27, 2024\n\n\n\n\nHomework 5\n\n\nReshaping and Pivoting DataFrames [Homework 5 may be subject to change during the Module.]\n\n\nMarch 5, 2024\n\n\n\n\nHomework 6\n\n\nMerging, Joining, and Concaternating DataFrames [Homework 6 may be subject to change during the Module.]\n\n\nMarch 12, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]